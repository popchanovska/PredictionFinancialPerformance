[
  {
    "title": "NVIDIA Blackwell Platform Arrives to Power a New Era of Computing",
    "link": "https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUhTVk5FYWtab1prbGpkbXB1VFJDb0FSaXNBaWdCTWdZbFJJN1BLUWc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "New Blackwell GPU, NVLink and Resilience Technologies Enable Trillion-Parameter-Scale AI Models\nNew Tensor Cores and TensorRT- LLM Compiler Reduce LLM Inference Operating Cost and Energy by up to 25x\nNew Accelerators Enable Breakthroughs in Data Processing, Engineering Simulation, Electronic Design Automation, Computer-Aided Drug Design and Quantum Computing\nWidespread Adoption by Every Major Cloud Provider, Server Maker and Leading AI Company\nGTC—Powering a new era of computing, NVIDIA today announced that the NVIDIA Blackwell platform has arrived — enabling organizations everywhere to build and run real-time generative AI on trillion-parameter large language models at up to 25x less cost and energy consumption than its predecessor.\nThe Blackwell GPU architecture features six transformative technologies for accelerated computing, which will help unlock breakthroughs in data processing, engineering simulation, electronic design automation, computer-aided drug design, quantum computing and generative AI — all emerging industry opportunities for NVIDIA.\n“For three decades we’ve pursued accelerated computing, with the goal of enabling transformative breakthroughs like deep learning and AI,” said Jensen Huang, founder and CEO of NVIDIA. “Generative AI is the defining technology of our time. Blackwell is the engine to power this new industrial revolution. Working with the most dynamic companies in the world, we will realize the promise of AI for every industry.”\nAmong the many organizations expected to adopt Blackwell are Amazon Web Services, Dell Technologies, Google, Meta, Microsoft, OpenAI, Oracle, Tesla and xAI.\nSundar Pichai, CEO of Alphabet and Google: “Scaling services like Search and Gmail to billions of users has taught us a lot about managing compute infrastructure. As we enter the AI platform shift, we continue to invest deeply in infrastructure for our own products and services, and for our Cloud customers. We are fortunate to have a longstanding partnership with NVIDIA, and look forward to bringing the breakthrough capabilities of the Blackwell GPU to our Cloud customers and teams across Google, including Google DeepMind, to accelerate future discoveries.”\nAndy Jassy, president and CEO of Amazon: “Our deep collaboration with NVIDIA goes back more than 13 years, when we launched the world’s first GPU cloud instance on AWS. Today we offer the widest range of GPU solutions available anywhere in the cloud, supporting the world’s most technologically advanced accelerated workloads. It's why the new NVIDIA Blackwell GPU will run so well on AWS and the reason that NVIDIA chose AWS to co-develop Project Ceiba, combining NVIDIA’s next-generation Grace Blackwell Superchips with the AWS Nitro System's advanced virtualization and ultra-fast Elastic Fabric Adapter networking, for NVIDIA's own AI research and development. Through this joint effort between AWS and NVIDIA engineers, we're continuing to innovate together to make AWS the best place for anyone to run NVIDIA GPUs in the cloud.”\nMichael Dell, founder and CEO of Dell Technologies: “Generative AI is critical to creating smarter, more reliable and efficient systems. Dell Technologies and NVIDIA are working together to shape the future of technology. With the launch of Blackwell, we will continue to deliver the next-generation of accelerated products and services to our customers, providing them with the tools they need to drive innovation across industries.”\nDemis Hassabis, cofounder and CEO of Google DeepMind: “The transformative potential of AI is incredible, and it will help us solve some of the world’s most important scientific problems. Blackwell’s breakthrough technological capabilities will provide the critical compute needed to help the world’s brightest minds chart new scientific discoveries.”\nMark Zuckerberg, founder and CEO of Meta: “AI already powers everything from our large language models to our content recommendations, ads, and safety systems, and it's only going to get more important in the future. We're looking forward to using NVIDIA's Blackwell to help train our open-source Llama models and build the next generation of Meta AI and consumer products.”\nSatya Nadella, executive chairman and CEO of Microsoft: “We are committed to offering our customers the most advanced infrastructure to power their AI workloads. By bringing the GB200 Grace Blackwell processor to our datacenters globally, we are building on our long-standing history of optimizing NVIDIA GPUs for our cloud, as we make the promise of AI real for organizations everywhere.”\nSam Altman, CEO of OpenAI: “Blackwell offers massive performance leaps, and will accelerate our ability to deliver leading-edge models. We’re excited to continue working with NVIDIA to enhance AI compute.”\nLarry Ellison, chairman and CTO of Oracle: \"Oracle’s close collaboration with NVIDIA will enable qualitative and quantitative breakthroughs in AI, machine learning and data analytics. In order for customers to uncover more actionable insights, an even more powerful engine like Blackwell is needed, which is purpose-built for accelerated computing and generative AI.”\nElon Musk, CEO of Tesla and xAI: “There is currently nothing better than NVIDIA hardware for AI.”\nNamed in honor of David Harold Blackwell — a mathematician who specialized in game theory and statistics, and the first Black scholar inducted into the National Academy of Sciences — the new architecture succeeds the NVIDIA Hopper™ architecture, launched two years ago.\nBlackwell Innovations to Fuel Accelerated Computing and Generative AI\nBlackwell’s six revolutionary technologies, which together enable AI training and real-time LLM inference for models scaling up to 10 trillion parameters, include:\nWorld’s Most Powerful Chip — Packed with 208 billion transistors, Blackwell-architecture GPUs are manufactured using a custom-built 4NP TSMC process with two-reticle limit GPU dies connected by 10 TB/second chip-to-chip link into a single, unified GPU.\nSecond-Generation Transformer Engine — Fueled by new micro-tensor scaling support and NVIDIA’s advanced dynamic range management algorithms integrated into NVIDIA TensorRT™-LLM and NeMo Megatron frameworks, Blackwell will support double the compute and model sizes with new 4-bit floating point AI inference capabilities.\nFifth-Generation NVLink — To accelerate performance for multitrillion-parameter and mixture-of-experts AI models, the latest iteration of NVIDIA NVLink® delivers groundbreaking 1.8TB/s bidirectional throughput per GPU, ensuring seamless high-speed communication among up to 576 GPUs for the most complex LLMs.\nRAS Engine — Blackwell-powered GPUs include a dedicated engine for reliability, availability and serviceability. Additionally, the Blackwell architecture adds capabilities at the chip level to utilize AI-based preventative maintenance to run diagnostics and forecast reliability issues. This maximizes system uptime and improves resiliency for massive-scale AI deployments to run uninterrupted for weeks or even months at a time and to reduce operating costs.\nSecure AI — Advanced confidential computing capabilities protect AI models and customer data without compromising performance, with support for new native interface encryption protocols, which are critical for privacy-sensitive industries like healthcare and financial services.\nDecompression Engine — A dedicated decompression engine supports the latest formats, accelerating database queries to deliver the highest performance in data analytics and data science. In the coming years, data processing, on which companies spend tens of billions of dollars annually, will be increasingly GPU-accelerated.\nThe NVIDIA GB200 Grace Blackwell Superchip connects two NVIDIA B200 Tensor Core GPUs to the NVIDIA Grace CPU over a 900GB/s ultra-low-power NVLink chip-to-chip interconnect.\nFor the highest AI performance, GB200-powered systems can be connected with the NVIDIA Quantum-X800 InfiniBand and Spectrum™-X800 Ethernet platforms, also announced today, which deliver advanced networking at speeds up to 800Gb/s.\nThe GB200 is a key component of the NVIDIA GB200 NVL72, a multi-node, liquid-cooled, rack-scale system for the most compute-intensive workloads. It combines 36 Grace Blackwell Superchips, which include 72 Blackwell GPUs and 36 Grace CPUs interconnected by fifth-generation NVLink. Additionally, GB200 NVL72 includes NVIDIA BlueField®-3 data processing units to enable cloud network acceleration, composable storage, zero-trust security and GPU compute elasticity in hyperscale AI clouds. The GB200 NVL72 provides up to a 30x performance increase compared to the same number of NVIDIA H100 Tensor Core GPUs for LLM inference workloads, and reduces cost and energy consumption by up to 25x.\nThe platform acts as a single GPU with 1.4 exaflops of AI performance and 30TB of fast memory, and is a building block for the newest DGX SuperPOD.\nNVIDIA offers the HGX B200, a server board that links eight B200 GPUs through NVLink to support x86-based generative AI platforms. HGX B200 supports networking speeds up to 400Gb/s through the NVIDIA Quantum-2 InfiniBand and Spectrum-X Ethernet networking platforms.\nGlobal Network of Blackwell Partners\nBlackwell-based products will be available from partners starting later this year.\nAWS, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure will be among the first cloud service providers to offer Blackwell-powered instances, as will NVIDIA Cloud Partner program companies Applied Digital, CoreWeave, Crusoe, IBM Cloud, Lambda and Nebius. Sovereign AI clouds will also provide Blackwell-based cloud services and infrastructure, including Indosat Ooredoo Hutchinson, Nexgen Cloud, Oracle EU Sovereign Cloud, the Oracle US, UK, and Australian Government Clouds, Scaleway, Singtel, Northern Data Group's Taiga Cloud, Yotta Data Services’ Shakti Cloud and YTL Power International.\nGB200 will also be available on NVIDIA DGX™ Cloud, an AI platform co-engineered with leading cloud service providers that gives enterprise developers dedicated access to the infrastructure and software needed to build and deploy advanced generative AI models. AWS, Google Cloud and Oracle Cloud Infrastructure plan to host new NVIDIA Grace Blackwell-based instances later this year.\nCisco, Dell, Hewlett Packard Enterprise, Lenovo and Supermicro are expected to deliver a wide range of servers based on Blackwell products, as are Aivres, ASRock Rack, ASUS, Eviden, Foxconn, GIGABYTE, Inventec, Pegatron, QCT, Wistron, Wiwynn and ZT Systems.\nAdditionally, a growing network of software makers, including Ansys, Cadence and Synopsys — global leaders in engineering simulation — will use Blackwell-based processors to accelerate their software for designing and simulating electrical, mechanical and manufacturing systems and parts. Their customers can use generative AI and accelerated computing to bring products to market faster, at lower cost and with higher energy efficiency.\nThe Blackwell product portfolio is supported by NVIDIA AI Enterprise, the end-to-end operating system for production-grade AI. NVIDIA AI Enterprise includes NVIDIA NIM™ inference microservices — also announced today — as well as AI frameworks, libraries and tools that enterprises can deploy on NVIDIA-accelerated clouds, data centers and workstations.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Supercharges Hopper, the World’s Leading AI Computing Platform",
    "link": "https://nvidianews.nvidia.com/news/nvidia-supercharges-hopper-the-worlds-leading-ai-computing-platform",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVNVMEUwVDI5TVNrRllYMTl5VFJDb0FSaXNBaWdCTWdZcGhaTE9NUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-11-13T08:00:00.000Z",
    "time": "Nov 13, 2023",
    "articleType": "regular",
    "content": "HGX H200 Systems and Cloud Instances Coming Soon From World’s Top Server Manufacturers and Cloud Service Providers\nSC23—NVIDIA today announced it has supercharged the world’s leading AI computing platform with the introduction of the NVIDIA HGX™ H200. Based on NVIDIA Hopper™ architecture, the platform features the NVIDIA H200 Tensor Core GPU with advanced memory to handle massive amounts of data for generative AI and high performance computing workloads.\nThe NVIDIA H200 is the first GPU to offer HBM3e — faster, larger memory to fuel the acceleration of generative AI and large language models, while advancing scientific computing for HPC workloads. With HBM3e, the NVIDIA H200 delivers 141GB of memory at 4.8 terabytes per second, nearly double the capacity and 2.4x more bandwidth compared with its predecessor, the NVIDIA A100.\nH200-powered systems from the world’s leading server manufacturers and cloud service providers are expected to begin shipping in the second quarter of 2024.\n“To create intelligence with generative AI and HPC applications, vast amounts of data must be efficiently processed at high speed using large, fast GPU memory,” said Ian Buck, vice president of hyperscale and HPC at NVIDIA. “With NVIDIA H200, the industry’s leading end-to-end AI supercomputing platform just got faster to solve some of the world’s most important challenges.”\nPerpetual Innovation, Perpetual Performance Leaps\nThe NVIDIA Hopper architecture delivers an unprecedented performance leap over its predecessor and continues to raise the bar through ongoing software enhancements with H100, including the recent release of powerful open-source libraries like NVIDIA TensorRT™-LLM.\nThe introduction of H200 will lead to further performance leaps, including nearly doubling inference speed on Llama 2, a 70 billion-parameter LLM, compared to the H100. Additional performance leadership and improvements with H200 are expected with future software updates.\nNVIDIA H200 will be available in NVIDIA HGX H200 server boards with four- and eight-way configurations, which are compatible with both the hardware and software of HGX H100 systems. It is also available in the NVIDIA GH200 Grace Hopper™ Superchip with HBM3e, announced in August.\nWith these options, H200 can be deployed in every type of data center, including on premises, cloud, hybrid-cloud and edge. NVIDIA’s global ecosystem of partner server makers — including ASRock Rack, ASUS, Dell Technologies, Eviden, GIGABYTE, Hewlett Packard Enterprise, Ingrasys, Lenovo, QCT, Supermicro, Wistron and Wiwynn — can update their existing systems with an H200.\nAmazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure will be among the first cloud service providers to deploy H200-based instances starting next year, in addition to CoreWeave, Lambda and Vultr.\nPowered by NVIDIA NVLink™ and NVSwitch™ high-speed interconnects, HGX H200 provides the highest performance on various application workloads, including LLM training and inference for the largest models beyond 175 billion parameters.\nAn eight-way HGX H200 provides over 32 petaflops of FP8 deep learning compute and 1.1TB of aggregate high-bandwidth memory for the highest performance in generative AI and HPC applications.\nWhen paired with NVIDIA Grace™ CPUs with an ultra-fast NVLink-C2C interconnect, the H200 creates the GH200 Grace Hopper Superchip with HBM3e — an integrated module designed to serve giant-scale HPC and AI applications.\nAccelerate AI With NVIDIA Full-Stack Software\nNVIDIA’s accelerated computing platform is supported by powerful software tools that enable developers and enterprises to build and accelerate production-ready applications from AI to HPC. This includes the NVIDIA AI Enterprise suite of software for workloads such as speech, recommender systems and hyperscale inference.\nThe NVIDIA H200 will be available from global system manufacturers and cloud service providers starting in the second quarter of 2024.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Announces Financial Results for First Quarter Fiscal 2025",
    "link": "https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2025",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUhXbkUyU2xObFUyMDBUV1Z5VFJDM0FSaVRBaWdCTWdZTkY0ZG5WZ00=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-22T07:00:00.000Z",
    "time": "May 22",
    "articleType": "regular",
    "content": "Record quarterly revenue of $26.0 billion, up 18% from Q4 and up 262% from a year ago\nRecord quarterly Data Center revenue of $22.6 billion, up 23% from Q4 and up 427% from a year ago\nTen-for-one forward stock split effective June 7, 2024\nQuarterly cash dividend raised 150% to $0.01 per share on a post-split basis\nNVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 28, 2024, of $26.0 billion, up 18% from the previous quarter and up 262% from a year ago.\nFor the quarter, GAAP earnings per diluted share was $5.98, up 21% from the previous quarter and up 629% from a year ago. Non-GAAP earnings per diluted share was $6.12, up 19% from the previous quarter and up 461% from a year ago.\n“The next industrial revolution has begun — companies and countries are partnering with NVIDIA to shift the trillion-dollar traditional data centers to accelerated computing and build a new type of data center — AI factories — to produce a new commodity: artificial intelligence,” said Jensen Huang, founder and CEO of NVIDIA. “AI will bring significant productivity gains to nearly every industry and help companies be more cost- and energy-efficient, while expanding revenue opportunities.\n“Our data center growth was fueled by strong and accelerating demand for generative AI training and inference on the Hopper platform. Beyond cloud service providers, generative AI has expanded to consumer internet companies, and enterprise, sovereign AI, automotive and healthcare customers, creating multiple multibillion-dollar vertical markets.\n“We are poised for our next wave of growth. The Blackwell platform is in full production and forms the foundation for trillion-parameter-scale generative AI. Spectrum-X opens a brand-new market for us to bring large-scale AI to Ethernet-only data centers. And NVIDIA NIM is our new software offering that delivers enterprise-grade, optimized generative AI to run on CUDA everywhere — from the cloud to on-prem data centers and RTX AI PCs — through our expansive network of ecosystem partners.”\nNVIDIA also announced a ten-for-one forward stock split of NVIDIA’s issued common stock to make stock ownership more accessible to employees and investors. The split will be effected through an amendment to NVIDIA’s Restated Certificate of Incorporation, which will result in a proportionate increase in the number of shares of authorized common stock. Each record holder of common stock as of the close of market on Thursday, June 6, 2024, will receive nine additional shares of common stock, to be distributed after the close of market on Friday, June 7, 2024. Trading is expected to commence on a split-adjusted basis at market open on Monday, June 10, 2024.\nNVIDIA is increasing its quarterly cash dividend by 150% from $0.04 per share to $0.10 per share of common stock. The increased dividend is equivalent to $0.01 per share on a post-split basis and will be paid on Friday, June 28, 2024, to all shareholders of record on Tuesday, June 11, 2024.\n($ in millions, except earnings\n($ in millions, except earnings\nNVIDIA’s outlook for the second quarter of fiscal 2025 is as follows:\nRevenue is expected to be $28.0 billion, plus or minus 2%.\nGAAP and non-GAAP gross margins are expected to be 74.8% and 75.5%, respectively, plus or minus 50 basis points. For the full year, gross margins are expected to be in the mid-70% range.\nGAAP and non-GAAP operating expenses are expected to be approximately $4.0 billion and $2.8 billion, respectively. Full-year operating expenses are expected to grow in the low-40% range.\nGAAP and non-GAAP other income and expense are expected to be an income of approximately $300 million, excluding gains and losses from non-affiliated investments.\nGAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items.\nNVIDIA achieved progress since its previous earnings announcement in these areas:\nFirst-quarter revenue was a record $22.6 billion, up 23% from the previous quarter and up 427% from a year ago.\nUnveiled the NVIDIA Blackwell platform to fuel a new era of AI computing at trillion-parameter scale and the Blackwell-powered DGX SuperPOD™ for generative AI supercomputing.\nAnnounced NVIDIA Quantum and NVIDIA Spectrum™ X800 series switches for InfiniBand and Ethernet, respectively, optimized for trillion-parameter GPU computing and AI infrastructure.\nLaunched NVIDIA AI Enterprise 5.0 with NVIDIA NIM inference microservices to speed enterprise app development.\nAnnounced TSMC and Synopsys are going into production with NVIDIA cuLitho to accelerate computational lithography, the semiconductor manufacturing industry’s most compute-intensive workload.\nAnnounced that nine new supercomputers worldwide are using Grace Hopper Superchips to ignite new era of AI supercomputing.\nUnveiled that Grace Hopper Superchips power the top three machines on the Green500 list of the world’s most energy-efficient supercomputers.\nExpanded collaborations with AWS, Google Cloud, Microsoft and Oracle to advance generative AI innovation.\nWorked with Johnson & Johnson MedTech to bring AI capabilities to support surgery.\nFirst-quarter Gaming revenue was $2.6 billion, down 8% from the previous quarter and up 18% from a year ago.\nIntroduced new AI gaming technologies at GDC for NVIDIA ACE and Neural Graphics.\nUnveiled new AI performance optimizations and integrations for Windows to deliver maximum performance on NVIDIA GeForce RTX AI PCs and workstations.\nAnnounced more blockbuster games that will incorporate RTX technology, including Star Wars Outlaws and Black Myth Wukong.\nAdded support for new models, including Google’s Gemma, for ChatRTX, which brings chatbot capabilities to RTX-powered Windows PCs and workstations.\nFirst-quarter revenue was $427 million, down 8% from the previous quarter and up 45% from a year ago.\nIntroduced NVIDIA RTX™ 500 and 1000 professional Ada generation laptop GPUs for AI-enhanced workflows.\nUnveiled NVIDIA RTX A400 and A1000 GPUs for desktop workstations, based on the NVIDIA Ampere architecture, to bring AI to design and productivity workflows.\nIntroduced NVIDIA Omniverse™ Cloud APIs to power industrial digital twin software tools, including an expanded Siemens partnership, and a new framework for the Apple Vision Pro.\nAnnounced the adoption of the new Earth-2 cloud APIs by The Weather Company and the Central Weather Administration of Taiwan for high-resolution global climate simulations.\nFirst-quarter Automotive revenue was $329 million, up 17% from the previous quarter and up 11% from a year ago.\nAnnounced BYD, XPENG, GAC’s AION Hyper, Nuro and others have chosen the next-generation NVIDIA DRIVE Thor™ platform, which now features Blackwell GPU architecture, to power their next-generation consumer and commercial electric vehicle fleets.\nRevealed U.S. and China electric vehicle makers Lucid and IM Motors are using the NVIDIA DRIVE Orin™ platform for vehicle models targeting the European market.\nAnnounced an array of partners are using NVIDIA generative AI technologies to transform in-vehicle experiences.\nIntroduced the Project GR00T foundation model for humanoid robots and major Isaac robotics platform updates.\nCommentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available at https://investor.nvidia.com.\nConference Call and Webcast Information\nNVIDIA will conduct a conference call with analysts and investors to discuss its first quarter fiscal 2025 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website, https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its second quarter of fiscal 2025.\nTo supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude stock-based compensation expense, acquisition-related and other costs, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases related to property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\nCONDENSED CONSOLIDATED STATEMENTS OF INCOME\n(In millions, except per share data)\nWeighted average shares used in per share computation:\nCash, cash equivalents and marketable securities\nPrepaid expenses and other current assets\nAccrued and other current liabilities\nTotal liabilities and shareholders' equity\nCONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS\nCash flows from operating activities:\nAdjustments to reconcile net income to net cash provided by operating activities:\nRealized and unrealized (gains) losses on investments in non-affiliated entities, net\nChanges in operating assets and liabilities, net of acquisitions:\nPrepaid expenses and other assets\nAccrued and other current liabilities\nNet cash provided by operating activities\nCash flows from investing activities:\nProceeds from maturities of marketable securities\nProceeds from sales of marketable securities\nPurchase related to property and equipment and intangible assets\nAcquisitions, net of cash acquired\nNet cash used in investing activities\nCash flows from financing activities:\nProceeds related to employee stock plans\nPayments related to repurchases of common stock\nPayments related to tax on restricted stock units\nPrincipal payments on property and equipment and intangible assets\nNet cash used in financing activities\nChange in cash and cash equivalents\nCash and cash equivalents at beginning of period\nCash and cash equivalents at end of period\nRECONCILIATION OF GAAP TO NON-GAAP FINANCIAL MEASURES\n(In millions, except per share data)\nAcquisition-related and other costs (A)\nAcquisition-related and other costs (A)\nTotal impact of non-GAAP adjustments to operating income\nGAAP other income (expense), net\n(Gains) losses from non-affiliated investments\nInterest expense related to amortization of debt discount\nNon-GAAP other income (expense), net\nTotal pre-tax impact of non-GAAP adjustments\nIncome tax impact of non-GAAP adjustments (D)\nDiluted net income per share\nWeighted average shares used in diluted net income per share computation\nGAAP net cash provided by operating activities\nPurchases related to property and equipment and intangible assets\nPrincipal payments on property and equipment and intangible assets\n(A) Acquisition-related and other costs are comprised of amortization of intangible assets and transaction costs, and are included in the following line items:\n(B) Stock-based compensation consists of the following:\n(C) Other consists of IP-related costs and assets held for sale related adjustments.\n(D) Income tax impact of non-GAAP adjustments, including the recognition of excess tax benefits or deficiencies related to stock-based compensation under GAAP accounting standard (ASU 2016-09).\nRECONCILIATION OF GAAP TO NON-GAAP OUTLOOK\nImpact of stock-based compensation expense, acquisition-related costs, and other costs\nStock-based compensation expense, acquisition-related costs, and other costs",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "High Can See Clearly Now: AI-Powered NVIDIA RTX Video HDR Transforms Standard Video Into Stunning High Dynamic Range",
    "link": "https://blogs.nvidia.com/blog/rtx-video-hdr-remix-studio-driver/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW1TMDlUWW5odFZuaFJSWHBYVFJDakFSaTBBaWdCTWdZaFFJYndxQU0=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-24T08:00:00.000Z",
    "time": "Jan 24",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Unveils Next-Generation GH200 Grace Hopper Superchip Platform for Era of Accelerated Computing and Generative AI",
    "link": "https://nvidianews.nvidia.com/news/gh200-grace-hopper-superchip-with-hbm3e-memory",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW1VMlp5WDFKdE1tRlJTRzVpVFJDb0FSaXNBaWdCTWdhbFZJN09KUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "World’s First HBM3e Processor Offers Groundbreaking Memory, Bandwidth; Ability to Connect Multiple GPUs for Exceptional Performance; Easily Scalable Server Design\nSIGGRAPH—NVIDIA today announced the next-generation NVIDIA GH200 Grace Hopper™ platform — based on a new Grace Hopper Superchip with the world’s first HBM3e processor — built for the era of accelerated computing and generative AI.\nCreated to handle the world’s most complex generative AI workloads, spanning large language models, recommender systems and vector databases, the new platform will be available in a wide range of configurations.\nThe dual configuration — which delivers up to 3.5x more memory capacity and 3x more bandwidth than the current generation offering — comprises a single server with 144 Arm Neoverse cores, eight petaflops of AI performance and 282GB of the latest HBM3e memory technology.\n“To meet surging demand for generative AI, data centers require accelerated computing platforms with specialized needs,” said Jensen Huang, founder and CEO of NVIDIA. “The new GH200 Grace Hopper Superchip platform delivers this with exceptional memory technology and bandwidth to improve throughput, the ability to connect GPUs to aggregate performance without compromise, and a server design that can be easily deployed across the entire data center.”\nThe new platform uses the Grace Hopper Superchip, which can be connected with additional Superchips by NVIDIA NVLink™, allowing them to work together to deploy the giant models used for generative AI. This high-speed, coherent technology gives the GPU full access to the CPU memory, providing a combined 1.2TB of fast memory when in dual configuration.\nHBM3e memory, which is 50% faster than current HBM3, delivers a total of 10TB/sec of combined bandwidth, allowing the new platform to run models 3.5x larger than the previous version, while improving performance with 3x faster memory bandwidth.\nGrowing Demand for Grace Hopper\nLeading manufacturers are already offering systems based on the previously announced Grace Hopper Superchip. To drive broad adoption of the technology, the next-generation Grace Hopper Superchip platform with HBM3e is fully compatible with the NVIDIA MGX™ server specification unveiled at COMPUTEX earlier this year. With MGX, any system manufacturer can quickly and cost-effectively add Grace Hopper into over 100 server variations.\nLeading system manufacturers are expected to deliver systems based on the platform in Q2 of calendar year 2024.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Announces Financial Results for Third Quarter Fiscal 2024",
    "link": "https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2024",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXJOREk1WVROVVZrbGljRFpwVFJDNEFSaVNBaWdCTWdZTkY0ZG5WZ00=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-11-21T08:00:00.000Z",
    "time": "Nov 21, 2023",
    "articleType": "regular",
    "content": "Record revenue of $18.12 billion, up 34% from Q2, up 206% from year ago\nRecord Data Center revenue of $14.51 billion, up 41% from Q2, up 279% from year ago\nNVIDIA (NASDAQ: NVDA) today reported revenue for the third quarter ended October 29, 2023, of $18.12 billion, up 206% from a year ago and up 34% from the previous quarter.\nGAAP earnings per diluted share for the quarter were $3.71, up more than 12x from a year ago and up 50% from the previous quarter. Non-GAAP earnings per diluted share were $4.02, up nearly 6x from a year ago and up 49% from the previous quarter.\n“Our strong growth reflects the broad industry platform transition from general-purpose to accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\n“Large language model startups, consumer internet companies and global cloud service providers were the first movers, and the next waves are starting to build. Nations and regional CSPs are investing in AI clouds to serve local demand, enterprise software companies are adding AI copilots and assistants to their platforms, and enterprises are creating custom AI to automate the world’s largest industries.\n“NVIDIA GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. The era of generative AI is taking off,” he said.\nNVIDIA will pay its next quarterly cash dividend of $0.04 per share on December 28, 2023, to all shareholders of record on December 6, 2023.\n($ in millions, except earnings\n($ in millions, except earnings\nNVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\nRevenue is expected to be $20.00 billion, plus or minus 2%.\nGAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points.\nGAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.20 billion, respectively.\nGAAP and non-GAAP other income and expense are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments.\nGAAP and non-GAAP tax rates are expected to be 15.0%, plus or minus 1%, excluding any discrete items.\nNVIDIA achieved progress since its previous earnings announcement in these areas:\nThird-quarter revenue was a record $14.51 billion, up 41% from the previous quarter and up 279% from a year ago.\nAnnounced NVIDIA HGX™ H200 with the new NVIDIA H200 Tensor Core GPU, the first GPU with HBM3e memory, with systems expected to be available in the second quarter of next year.\nIntroduced an AI foundry service — with NVIDIA AI Foundation Models, NVIDIA NeMo™ framework and NVIDIA DGX™ Cloud AI supercomputing — to accelerate the development and tuning of custom generative AI applications, first available on Microsoft Azure, with SAP and Amdocs among the first customers.\nAnnounced that the NVIDIA Spectrum-X™ Ethernet networking platform for AI will be integrated into servers from Dell Technologies, Hewlett Packard Enterprise and Lenovo in the first quarter of next year.\nAnnounced that NVIDIA GH200 Grace Hopper Superchips, including a new quad configuration, will power more than 40 new supercomputers, including the JUPITER system at Jülich Supercomputing Centre and Isambard-AI at the University of Bristol.\nMade advances with global cloud service providers:\nGoogle Cloud Platform made generally available new A3 instances powered by NVIDIA H100 Tensor Core GPUs and NVIDIA AI Enterprise software in Google Cloud Marketplace.\nMicrosoft Azure will be offering customers access to NVIDIA Omniverse™ Cloud Services for accelerating automotive digitalization, as well as new instances featuring NVL H100 Tensor Core GPUs and H100 with confidential computing, with H200 GPUs coming next year.\nOracle Cloud Infrastructure made NVIDIA DGX Cloud and NVIDIA AI Enterprise software available in Oracle Cloud Marketplace.\nPartnered with a range of leading companies on AI initiatives, including Amdocs, Dropbox, Foxconn, Genentech (member of Roche Group), Infosys, Lenovo, Reliance Industries, Scaleway and Tata Group.\nAnnounced record-setting performance in the latest two sets of MLPerf benchmarks for inference and training, with the NVIDIA Eos AI supercomputer training a GPT-3 model 3x faster than the previous record.\nAnnounced growing worldwide support for the NVIDIA® CUDA® Quantum platform, including new efforts in Israel, the Netherlands, the U.K. and the U.S.\nThird-quarter revenue was $2.86 billion, up 15% from the previous quarter and up 81% from a year ago.\nLaunched DLSS 3.5 Ray Reconstruction, which creates high-quality ray-traced images for intensive ray-traced games and apps, including Alan Wake 2 and Cyberpunk 2077.\nReleased TensorRT-LLM™ for Windows, speeding on-device LLM inference by up to 4x.\nAdded 56 DLSS games and over 15 Reflex games, bringing the total number of RTX games and applications to over 475.\nSurpassed 1,700 games on GeForce NOW™, including launches of Alan Wake 2, Baldur’s Gate 3, Cyberpunk 2077: Phantom Liberty, Forza Motorsport and Starfield.\nThird-quarter revenue was $416 million, up 10% from the previous quarter and up 108% from a year ago.\nAnnounced that Mercedes-Benz is using NVIDIA Omniverse to create digital twins to help plan, design, build and operate its manufacturing and assembly facilities around the world.\nAnnounced a new line of desktop workstations with NVIDIA RTX™ 6000 Ada Generation GPUs and NVIDIA ConnectX® smart interface cards for training smaller AI models, fine-tuning models and running inference locally.\nThird-quarter revenue was $261 million, up 3% from the previous quarter and up 4% from a year ago.\nFurthered its collaboration with Foxconn to develop next-generation electric vehicles for the global market, using the next-generation NVIDIA DRIVE Hyperion™ platform and NVIDIA DRIVE Thor™ system-on-a-chip.\nCommentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available at https://investor.nvidia.com.\nConference Call and Webcast Information\nNVIDIA will conduct a conference call with analysts and investors to discuss its third quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website, https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its fourth quarter and fiscal 2024.\nTo supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\nCONDENSED CONSOLIDATED STATEMENTS OF INCOME\n(In millions, except per share data)\nWeighted average shares used in per share computation:\nCash, cash equivalents and marketable securities\nPrepaid expenses and other current assets\nAccrued and other current liabilities\nTotal liabilities and shareholders’ equity\nCONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS\nCash flows from operating activities:\nAdjustments to reconcile net income to net cash\nLosses on investments in non-affiliates, net\nChanges in operating assets and liabilities, net of acquisitions:\nPrepaid expenses and other assets\nAccrued liabilities and other current liabilities\nNet cash provided by operating activities\nCash flows from investing activities:\nProceeds from maturities of marketable securities\nProceeds from sales of marketable securities\nPurchase related to property and equipment and intangible assets\nAcquisitions, net of cash acquired\nInvestments in non-affiliates and other, net\nNet cash provided by (used in) investing activities\nCash flows from financing activities:\nProceeds related to employee stock plans\nPayments related to repurchases of common stock\nPayments related to tax on restricted stock units\nPrincipal payments on property and equipment and intangible assets\nNet cash used in financing activities\nChange in cash, cash equivalents, and restricted cash\nCash, cash equivalents, and restricted cash at beginning of period\nCash, cash equivalents, and restricted cash at end of period\nSupplemental disclosure of cash flow information:\nCash paid for income taxes, net\nRECONCILIATION OF GAAP TO NON-GAAP FINANCIAL MEASURES\n(In millions, except per share data)\nAcquisition-related and other costs (A)\nAcquisition-related and other costs (A)\nTotal impact of non-GAAP adjustments to operating income\nGAAP other income (expense), net\n(Gains) losses from non-affiliated investments\nInterest expense related to amortization of debt discount\nNon-GAAP other income (expense), net\nTotal pre-tax impact of non-GAAP adjustments\nIncome tax impact of non-GAAP adjustments (D)\nDiluted net income per share\nWeighted average shares used in diluted net income per share computation\nGAAP net cash provided by operating activities\nPurchases related to property and equipment and intangible assets\nPrincipal payments on property and equipment and intangible assets\n(A) Acquisition-related and other costs are comprised of amortization of intangible assets and transaction costs, and are included in the following line items:\n(B) Stock-based compensation consists of the following:\n(C) Other consists of costs related to Russia branch office closure, assets held for sale related adjustments, legal settlement costs, and contributions.\n(D) Income tax impact of non-GAAP adjustments, including the recognition of excess tax benefits or deficiencies related to stock-based compensation under GAAP accounting standard (ASU 2016-09).\nRECONCILIATION OF GAAP TO NON-GAAP OUTLOOK\nImpact of stock-based compensation expense, acquisition-related costs, and other costs\nStock-based compensation expense, acquisition-related costs, and other costs",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Omniverse Expands Worlds Using Apple Vision Pro",
    "link": "https://blogs.nvidia.com/blog/omniverse-apple-vision-pro/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDVZMWQ0WlVkSmQzQTFWMEZIVFJDb0FSaXNBaWdCTWdZTlVKS0tLUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Omniverse Cloud APIs to Power Wave of Industrial Digital Twin Software Tools",
    "link": "https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTFkRkpqTlhCUmNVRjVNWGRPVFJDb0FSaXNBaWdCTWdZcE5ZYXVIUW8=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Ansys, Cadence, Hexagon, Microsoft, Rockwell Automation, Siemens, Trimble Adopt Omniverse Technologies to Help Customers Design, Simulate, Build and Operate Physically Based Digital Twins\nGTC—NVIDIA today announced that NVIDIA Omniverse™ Cloud will be available as APIs, extending the reach of the world’s leading platform for creating industrial digital twin applications and workflows across the entire ecosystem of software makers.\nThe five new Omniverse Cloud application programming interfaces enable developers to easily integrate core Omniverse technologies directly into existing design and automation software applications for digital twins, or their simulation workflows for testing and validating autonomous machines like robots or self-driving vehicles.\nSome of the world’s largest industrial software makers that are embracing Omniverse Cloud APIs into their software portfolios include: Ansys, Cadence, Dassault Systèmes for its 3DEXCITE brand, Hexagon, Microsoft, Rockwell Automation, Siemens and Trimble.\n“Everything manufactured will have digital twins,” said Jensen Huang, founder and CEO of NVIDIA. “Omniverse is the operating system for building and operating physically realistic digital twins. Omniverse and generative AI are the foundational technologies to digitalize the $50 trillion heavy industries market.”\nThe five new Omniverse Cloud APIs, which can be used individually or collectively, include:\nUSD Render — generates fully ray-traced NVIDIA RTX™ renders of OpenUSD data.\nUSD Write — lets users modify and interact with OpenUSD data.\nUSD Query — enables scene queries and interactive scenarios.\nUSD Notify — tracks USD changes and provides updates.\nOmniverse Channel — connects users, tools and worlds to enable collaboration across scenes.\nBringing Interactive Visualization and Collaboration to Industrial Applications\nSiemens, a leading technology company for automation, digitalization and sustainability, is adopting Omniverse Cloud APIs within its Siemens Xcelerator Platform, starting with Teamcenter X, the industry-leading cloud-based product lifecycle management (PLM) software.\nIn his GTC keynote, Huang showed Teamcenter X connected to Omniverse APIs, giving the software the ability to connect design data to NVIDIA generative AI APIs, and use Omniverse RTX rendering directly inside the app.\n“Through the NVIDIA Omniverse API, Siemens empowers customers with generative AI to make their physics-based digital twins even more immersive,” said Roland Busch, president and CEO of Siemens AG. “This will help everybody to design, build and test next-generation products, manufacturing processes and factories virtually before they are built in the physical world. By combining the real and the digital worlds, Siemens digital twin technology is enabling companies around the world to become more competitive, resilient and sustainable.”\nAnsys, a leader in engineering-simulation software, is adopting Omniverse Cloud APIs to enable data interoperability and RTX visualization in solutions such as Ansys AVxcelerate™ for autonomous vehicles, Ansys Perceive EM for 6G simulation, and NVIDIA accelerated solvers such as Ansys Fluent™.\nCadence, a leading computational software provider, is adopting Omniverse Cloud APIs into its Cadence® Reality Digital Twin Platform so enterprises can design, simulate and optimize data centers in a digital twin prior to physical build-out.\nDassault Systèmes, a leader in virtual universes for sustainable innovation, is adopting Omniverse Cloud APIs and Shutterstock 3D AI Services to power generative storytelling in its 3DEXCITE applications for content creation.\nConstruction and geospatial technology leader Trimble plans to leverage the APIs to enable the use of interactive NVIDIA Omniverse RTX viewers with Trimble model data.\nHexagon, the global leader in reality technology, will integrate its reality capture sensors and digital reality platforms with the NVIDIA Omniverse Cloud APIs through USD interoperability, empowering customers with hyper-realistic simulation and visualization capabilities.\nIndustrial-automation and digital transformation company Rockwell Automation will use Omniverse Cloud APIs to power RTX-enabled visualization.\nIn a demo released at GTC, Microsoft and NVIDIA reveal early work with Hexagon and Rockwell Automation that showcases these advancements.\nAs demand increases for robots, autonomous vehicles (AVs) and AI-based monitoring systems, developers are seeking to accelerate their end-to-end workflows.\nSensor data is critical for training, testing and validating full-stack autonomy, from perception to planning and control.\nOmniverse Cloud APIs connect a rich developer ecosystem of simulation tools and applications — such as Foretellix’s Foretify™ Platform, CARLA, MathWorks — and industry-leading sensor solution providers like FORVIA HELLA, Luminar, SICK AG and Sony Semiconductor Solutions to enable full-stack training and testing with high-fidelity, physically based sensor simulation.\nAvailable first on Microsoft Azure, Omniverse Cloud APIs will be offered later this year to developers for use on self-hosted and managed NVIDIA accelerated systems.\n“The next era of industrial digitalization has arrived,” said Andy Pratt, corporate vice president, Microsoft Emerging Technologies. “With NVIDIA Omniverse APIs on Microsoft Azure, organizations across industries and around the world can connect, collaborate and enhance their existing tools to create the next wave of AI-enabled digital twins.”\nTransforming Industries With Omniverse Digital Twins\nThe new cloud APIs complement the broad adoption of Omniverse by a range of global leaders across industries, including:\nWPP, the world's largest marketing and communications services company, announced a further phase of its Omniverse Cloud-based generative AI content generation engine, bringing the AI-driven solution to the retail and consumer packaged goods sector.\nMedia.Monks announced the adoption of Omniverse to build a generative AI and OpenUSD-enabled content creation pipeline to unlock scale and hyper-personalization across any customer journey.\nContinental, a major automotive supplier, is developing a digital twin platform to optimize its factory operations and accelerate its time-to-market.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Shining Brighter Together: Google’s Gemma Optimized to Run on NVIDIA GPUs",
    "link": "https://blogs.nvidia.com/blog/google-gemma-llm-rtx-ai-pc/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNHpkMjl2UTIxQlpqWnBWamhQVFJDakFSaTBBaWdCTWdtUnNaWm9QbWM5N3dF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-21T08:00:00.000Z",
    "time": "Feb 21",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Financial Results for Second Quarter Fiscal 2024",
    "link": "https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-second-quarter-fiscal-2024",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDRNWHBrZVRnMk0ycEVlSFJJVFJDM0FSaVRBaWdCTWdZNUlZZlMxQUk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-23T07:00:00.000Z",
    "time": "Aug 23, 2023",
    "articleType": "regular",
    "content": "Record revenue of $13.51 billion, up 88% from Q1, up 101% from year ago\nRecord Data Center revenue of $10.32 billion, up 141% from Q1, up 171% from year ago\nNVIDIA (NASDAQ: NVDA) today reported revenue for the second quarter ended July 30, 2023, of $13.51 billion, up 101% from a year ago and up 88% from the previous quarter.\nGAAP earnings per diluted share for the quarter were $2.48, up 854% from a year ago and up 202% from the previous quarter. Non-GAAP earnings per diluted share were $2.70, up 429% from a year ago and up 148% from the previous quarter.\n“A new computing era has begun. Companies worldwide are transitioning from general-purpose to accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\n“NVIDIA GPUs connected by our Mellanox networking and switch technologies and running our CUDA AI software stack make up the computing infrastructure of generative AI.\n“During the quarter, major cloud service providers announced massive NVIDIA H100 AI infrastructures. Leading enterprise IT system and software providers announced partnerships to bring NVIDIA AI to every industry. The race is on to adopt generative AI,” he said.\nDuring the second quarter of fiscal 2024, NVIDIA returned $3.38 billion to shareholders in the form of 7.5 million shares repurchased for $3.28 billion, and cash dividends. As of the end of the second quarter, the company had $3.95 billion remaining under its share repurchase authorization. On August 21, 2023, the Board of Directors approved an additional $25.00 billion in share repurchases, without expiration. NVIDIA plans to continue share repurchases this fiscal year.\nNVIDIA will pay its next quarterly cash dividend of $0.04 per share on September 28, 2023, to all shareholders of record on September 7, 2023.\n($ in millions, except earnings\n($ in millions, except earnings\nNVIDIA’s outlook for the third quarter of fiscal 2024 is as follows:\nRevenue is expected to be $16.00 billion, plus or minus 2%.\nGAAP and non-GAAP gross margins are expected to be 71.5% and 72.5%, respectively, plus or minus 50 basis points.\nGAAP and non-GAAP operating expenses are expected to be approximately $2.95 billion and $2.00 billion, respectively.\nGAAP and non-GAAP other income and expense are expected to be an income of approximately $100 million, excluding gains and losses from non-affiliated investments.\nGAAP and non-GAAP tax rates are expected to be 14.5%, plus or minus 1%, excluding any discrete items.\nNVIDIA achieved progress since its previous earnings announcement in these areas:\nSecond-quarter revenue was a record $10.32 billion, up 141% from the previous quarter and up 171% from a year ago.\nAnnounced that the NVIDIA® GH200 Grace™ Hopper™ Superchip for complex AI and HPC workloads is shipping this quarter, with a second-generation version with HBM3e memory expected to ship in Q2 of calendar 2024.\nAnnounced the NVIDIA L40S GPU — a universal data center processor designed to accelerate the most compute-intensive applications — available from leading server makers in a broad range of platforms, including NVIDIA OVX™ and NVIDIA AI-ready servers with NVIDIA BlueField® DPUs, beginning this quarter.\nUnveiled NVIDIA MGX™, a server reference design available this quarter that lets system makers quickly and cost-effectively build more than 100 server variations for AI, HPC and NVIDIA Omniverse™ applications.\nAnnounced NVIDIA Spectrum-X™, an accelerated networking platform designed to improve the performance and efficiency of Ethernet-based AI clouds, which is shipping this quarter.\nJoined with global system makers to announce new NVIDIA RTX™ workstations with up to four new NVIDIA RTX 6000 Ada GPUs, as well as NVIDIA AI Enterprise and NVIDIA Omniverse Enterprise software, expected to ship this quarter.\nLaunched general availability of cloud instances based on NVIDIA H100 Tensor Core GPUs with Amazon Web Services, Microsoft Azure and regional cloud service providers.\nPartnered with a range of companies on AI initiatives, including:\nServiceNow and Accenture to develop AI Lighthouse, a first-of-its-kind program to fast-track the development and adoption of enterprise generative AI capabilities.\nVMware to extend the companies’ strategic partnership to ready enterprises running VMware’s cloud infrastructure for the era of generative AI with VMware Private AI Foundation with NVIDIA.\nSnowflake to provide businesses with an accelerated path to create customized generative AI applications using their own proprietary data.\nWPP to develop a generative AI-enabled content engine that lets creative teams produce high-quality commercial content faster, more efficiently and at scale while staying fully aligned with a client’s brand.\nSoftBank to create a platform for generative AI and 5G/6G applications based on the GH200, which SoftBank plans to roll out at new, distributed AI data centers across Japan.\nHugging Face to give developers access to NVIDIA DGX™ Cloud AI supercomputing within the Hugging Face platform to train and tune advanced AI models.\nAnnounced NVIDIA AI Workbench, an easy-to-use toolkit allowing developers to quickly create, test and customize pretrained generative AI models on a PC or workstation and then scale them, as well as NVIDIA AI Enterprise 4.0, the latest version of its enterprise software.\nSet records in the latest MLPerf training benchmarks with H100 GPUs, excelling in a new measure for generative AI.\nSecond-quarter revenue was $2.49 billion, up 11% from the previous quarter and up 22% from a year ago.\nBegan shipping the GeForce RTX™ 4060 family of GPUs, bringing to gamers NVIDIA Ada Lovelace architecture and DLSS, starting at $299.\nAnnounced NVIDIA Avatar Cloud Engine, or ACE, for Games, a custom AI model foundry service using AI-powered natural language interactions to transform games by bringing intelligence to non-playable characters.\nAdded 35 DLSS games, including Diablo IV, Ratchet & Clank: Rift Apart, Baldur’s Gate 3 and F1 23, as well as Portal: Prelude RTX, a path-traced game made by the community using NVIDIA’s RTX Remix creator tool.\nSecond-quarter revenue was $379 million, up 28% from the previous quarter and down 24% from a year ago.\nAnnounced three new desktop workstation RTX GPUs based on the Ada Lovelace architecture — NVIDIA RTX 5000, RTX 4500 and RTX 4000 — to deliver the latest AI, graphics and real-time rendering, which are shipping this quarter.\nAnnounced a major release of the NVIDIA Omniverse platform, with new foundation applications and services for developers and industrial enterprises to optimize and enhance their 3D pipelines with OpenUSD and generative AI.\nJoined with Pixar, Adobe, Apple and Autodesk to form the Alliance for OpenUSD to promote the standardization, development, evolution and growth of Universal Scene Description technology.\nSecond-quarter revenue was $253 million, down 15% from the previous quarter and up 15% from a year ago.\nAnnounced that NVIDIA DRIVE Orin™ is powering the new XPENG G6 Coupe SUV’s intelligent advanced driver assistance system.\nPartnered with MediaTek, which will develop mainstream automotive systems on chips for global OEMs, which integrate new NVIDIA GPU chiplet IP for AI and graphics.\nCommentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available at https://investor.nvidia.com.\nConference Call and Webcast Information\nNVIDIA will conduct a conference call with analysts and investors to discuss its second quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website, https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its third quarter of fiscal 2024.\nTo supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, legal settlement costs, contributions, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\nCONDENSED CONSOLIDATED STATEMENTS OF INCOME\n(In millions, except per share data)\nWeighted average shares used in per share computation:\nCash, cash equivalents and marketable securities\nPrepaid expenses and other current assets\nAccrued and other current liabilities\nTotal liabilities and shareholders' equity\nCONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS\nCash flows from operating activities:\nAdjustments to reconcile net income to net cash provided by operating activities:\n(Gains) losses on investments in non affiliates, net\nChanges in operating assets and liabilities, net of acquisitions:\nPrepaid expenses and other assets\nAccrued liabilities and other current liabilities\nNet cash provided by operating activities\nCash flows from investing activities:\nProceeds from maturities of marketable securities\nProceeds from sales of marketable securities\nPurchase related to property and equipment and intangible assets\nAcquisitions, net of cash acquired\nNet cash provided by (used in) investing activities\nCash flows from financing activities:\nProceeds related to employee stock plans\nPayments related to repurchases of common stock\nPayments related to tax on restricted stock units\nPrincipal payments on property and equipment and intangible assets\nNet cash used in financing activities\nChange in cash, cash equivalents, and restricted cash\nCash, cash equivalents, and restricted cash at beginning of period\nCash, cash equivalents, and restricted cash at end of period\nReconciliation of cash, cash equivalents, and restricted cash to the Condensed Consolidated Balance Sheet:\nRestricted cash, included in prepaid expenses and other current assets\nTotal cash, cash equivalents, and restricted cash\nSupplemental disclosures of cash flow information:\nCash paid for income taxes, net\nRECONCILIATION OF GAAP TO NON-GAAP FINANCIAL MEASURES\n(In millions, except per share data)\nAcquisition-related and other costs (A)\nAcquisition-related and other costs (A)\nTotal impact of non-GAAP adjustments to operating income\nGAAP other income (expense), net\n(Gains) losses from non-affiliated investments\nInterest expense related to amortization of debt discount\nNon-GAAP other income (expense), net\nTotal pre-tax impact of non-GAAP adjustments\nIncome tax impact of non-GAAP adjustments (D)\nDiluted net income per share\nWeighted average shares used in diluted net income per share computation\nGAAP net cash provided by operating activities\nPurchases related to property and equipment and intangible assets\nPrincipal payments on property and equipment and intangible assets\n(A) Acquisition-related and other costs are comprised of amortization of intangible assets, transaction costs, and certain compensation charges and are included in the following line items:\n(B) Stock-based compensation consists of the following:\n(C) Other consists of assets held for sale related adjustments.\n(D) Income tax impact of non-GAAP adjustments, including the recognition of excess tax benefits or deficiencies related to stock-based compensation under GAAP accounting standard (ASU 2016-09).\nRECONCILIATION OF GAAP TO NON-GAAP OUTLOOK\nImpact of stock-based compensation expense, acquisition-related costs, and other costs\nStock-based compensation expense, acquisition-related costs, and other costs",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA RTX 500 and 1000 Professional Ada Generation Laptop GPUs Drive AI-Enhanced Workflows From Anywhere",
    "link": "https://blogs.nvidia.com/blog/rtx-ada-ai-workflows/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHlWMGx1V2kxZlNUUTVOM3BGVFJDakFSaTBBaWdCTWdhcGc1S1BNUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-26T08:00:00.000Z",
    "time": "Feb 26",
    "articleType": "regular",
    "content": "With generative AI and hybrid work environments becoming the new standard, nearly every professional, whether a content creator, researcher or engineer, needs a powerful, AI-accelerated laptop to help users tackle their industry’s toughest challenges — even on the go.\nThe new NVIDIA RTX 500 and 1000 Ada Generation Laptop GPUs will be available in new, highly portable mobile workstations, expanding the NVIDIA Ada Lovelace architecture-based lineup, which includes the RTX 2000, 3000, 3500, 4000 and 5000 Ada Generation Laptop GPUs.\nAI is rapidly being adopted to drive efficiencies across professional design and content creation workflows and everyday productivity applications, underscoring the importance of having powerful local AI acceleration and sufficient processing power in systems.\nThe next generation of mobile workstations with Ada Generation GPUs, including the RTX 500 and 1000 GPUs, will include both a neural processing unit (NPU), a component of the CPU, and an NVIDIA RTX GPU, which includes Tensor Cores for AI processing. The NPU helps offload light AI tasks, while the GPU provides up to an additional 682 TOPS of AI performance for more demanding day-to-day AI workflows.\nThe higher level of AI acceleration delivered by the GPU is useful for tackling a wide range of AI-based tasks, such as video conferencing with high-quality AI effects, streaming videos with AI upscaling, or working faster with generative AI and content creation applications.\nThe new RTX 500 GPU delivers up to 14x the generative AI performance for models like Stable Diffusion, up to 3x faster photo editing with AI and up to 10x the graphics performance for 3D rendering compared with a CPU-only configuration — bringing massive leaps in productivity for traditional and emerging workflows.\nEnhancing Professional Workflows Across Industries\nThe RTX 500 and 1000 GPUs elevate workflows with AI for laptop users everywhere in compact designs. Video editors can streamline tasks such as removing background noise with AI. Graphic designers can bring blurry images to life with AI upscaling. Professionals can work on the go while using AI for higher-quality video conferencing and streaming experiences.\nFor users looking to tap AI for advanced rendering, data science and deep learning workflows, NVIDIA also offers the RTX 2000, 3000, 3500, 4000 and 5000 Ada Generation Laptop GPUs. 3D creators can use AI denoising and deep learning super sampling (DLSS) to visualize photorealistic renders in real time. Businesses can query their internal knowledge base with chatbot-like interfaces using local large language models. And researchers and scientists can experiment with data science, AI model training and tuning, and development projects.\nPerformance and Portability With NVIDIA RTX\nThe RTX 500 and 1000 GPUs, based on the NVIDIA Ada Lovelace architecture, bring the latest advancements to thin and light laptops, including:\nThird-generation RT Cores: Up to 2x the ray tracing performance of the previous generation for high-fidelity, photorealistic rendering.\nFourth-generation Tensor Cores: Up to 2x the throughput of the previous generation, accelerating deep learning training, inferencing and AI-based creative workloads.\nAda Generation CUDA cores: Up to 30% the single-precision floating point (FP32) throughput compared to the previous generation for significant performance improvements in graphics and compute workloads.\nDedicated GPU memory: 4GB GPU memory with the RTX 500 GPU and 6GB with the RTX 1000 GPU allows users to run demanding 3D and AI-based applications, as well as tackle larger projects, datasets and multi-app workflows.\nDLSS 3: Delivers a breakthrough in AI-powered graphics, significantly boosting performance by generating additional high-quality frames.\nAV1 encoder: Eighth-generation NVIDIA encoder, aka NVENC, with AV1 support is up to 40% more efficient than H.264, enabling new possibilities for broadcasting, streaming and video calling.\nThe new NVIDIA RTX 500 and 1000 Ada Generation Laptop GPUs will be available this spring in mobile workstations from global manufacturing partners including Dell Technologies, HP, Lenovo and MSI.",
    "favicon": ""
  },
  {
    "title": "Widescreen Wonder: Las Vegas Sphere Delivers Dazzling Displays",
    "link": "https://blogs.nvidia.com/blog/sphere-las-vegas/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNUhaMGMxTm1vM1FXUkRYMjlDVFJDakFSaTBBaWdCTWdNQjhBZw=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-09T07:00:00.000Z",
    "time": "Jul 9",
    "articleType": "regular",
    "content": "Sphere, a new kind of entertainment medium in Las Vegas, is joining the ranks of legendary circular performance spaces such as the Roman Colosseum and Shakespeare’s Globe Theater — captivating audiences with eye-popping LED displays that cover nearly 750,000 square feet inside and outside the venue.\nBehind the screens, around 150 NVIDIA RTX A6000 GPUs help power stunning visuals on floor-to-ceiling, 16x16K displays across the Sphere’s interior, as well as 1.2 million programmable LED pucks on the venue’s exterior — the Exosphere, which is the world’s largest LED screen.\nDelivering robust network connectivity, NVIDIA BlueField DPUs and NVIDIA ConnectX-6 Dx NICs — along with the NVIDIA DOCA Firefly Service and NVIDIA Rivermax software for media streaming — ensure that all the display panels act as one synchronized canvas.\n“Sphere is captivating audiences not only in Las Vegas, but also around the world on social media, with immersive LED content delivered at a scale and clarity that has never been done before,” said Alex Luthwaite, senior vice president of show systems technology at Sphere Entertainment. “This would not be possible without the expertise and innovation of companies such as NVIDIA that are critical to helping power our vision, working closely with our team to redefine what is possible with cutting-edge display technology.”\nNamed one of TIME’s Best Inventions of 2023, Sphere hosts original Sphere Experiences, concerts and residencies from the world’s biggest artists, and premier marquee and corporate events.\nRock band U2 opened Sphere with a 40-show run that concluded in March. Other shows include The Sphere Experience featuring Darren Aronofsky’s Postcard From Earth, a specially created multisensory cinematic experience that showcases all of the venue’s immersive technologies, including high-resolution visuals, advanced concert-grade sound, haptic seats and atmospheric effects such as wind and scents.\n“Postcard From Earth” is a multisensory immersive experience. Image courtesy of Sphere Entertainment.\nBehind the Screens: Visual Technology Fueling the Sphere\nSphere Studios creates video content in its Burbank, Calif., facility, then transfers it digitally to Sphere in Las Vegas. The content is then streamed in real time to rack-mounted workstations equipped with NVIDIA RTX A6000 GPUs, achieving unprecedented performance capable of delivering three layers of 16K resolution at 60 frames per second.\nThe NVIDIA Rivermax software helps provide media streaming acceleration, enabling direct data transfers to and from the GPU. Combined, the software and hardware acceleration eliminates jitter and optimizes latency.\nNVIDIA BlueField DPUs also facilitate precision timing through the DOCA Firefly Service, which is used to synchronize clocks in a network with sub-microsecond accuracy.\n“The integration of NVIDIA RTX GPUs, BlueField DPUs and Rivermax software creates a powerful trifecta of advantages for modern accelerated computing, supporting the unique high-resolution video streams and strict timing requirements needed at Sphere and setting a new standard for media processing capabilities,” said Nir Nitzani, senior product director for networking software at NVIDIA. “This collaboration results in remarkable performance gains, culminating in the extraordinary experiences guests have at Sphere.”\nWell-Rounded: From Simulation to Sphere Stage\nTo create new immersive content exclusively for Sphere, Sphere Entertainment launched Sphere Studios, which is dedicated to developing the next generation of original immersive entertainment. The Burbank campus consists of numerous development facilities, including a quarter-sized version of Sphere screen in Las Vegas, dubbed Big Dome, which serves as a specialized screening, production facility and lab for content.\nThe Big Dome is 100 feet high and 28,000 square feet. Image courtesy of Sphere Entertainment.\nSphere Studios also developed the Big Sky camera system, which captures uncompressed, 18K images from a single camera, so that the studio can film content for Sphere without needing to stitch multiple camera feeds together. The studio’s custom image processing software runs on Lenovo servers powered by NVIDIA A40 GPUs.\nThe A40 GPUs also fuel creative work, including 3D video, virtualization and ray tracing. To develop visuals for different kinds of shows, the team works with apps including Unreal Engine, Unity, Touch Designer and Notch.\nFor more, explore upcoming sessions in NVIDIA’s room at SIGGRAPH and watch the panel discussion “Immersion in Sphere: Redefining Live Entertainment Experiences” on NVIDIA On-Demand.\nAll images courtesy of Sphere Entertainment.",
    "favicon": ""
  },
  {
    "title": "NVIDIA Brings AI Assistants to Life With GeForce RTX AI PCs",
    "link": "https://nvidianews.nvidia.com/news/nvidia-brings-ai-assistants-to-life-with-geforce-rtx-ai-pcs",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXVaSGh2VDJsaWJpMVFZM0U1VFJDb0FSaXNBaWdCTWdZQk1Jd2JGQW8=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-02T07:00:00.000Z",
    "time": "Jun 2",
    "articleType": "regular",
    "content": "Project G-Assist, NVIDIA ACE NIMs for Digital Humans, and Generative AI Tools Deliver Advanced AI Experiences on RTX Laptops; Plus, RTX-Accelerated APIs for Small Language Models Coming to Windows Copilot Runtime\nCOMPUTEX—NVIDIA today announced new NVIDIA RTX™ technology to power AI assistants and digital humans running on new GeForce RTX™ AI laptops.\nNVIDIA unveiled Project G-Assist — an RTX-powered AI assistant technology demo that provides context-aware help for PC games and apps. The Project G-Assist tech demo debuted with ARK: Survival Ascended from Studio Wildcard. NVIDIA also introduced the first PC-based NVIDIA NIM™ inference microservices for the NVIDIA ACE digital human platform.\nThese technologies are enabled by the NVIDIA RTX AI Toolkit, a new suite of tools and software development kits that aid developers in optimizing and deploying large generative AI models on Windows PCs. They join NVIDIA’s full-stack RTX AI innovations accelerating over 500 PC applications and games and 200 laptop designs from manufacturers.\nIn addition, newly announced RTX AI PC laptops from ASUS and MSI feature up to GeForce RTX 4070 GPUs and power-efficient systems-on-a-chip with Windows 11 AI PC capabilities. These Windows 11 AI PCs will receive a free update to Copilot+ PC experiences when available.\n“NVIDIA launched the era of AI PCs in 2018 with the release of RTX Tensor Core GPUs and NVIDIA DLSS,” said Jason Paul, vice president of consumer AI at NVIDIA. “Now, with Project G-Assist and NVIDIA ACE, we’re unlocking the next generation of AI-powered experiences for over 100 million RTX AI PC users.”\nProject G-Assist, a GeForce AI Assistant\nAI assistants are set to transform gaming and in-app experiences — from offering gaming strategies and analyzing multiplayer replays to assisting with complex creative workflows. Project G-Assist is a glimpse into this future.\nPC games offer vast universes to explore and intricate mechanics to master, which are challenging and time-consuming feats even for the most dedicated gamers. Project G-Assist aims to put game knowledge at players’ fingertips using generative AI.\nProject G-Assist takes voice or text inputs from the player, along with contextual information from the game screen, and runs the data through AI vision models. These models enhance the contextual awareness and app-specific understanding of a large language model (LLM) linked to a game knowledge database, and then generate a tailored response delivered as text or speech.\nNVIDIA partnered with Studio Wildcard to demo the technology with ARK: Survival Ascended. Project G-Assist can help answer questions about creatures, items, lore, objectives, difficult bosses and more. Because Project G-Assist is context-aware, it personalizes its responses to the player’s game session.\nIn addition, Project G-Assist can configure the player’s gaming system for optimal performance and efficiency. It can provide insights into performance metrics, optimize graphics settings depending on the user’s hardware, apply a safe overclock and even intelligently reduce power consumption while maintaining a performance target.\nFirst ACE PC NIM Debuts\nNVIDIA ACE technology for powering digital humans is now coming to RTX AI PCs and workstations with NVIDIA NIM — inference microservices that enable developers to reduce deployment times from weeks to minutes. ACE NIM microservices deliver high-quality inference running locally on devices for natural language understanding, speech synthesis, facial animation and more.\nAt COMPUTEX, the gaming debut of NVIDIA ACE NIM on the PC will be featured in the Covert Protocol tech demo, developed in collaboration with Inworld AI. It now showcases NVIDIA Audio2Face™ and NVIDIA Riva automatic speech recognition running locally on devices.\nWindows Copilot Runtime to Add GPU Acceleration for Local PC SLMs\nMicrosoft and NVIDIA are collaborating to help developers bring new generative AI capabilities to their Windows native and web apps. This collaboration will provide application developers with easy application programming interface (API) access to GPU-accelerated small language models (SLMs) that enable retrieval-augmented generation (RAG) capabilities that run on-device as part of Windows Copilot Runtime.\nSLMs provide tremendous possibilities for Windows developers, including content summarization, content generation and task automation. RAG capabilities augment SLMs by giving the AI models access to domain-specific information not well represented in ‌base models. RAG APIs enable developers to harness application-specific data sources and tune SLM behavior and capabilities to application needs.\nThese AI capabilities will be accelerated by NVIDIA RTX GPUs, as well as AI accelerators from other hardware vendors, providing end users with fast, responsive AI experiences across the breadth of the Windows ecosystem.\nThe API will be released in developer preview later this year.\n4x Faster, 3x Smaller Models With the RTX AI Toolkit\nThe AI ecosystem has built hundreds of thousands of open-source models for app developers to leverage, but most models are pretrained for general purposes and built to run in a data center.\nTo help developers build application-specific AI models that run on PCs, NVIDIA is introducing RTX AI Toolkit — a suite of tools and SDKs for model customization, optimization and deployment on RTX AI PCs. RTX AI Toolkit will be available later this month for broader developer access.\nDevelopers can customize a pretrained model with open-source QLoRa tools. Then, they can use the NVIDIA TensorRT™ model optimizer to quantize models to consume up to 3x less RAM. NVIDIA TensorRT Cloud then optimizes the model for peak performance across the RTX GPU lineups. The result is up to 4x faster performance compared with the pretrained model.\nSoftware partners such as Adobe, Blackmagic Design and Topaz are integrating components of the RTX AI Toolkit within their popular creative apps to accelerate AI performance on RTX PCs.\n“Adobe and NVIDIA continue to collaborate to deliver breakthrough customer experiences across all creative workflows, from video to imaging, design, 3D and beyond,” said Deepa Subramaniam, vice president of product marketing, Creative Cloud at Adobe. “TensorRT 10.0 on RTX PCs delivers unprecedented performance and AI-powered capabilities for creators, designers and developers, unlocking new creative possibilities for content creation in industry-leading creative tools like Photoshop.”\nComponents of the RTX AI Toolkit, such as TensorRT-LLM, are integrated in popular developer frameworks and applications for generative AI, including Automatic1111, ComfyUI, Jan.AI, LangChain, LlamaIndex, Oobabooga and Sanctum.AI.\nNVIDIA is also integrating RTX AI acceleration into apps for creators, modders and video enthusiasts.\nLast year, NVIDIA introduced RTX acceleration using TensorRT for one of the most popular Stable Diffusion user interfaces, Automatic1111. Starting this week, RTX will also accelerate the highly popular ComfyUI, delivering up to a 60% improvement in performance over the currently shipping version, and 7x faster performance compared with the MacBook Pro M3 Max.\nNVIDIA RTX Remix is a modding platform for remastering classic DirectX 8 and DirectX 9 games with full ray tracing, NVIDIA DLSS 3.5 and physically accurate materials. RTX Remix includes a runtime renderer and the RTX Remix Toolkit app, which facilitates the modding of game assets and materials.\nLast year, NVIDIA made RTX Remix Runtime open source, allowing modders to expand game compatibility and advance rendering capabilities.\nSince RTX Remix Toolkit launched earlier this year, 20,000 modders have used it to mod classic games, resulting in over 100 RTX remasters in development on the RTX Remix Showcase Discord.\nThis month, NVIDIA will make the RTX Remix Toolkit open source, allowing modders to streamline how assets are replaced and scenes are relit, increase supported file formats for RTX Remix’s asset ingestor and bolster RTX Remix’s AI Texture Tools with new models.\nIn addition, NVIDIA is making the capabilities of RTX Remix Toolkit accessible via a REST API, allowing modders to livelink RTX Remix to digital content creation tools such as Blender, modding tools such as Hammer and generative AI apps such as ComfyUI. NVIDIA is also providing an SDK for RTX Remix Runtime to allow modders to deploy RTX Remix’s renderer into other applications and games beyond DirectX 8 and 9 classics.\nWith more of the RTX Remix platform being made open source, modders across the globe can build even more stunning RTX remasters.\nNVIDIA RTX Video, the popular AI-powered super-resolution feature supported in the Google Chrome, Microsoft Edge and Mozilla Firefox browsers, is now available as an SDK to all developers, helping them natively integrate AI for upscaling, sharpening, compression artifact reduction and high-dynamic range (HDR) conversion.\nComing soon to video editing software Blackmagic Design’s DaVinci Resolve and Wondershare Filmora, RTX Video will enable video editors to upscale lower-quality video files to 4K resolution, as well as convert standard dynamic range source files into HDR. In addition, the free media player VLC media will soon add RTX Video HDR to its existing super-resolution capability.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "New Performance Optimizations Supercharge NVIDIA RTX AI PCs for Gamers, Creators and Developers",
    "link": "https://blogs.nvidia.com/blog/rtx-advanced-ai-windows-pc-build/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNW9Uell5ZG14MlRsZHdWWGxEVFJDakFSaTBBaWdCTWdrRnNaVEVPbWN0endF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-21T07:00:00.000Z",
    "time": "May 21",
    "articleType": "regular",
    "content": "NVIDIA today announced at Microsoft Build new AI performance optimizations and integrations for Windows that help deliver maximum performance on NVIDIA GeForce RTX AI PCs and NVIDIA RTX workstations.\nLarge language models (LLMs) power some of the most exciting new use cases in generative AI and now run up to 3x faster with ONNX Runtime (ORT) and DirectML using the new NVIDIA R555 Game Ready Driver. ORT and DirectML are high-performance tools used to run AI models locally on Windows PCs.\nWebNN, an application programming interface for web developers to deploy AI models, is now accelerated with RTX via DirectML, enabling web apps to incorporate fast, AI-powered capabilities. And PyTorch will support DirectML execution backends, enabling Windows developers to train and infer complex AI models on Windows natively. NVIDIA and Microsoft are collaborating to scale performance on RTX GPUs.\nThese advancements build on NVIDIA’s world-leading AI platform, which accelerates more than 500 applications and games on over 100 million RTX AI PCs and workstations worldwide.\nNVIDIA introduced the first PC GPUs with dedicated AI acceleration, the GeForce RTX 20 Series with Tensor Cores, along with the first widely adopted AI model to run on Windows, NVIDIA DLSS, in 2018. Its latest GPUs offer up to 1,300 trillion operations per second of dedicated AI performance.\nIn the coming months, Copilot+ PCs equipped with new power-efficient systems-on-a-chip and RTX GPUs will be released, giving gamers, creators, enthusiasts and developers increased performance to tackle demanding local AI workloads, along with Microsoft’s new Copilot+ features.\nFor gamers on RTX AI PCs, NVIDIA DLSS boosts frame rates by up to 4x, while NVIDIA ACE brings game characters to life with AI-driven dialogue, animation and speech.\nFor content creators, RTX powers AI-assisted production workflows in apps like Adobe Premiere, Blackmagic Design DaVinci Resolve and Blender to automate tedious tasks and streamline workflows. From 3D denoising and accelerated rendering to text-to-image and video generation, these tools empower artists to bring their visions to life.\nFor game modders, NVIDIA RTX Remix, built on the NVIDIA Omniverse platform, provides AI-accelerated tools to create RTX remasters of classic PC games. It makes it easier than ever to capture game assets, enhance materials with generative AI tools and incorporate full ray tracing.\nFor livestreamers, the NVIDIA Broadcast application delivers high-quality AI-powered background subtraction and noise removal, while NVIDIA RTX Video provides AI-powered upscaling and auto-high-dynamic range to enhance streamed video quality.\nEnhancing productivity, LLMs powered by RTX GPUs execute AI assistants and copilots faster, and can process multiple requests simultaneously.\nAnd RTX AI PCs allow developers to build and fine-tune AI models directly on their devices using NVIDIA’s AI developer tools, which include NVIDIA AI Workbench, NVIDIA cuDNN and CUDA on Windows Subsystem for Linux. Developers also have access to RTX-accelerated AI frameworks and software development kits like NVIDIA TensorRT, NVIDIA Maxine and RTX Video.\nThe combination of AI capabilities and performance deliver enhanced experiences for gamers, creators and developers.\nFaster LLMs and New Capabilities for Web Developers\nMicrosoft recently released the generative AI extension for ORT, a cross-platform library for AI inference. The extension adds support for optimization techniques like quantization for LLMs like Phi-3, Llama 3, Gemma and Mistral. ORT supports different execution providers for inferencing via various software and hardware stacks, including DirectML.\nORT with the DirectML backend offers Windows AI developers a quick path to develop AI capabilities, with stability and production-grade support for the broad Windows PC ecosystem. NVIDIA optimizations for the generative AI extension for ORT, available now in R555 Game Ready, Studio and NVIDIA RTX Enterprise Drivers, help developers get up to 3x faster performance on RTX compared to previous drivers.\nInference performance for three LLMs using ONNX Runtime and the DirectML execution provider with the latest R555 GeForce driver compared to the previous R550 driver. INSEQ=2000 representative of document summarization workloads. All data captured with GeForce RTX 4090 GPU using batch size 1. The generative AI extension support for int4 quantization, plus the NVIDIA optimizations, result in up to 3x faster performance for LLMs.\nDevelopers can unlock the full capabilities of RTX hardware with the new R555 driver, bringing better AI experiences to consumers, faster. It includes:\nSupport for DQ-GEMM metacommand to handle INT4 weight-only quantization for LLMs\nNew RMSNorm normalization methods for Llama 2, Llama 3, Mistral and Phi-3 models\nGroup and multi-query attention mechanisms, and sliding window attention to support Mistral\nIn-place KV updates to improve attention performance\nSupport for GEMM of non-multiple-of-8 tensors to improve context phase performance\nAdditionally, NVIDIA has optimized AI workflows within WebNN to deliver the powerful performance of RTX GPUs directly within browsers. The WebNN standard helps web app developers accelerate deep learning models with on-device AI accelerators, like Tensor Cores.",
    "favicon": ""
  },
  {
    "title": "GeForce RTX 40 SUPER Series: New Heroes Debut in the Gaming and Creating Universe With AI as Their Superpower",
    "link": "https://nvidianews.nvidia.com/news/geforce-rtx-40-super-series",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVFWbTlZWDNsYVJHeHdaV0pqVFJDb0FSaXNBaWdCTWdZZGtvNU9PUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "Gaming GPUs Amplified With More Performance and Generative AI Capabilities, Starting at $599\nCES — NVIDIA today announced the GeForce RTX™ 40 SUPER Series family of GPUs — including the GeForce RTX 4080 SUPER, GeForce RTX 4070 Ti SUPER and GeForce RTX 4070 SUPER — which supercharge the latest games and form the core of AI-powered PCs.\nThis latest iteration of NVIDIA Ada Lovelace architecture-based GPUs delivers up to 52 shader TFLOPS, 121 RT TFLOPS and 836 AI TOPS to supercharge gaming and creating — and provide the power to develop new entertainment worlds and experiences. The GeForce RTX 4070 SUPER starts from $599.\nPC gamers demand the very best in visual quality, and AI-powered NVIDIA Deep Learning Super Sampling (DLSS) Super Resolution, Frame Generation and Ray Reconstruction combine with ray tracing to offer stunning worlds — just a click away in titles such as Diablo IV, Pax Dei and Horizon Forbidden West. With DLSS, seven out of eight pixels can be AI-generated, accelerating full ray tracing by up to 4x with better image quality.\n“For everyone from gaming enthusiasts to creative professionals, GeForce RTX SUPER GPUs are simply awesome upgrades,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “GeForce RTX SUPER cards support over 500 RTX games and applications and will have users prepared for the wave of generative AI apps coming to PC.”\nAn AI-Powered Leap in PC Computing\nThe new GeForce RTX SUPER GPUs are the ultimate way to experience AI on PCs. Specialized AI Tensor Cores deliver up to 836 AI TOPS to deliver transformative capabilities for AI in gaming, creating and everyday productivity. The rich software stack built on top of RTX GPUs further accelerates AI.\nNVIDIA TensorRT™ is software for high-performance deep learning inference, which includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications. TensorRT-LLM for Windows is an open-source library that accelerates inference performance for the latest large language models. In AI workloads, the GeForce RTX 4080 SUPER generates video over 1.5x faster and images over 1.7x faster than the RTX 3080 Ti.\nFor games, AI-powered DLSS provides greater in-game immersion. Meanwhile, generative AI applications like Adobe Photoshop take advantage of Tensor Cores to speed productivity and keep creative workflows moving. And for productivity, NVIDIA Broadcast can remove background noise and provide seamless virtual backgrounds.\nWith GeForce RTX SUPER GPUs, users can unlock the full potential of AI on Windows PCs.\nA 4K Monster: The GeForce RTX 4080 SUPER\nThe GeForce RTX 4080 SUPER powers fully ray-traced games in 4K resolution. At 1.4x faster than the GeForce RTX 3080 Ti without DLSS Frame Generation, the RTX 4080 SUPER delivers blistering performance with traditional rasterization. With 836 AI TOPS, DLSS Frame Generation delivers an extra performance boost, making the RTX 4080 SUPER twice as fast as the RTX 3080 Ti. The RTX 4080 SUPER features more cores and faster memory for a performance edge. It will be available starting Jan. 31 from $999.\nPrecision Gaming: The GeForce RTX 4070 Ti SUPER\nThe RTX 4070 Ti SUPER is the ideal GPU for maxing out games at super-high frame rates at 1440p, and up to 4K. Compared to the RTX 4070 Ti, it has more cores, an increased frame buffer to 16GB, and a 256-bit memory bus, providing a significant memory bandwidth increase to 672 GB/sec. It is 1.6x faster than a RTX 3070 Ti and 2.5x with DLSS 3. The GeForce RTX 4070 Ti SUPER will be available starting Jan. 24 at $799.\nPerfectly Balanced: The GeForce RTX 4070 SUPER\nThe RTX 4070 SUPER arrives with 20% more cores than the RTX 4070, making it faster than an RTX 3090 at a fraction of the power. With DLSS 3, its lead stretches to 1.5x faster. It will be available starting Jan. 17 at $599.\nFor the GeForce RTX 4080 SUPER and 4070 SUPER, an NVIDIA Founders Edition Design will be available direct from NVIDIA.com and select retailers. Custom boards, including stock-clocked and factory-overclocked models for all GeForce RTX 40 SUPER Series GPUs, will be available from top add-in card providers such as ASUS, Colorful, Gainward, GALAX, GIGABYTE, INNO3D, KFA2, MSI, Palit, PNY and ZOTAC.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Fire It Up: Mozilla Firefox Adds Support for AI-Powered NVIDIA RTX Video",
    "link": "https://blogs.nvidia.com/blog/ai-decoded-rtxvideo-firefox/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDJMVEV5UVUxMGNtZDZOMVYxVFJDakFSaTBBaWdCTWdZdGhaRE9zUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-15T07:00:00.000Z",
    "time": "May 15",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Eureka! NVIDIA Research Breakthrough Puts New Spin on Robot Learning",
    "link": "https://blogs.nvidia.com/blog/eureka-robotics-research/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTFVVWxLUkdwRVdpMWhValJqVFJDakFSaTBBaWdCTWdhdE5aRE9uUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-10-20T07:00:00.000Z",
    "time": "Oct 20, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2024",
    "link": "https://investor.nvidia.com/news/press-release-details/2024/NVIDIA-Announces-Financial-esults-for-Fourth-Quarter-and-Fiscal-2024/R",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXVkV1F5TWxaMWNtMWxRWEprVFJDM0FSaVRBaWdCTWdZNUlZZlMxQUk=-w400-h224-p-df-rw",
    "source": "NVIDIA Newsroom",
    "datetime": "2024-02-21T08:00:00.000Z",
    "time": "Feb 21",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Brings Generative AI to Millions, With Tensor Core GPUs, LLMs, Tools for RTX PCs and Workstations",
    "link": "https://nvidianews.nvidia.com/news/generative-ai-rtx-pcs-and-workstations",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHhaVmRpUjBWV1FrTnpNRUZqVFJDb0FSaXNBaWdCTWdhdGc1S1BNUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "Leading AI Platform Gets RTX-Accelerated Boost From New GeForce RTX SUPER GPUs, AI Laptops From Every Top Manufacturer\nCES — NVIDIA today announced GeForce RTX™ SUPER desktop GPUs for supercharged generative AI performance, new AI laptops from every top manufacturer, and new NVIDIA RTX™-accelerated AI software and tools for both developers and consumers.\nBuilding on decades of PC leadership, with over 100 million of its RTX GPUs driving the AI PC era, NVIDIA is now offering these tools to enhance PC experiences with generative AI: NVIDIA TensorRT™ acceleration of the popular Stable Diffusion XL model for text-to-image workflows, NVIDIA RTX Remix with generative AI texture tools, NVIDIA ACE microservices and more games that use DLSS 3 technology with Frame Generation.\nAI Workbench, a unified, easy-to-use toolkit for AI developers, will be available in beta later this month. In addition, NVIDIA TensorRT-LLM (TRT-LLM), an open-source library that accelerates and optimizes inference performance of the latest large language models (LLMs), now supports more pre-optimized models for PCs. Accelerated by TRT-LLM, Chat with RTX, an NVIDIA tech demo also releasing this month, allows AI enthusiasts to interact with their notes, documents and other content.\n“Generative AI is the single most significant platform transition in computing history and will transform every industry, including gaming,” said Jensen Huang, founder and CEO of NVIDIA. “With over 100 million RTX AI PCs and workstations, NVIDIA is a massive installed base for developers and gamers to enjoy the magic of generative AI.”\nRunning generative AI locally on a PC is critical for privacy, latency and cost-sensitive applications. It requires a large installed base of AI-ready systems, as well as the right developer tools to tune and optimize AI models for the PC platform.\nTo meet these needs, NVIDIA is delivering innovations across its full technology stack, driving new experiences and building on the 500+ AI-enabled PC applications and games already accelerated by NVIDIA RTX technology.\nRTX AI PCs and Workstations\nNVIDIA RTX GPUs — capable of running a broad range of applications at the highest performance — unlock the full potential of generative AI on PCs. Tensor Cores in these GPUs dramatically speed AI performance across the most demanding applications for work and play.\nThe new GeForce RTX 40 SUPER Series graphics cards, also announced today at CES, include the GeForce RTX 4080 SUPER, 4070 Ti SUPER and 4070 SUPER for top AI performance. The GeForce RTX 4080 SUPER generates AI video 1.5x faster — and images 1.7x faster — than the GeForce RTX 3080 Ti GPU. The Tensor Cores in SUPER GPUs deliver up to 836 trillion operations per second, bringing transformative AI capabilities to gaming, creating and everyday productivity.\nLeading manufacturers — including Acer, ASUS, Dell, HP, Lenovo, MSI, Razer and Samsung — are releasing a new wave of RTX AI laptops, bringing a full set of generative AI capabilities to users right out of the box. The new systems, which deliver a performance increase ranging from 20x-60x compared with using neural processing units, will start shipping this month.\nMobile workstations with RTX GPUs can run NVIDIA AI Enterprise software, including TensorRT and NVIDIA RAPIDS™ for simplified, secure generative AI and data science development. A three-year license for NVIDIA AI Enterprise is included with every NVIDIA A800 40GB Active GPU, providing an ideal workstation development platform for AI and data science.\nNew PC Developer Tools for Building AI Models\nTo help developers quickly create, test and customize pretrained generative AI models and LLMs using PC-class performance and memory footprint, NVIDIA recently announced NVIDIA AI Workbench.\nAI Workbench, which will be available in beta later this month, offers streamlined access to popular repositories like Hugging Face, GitHub and NVIDIA NGC™, along with a simplified user interface that enables developers to easily reproduce, collaborate on and migrate projects.\nProjects can be scaled out to virtually anywhere — whether the data center, a public cloud or NVIDIA DGX™ Cloud — and then brought back to local RTX systems on a PC or workstation for inference and light customization.\nIn collaboration with HP, NVIDIA is also simplifying AI model development by integrating NVIDIA AI Foundation Models and Endpoints, which include RTX-accelerated AI models and software development kits, into the HP AI Studio, a centralized platform for data science. This will allow users to easily search, import and deploy optimized models across PCs and the cloud.\nAfter building AI models for PC use cases, developers can optimize them using NVIDIA TensorRT to take full advantage of RTX GPUs’ Tensor Cores.\nNVIDIA recently extended TensorRT to text-based applications with TensorRT-LLM for Windows, an open-source library for accelerating LLMs. The latest update to TensorRT-LLM, available now, adds Phi-2 to the growing list of pre-optimized models for PC, which run up to 5x faster compared to other inference backends.\nRTX-Accelerated Generative AI Powers New PC Experiences\nAt CES, NVIDIA and its developer partners are releasing new generative AI-powered applications and services for PCs, including:\nNVIDIA RTX Remix, a platform for creating stunning RTX remasters of classic games. Releasing in beta later this month, it delivers generative AI tools that can transform basic textures from classic games into modern, 4K-resolution, physically based rendering materials.\nNVIDIA ACE microservices, including generative AI-powered speech and animation models, which enable developers to add intelligent, dynamic digital avatars to games.\nTensorRT acceleration for Stable Diffusion XL (SDXL) Turbo and latent consistency models, two of the most popular Stable Diffusion acceleration methods. TensorRT improves performance for both by up to 60% compared with the previous fastest implementation. An updated version of the Stable Diffusion WebUI TensorRT extension is also now available, including acceleration for SDXL, SDXL Turbo, LCM - Low-Rank Adaptation (LoRA) and improved LoRA support.\nNVIDIA DLSS 3 with Frame Generation, which uses AI to increase frame rates up to 4x compared with native rendering, will be featured in a dozen of the 14 new RTX games announced, including Horizon Forbidden West, Pax Dei and Dragon’s Dogma 2.\nChat with RTX, an NVIDIA tech demo available later this month, allows AI enthusiasts to easily connect PC LLMs to their own data using a popular technique known as retrieval-augmented generation (RAG). The demo, accelerated by TensorRT-LLM, enables users to quickly interact with their notes, documents and other content. It will also be available as an open-source reference project, so developers can easily implement the same capabilities in their own applications.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Robotics Adopted by Industry Leaders for Development of Tens of Millions of AI-Powered Autonomous Machines",
    "link": "https://nvidianews.nvidia.com/news/robotics-industry-development-ai-autonomous-machines",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDBiMnd3WHpaamRYQm1lVlp3VFJDb0FSaXNBaWdCTWdZQlFJaU1wUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-02T07:00:00.000Z",
    "time": "Jun 2",
    "articleType": "regular",
    "content": "BYD Electronics, Siemens, Teradyne Robotics and Intrinsic, an Alphabet Company, Using NVIDIA Isaac Robotics Platform for Autonomous Robot Arms, Humanoids, Mobile Robots\nCOMPUTEX—NVIDIA today announced that the world’s leaders in robot development are adopting the NVIDIA Isaac™ robotics platform for the research, development and production of the next generation of AI-enabled autonomous machines and robots.\nBYD Electronics, Siemens, Teradyne Robotics and Intrinsic, an Alphabet company, are among more than a dozen robotics industry leaders globally that are integrating NVIDIA Isaac accelerated libraries, physically based simulation and AI models into their software frameworks and robot models to make factories, warehouses and distribution centers highly efficient and safer for their human coworkers, and act as intelligent assistants for repetitive or ultra-precise tasks.\n“The era of robotics has arrived. Everything that moves will one day be autonomous,” said Jensen Huang, founder and CEO of NVIDIA. “We are working to accelerate generative physical AI by advancing the NVIDIA robotics stack, including Omniverse for simulation applications, Project GR00T humanoid foundation models and the Jetson Thor robotics computer.”\nThe Isaac platform features a suite of NVIDIA-accelerated libraries, AI foundation models and simulation technologies that are available today to robot makers to integrate into their technology stacks.\nNVIDIA Isaac ROS — a collection of modular ROS 2 packages that brings NVIDIA-acceleration and AI models to ROS community developers.\nNVIDIA Isaac Perceptor — a reference workflow built on Isaac ROS that provides multi-camera, 3D surround-vision capabilities for AI-based autonomous mobile robots.\nNVIDIA Isaac Manipulator — a reference workflow built on Isaac ROS that simplifies development of AI-enabled robot arms, or manipulators, that can seamlessly perceive, understand and interact with their environments.\nNVIDIA Isaac Sim™ — a reference application for simulating, testing and validating robots in physically based environments, and for generating synthetic data, based on the NVIDIA Omniverse™ platform.\nNVIDIA Isaac Lab — a reference application in Isaac Sim optimized for reinforcement, imitation and transfer learning for AI robot foundation model training.\nNVIDIA Isaac’s early adopters are leaders in robotics and autonomous machine development across Asia, Europe and North America.\nSiemens, global leader in industrial automation software and systems, is using NVIDIA Isaac Sim for its powerful software-in-the-loop capabilities. The Isaac technologies accelerate Siemens development and testing of advanced robotics skills such as SIMATIC Robot PickAI (PRO) and SIMATIC Robot Pack AI. The AI vision software provides cognitive AI-driven capabilities and enables industrial robot systems to autonomously and reliably pick-and-pack arbitrary items without any prior training of the AI by the user. The companies plan to expand their partnership and announce new capabilities later this year at the SPS Expo.\nSiemens delivers industrial-grade AI and is pushing it to the forefront of robotics by seamlessly integrating with automation solutions and making it easy to use when deployed on a NVIDIA-powered Siemens industrial PC foundation, bringing vision AI to the ecosystem of industrial robots.\n“AI-powered robots will accelerate the digital transformation of industry and take over repetitive tasks that were previously impossible to automate so we can unlock human potential for more creative and valuable work,” said Roland Busch, president and CEO at Siemens AG. “Together with NVIDIA, Siemens is empowering our customers and partners to use AI to create new innovations, incorporate them as part of their industrial automation solutions and drive efficiency and competitive advantage.”\nIntrinsic, a software and AI robotics subsidiary of Alphabet that acquired the Open Source Robotics Corporation in late 2022, has successfully tested Isaac Manipulator in its robot-agnostic software platform. Intrinsic has demonstrated, using Manipulator, the potential for a scalable, universally applicable robotic-grasping skill to work across grippers, environments and objects.\n“We couldn’t have found a better collaborator in NVIDIA, who are helping to pave the way for foundation models to have a profound impact on industrial robotics,” said Wendy Tan White, CEO of Intrinsic. “As our teams work together on integrating NVIDIA Isaac and Intrinsic’s platform, the potential value we can unlock for millions of developers and businesses is immense.”\nBYD Group has a strong manufacturing footprint across four major industries, including electronics, automotive, new energy and rail transportation worldwide. Its one subsidiary, BYD Electronics (BYDE), a global leading provider of high-tech and innovative products, is developing a full range of autonomous mobile robots that provide factories with complete logistics solutions using NVIDIA Isaac Sim and Isaac Perceptor.\n“BYDE has a strong focus on helping customers accelerate deployment of logistics applications,” said Chris Yotive, senior business development director of BYD Electronics. “In collaboration with NVIDIA, we have developed advanced autonomous mobile robots powered by NVIDIA Isaac that will improve worker safety, reduce production costs and enhance production intelligence for our customers.”\nUniversal Robots (UR) and Mobile Industrial Robots (MiR), Teradyne Robotics companies, are using NVIDIA Isaac to integrate AI into automation. UR is integrating Isaac Manipulator into its PolyScope X software platform to unlock new cobot solutions. MiR is leveraging Isaac Sim to generate synthetic data and simulate its MiR1200 Pallet Jack for real-world deployments.\n“The key to tackling our customers’ challenges in robotics lies in the industry’s ability to work together, in one collective effort,” said Ujjwal Kumar, group president of Teradyne Robotics. “With NVIDIA Isaac’s advanced AI and simulation capabilities plugged into our large installed base of autonomous mobile robots and cobots, we will push the envelope of innovation to achieve swift solutions for multiple industries.”\nThe NVIDIA Isaac platform is modular, enabling companies to adopt individual or several technologies together.\nCompanies leveraging Isaac Perceptor for development of advanced perception-based autonomous mobile robots include: ArcBest, BYD Electronics, Gideon, idealworks and RGo Robotics.\nCompanies leveraging Isaac Manipulator for building AI-based robotic arms include: Solomon, Techman Robot, Vention and Yaskawa.\nOver 100 companies are adopting Isaac Sim to simulate, test and validate robotic applications, including Hexagon, Husqvarna Group and MathWorks. Isaac Lab is being adopted by Agility, Boston Dynamics, Figure AI, Fourier Intelligence and Sanctuary AI.\nRobotics Innovation in Action at Computex\nIn his COMPUTEX keynote, Huang demonstrated robots used in transportation, healthcare and industrial manufacturing. In one demonstration, Foxconn, the world’s largest electronics manufacturer, showcases a fully simulated autonomous factory in NVIDIA Omniverse, featuring fleets of AI robots developed by NVIDIA robotics partners, based on NVIDIA Isaac.\nWatch Huang’s COMPUTEX keynote to get the latest on AI and more. Read more about the updates available now to the NVIDIA Isaac platform.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "New NVIDIA RTX A400 and A1000 GPUs Enhance AI-Powered Design and Productivity Workflows",
    "link": "https://blogs.nvidia.com/blog/ampere-rtx-a400-a1000-ai/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUxjakEwU1hvMGVFWlhTVFI2VFJDb0FSaXNBaWdCTWdhaHhKQ3R3UVU=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-04-16T07:00:00.000Z",
    "time": "Apr 16",
    "articleType": "regular",
    "content": "AI integration across design and productivity applications is becoming the new standard, fueling demand for advanced computing performance. This means professionals and creatives will need to tap into increased compute power, regardless of the scale, complexity or scope of their projects.\nTo meet this growing need, NVIDIA is expanding its RTX professional graphics offerings with two new NVIDIA Ampere architecture-based GPUs for desktops: the NVIDIA RTX A400 and NVIDIA RTX A1000.\nThey expand access to AI and ray-tracing technology, equipping professionals with the tools they need to transform their daily workflows.\nA New Era of Creativity, Performance and Efficiency\nThe RTX A400 GPU introduces accelerated ray tracing and AI to the RTX 400 series GPUs. With 24 Tensor Cores for AI processing, it surpasses traditional CPU-based solutions, enabling professionals to run cutting-edge AI applications, such as intelligent chatbots and copilots, directly on their desktops.\nThe GPU delivers real-time ray tracing so creators can build vivid, physically accurate 3D renders that push the boundaries of creativity and realism.\nThe A400 also includes four display outputs, a first for its series. This makes it ideal for high-density display environments, which are critical for industries like financial services, command and control, retail, and transportation.\nThe NVIDIA RTX A1000 GPU brings Tensor Cores and RT Cores to the RTX 1000 series GPUs for the first time, unlocking accelerated AI and ray-tracing performance for creatives and professionals.\nWith 72 Tensor Cores, the A1000 offers a tremendous upgrade over the previous generation, delivering over 3x faster generative AI processing for tools like Stable Diffusion. In addition, its 18 RT Cores speed graphics and rendering tasks by up to 3x, accelerating professional workflows such as 2D and 3D computer-aided design (CAD), product and architectural design, and 4K video editing.\nThe A1000 also excels in video processing, handling up to 38% more encode streams and offering 2x faster decode performance over the previous generation.\nWith a sleek, single-slot design and consuming just 50W, the A400 and A1000 GPUs bring impressive features to compact, energy-efficient workstations.\nExpanding the Reach of RTX\nThese new GPUs empower users with cutting-edge AI, graphics and compute capabilities to boost productivity and unlock creative possibilities. Advanced workflows involving ray-traced renders and AI are now within reach, allowing professionals to push the boundaries of their work and achieve stunning levels of realism.\nIndustrial planners can use ‌these new powerful and energy-efficient computing solutions for edge deployments. Creators can boost editing and rendering speeds to produce richer visual content. Architects and engineers can seamlessly transition ideas from 3D CAD concepts into tangible designs. Teams working in smart spaces can use the GPUs for real-time data processing, AI-enhanced security and digital signage management in space-constrained settings. And healthcare professionals can achieve quicker, more precise medical imaging analyses.\nFinancial professionals have always used expansive, high-resolution visual workspaces for more effective trading, analysis and data management. With the RTX A400 GPU supporting up to four 4K displays natively, financial services users can now achieve a high display density with fewer GPUs, streamlining their setups and reducing costs.\nNext-Generation Features and Accelerated Performance\nThe NVIDIA RTX A400 and A1000 GPUs are equipped with features designed to supercharge everyday workflows, including:\nSecond-generation RT Cores: Real-time ray tracing, photorealistic, physically based rendering and visualization for all professional workflows, including architectural drafting, 3D design and content creation, where accurate lighting and shadow simulations can greatly enhance the quality of work.\nThird-generation Tensor Cores: Accelerates AI-augmented tools and applications such as generative AI, image rendering denoising and deep learning super sampling to improve image generation speed and quality.\nAmpere architecture-based CUDA cores: Up to 2x the single-precision floating point throughput of the previous generation for significant speedups in graphics and compute workloads.\n4GB or 8GB of GPU memory: 4GB of GPU memory with the A400 GPU and 8GB with the A1000 GPU accommodate a range of professional needs, from basic graphic design and photo editing to more demanding 3D modeling with textures or high-resolution editing and data analyses. The GPUs also feature increased memory bandwidth over the previous generation for quicker data processing and smoother handling of larger datasets and scenes.\nEncode and decode engines: With seventh-generation encode (NVENC) and fifth-generation decode (NVDEC) engines, the GPUs offer efficient video processing to support high-resolution video editing, streaming and playback with ultra-low latency. Inclusion of AV1 decode enables higher efficiency and smoother playback of more video formats.\nThe NVIDIA RTX A1000 GPU is now available through global distribution partners such as PNY and Ryoyo Electric. The RTX A400 GPU is expected to be available from channel partners starting in May, with anticipated availability from manufacturers in the summer.",
    "favicon": ""
  },
  {
    "title": "NVIDIA Reveals Gaming, Creating, Generative AI, Robotics Innovations at CES",
    "link": "https://blogs.nvidia.com/blog/ces-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVlibTlNU0hkNk5XMWxaa0psVFJDb0FSaXNBaWdCTWdhQk5ack9HUW8=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA to Acquire GPU Orchestration Software Provider Run:ai",
    "link": "https://blogs.nvidia.com/blog/runai/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUZVMFowZDNOM1YyUlFla1pEVFJDakFSaTBBaWdCTWdrVmtwSm9OaVk5N1FF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-04-24T07:00:00.000Z",
    "time": "Apr 24",
    "articleType": "regular",
    "content": "To help customers make more efficient use of their AI computing resources, NVIDIA today announced it has entered into a definitive agreement to acquire Run:ai, a Kubernetes-based workload management and orchestration software provider.\nCustomer AI deployments are becoming increasingly complex, with workloads distributed across cloud, edge and on-premises data center infrastructure.\nManaging and orchestrating generative AI, recommender systems, search engines and other workloads requires sophisticated scheduling to optimize performance at the system level and on the underlying infrastructure.\nRun:ai enables enterprise customers to manage and optimize their compute infrastructure, whether on premises, in the cloud or in hybrid environments.\nThe company has built an open platform on Kubernetes, the orchestration layer for modern AI and cloud infrastructure. It supports all popular Kubernetes variants and integrates with third-party AI tools and frameworks.\nRun:ai customers include some of the world’s largest enterprises across multiple industries, which use the Run:ai platform to manage data-center-scale GPU clusters.\n“Run:ai has been a close collaborator with NVIDIA since 2020 and we share a passion for helping our customers make the most of their infrastructure,” said Omri Geller, Run:ai cofounder and CEO. “We’re thrilled to join NVIDIA and look forward to continuing our journey together.”\nThe Run:ai platform provides AI developers and their teams:\nA centralized interface to manage shared compute infrastructure, enabling easier and faster access for complex AI workloads.\nFunctionality to add users, curate them under teams, provide access to cluster resources, control over quotas, priorities and pools, and monitor and report on resource use.\nThe ability to pool GPUs and share computing power — from fractions of GPUs to multiple GPUs or multiple nodes of GPUs running on different clusters — for separate tasks.\nEfficient GPU cluster resource utilization, enabling customers to gain more from their compute investments.\nNVIDIA will continue to offer Run:ai’s products under the same business model for the immediate future. And NVIDIA will continue to invest in the Run:ai product roadmap, including enabling on NVIDIA DGX Cloud, an AI platform co-engineered with leading clouds for enterprise developers, offering an integrated, full-stack service optimized for generative AI.\nNVIDIA HGX, DGX and DGX Cloud customers will gain access to Run:ai’s capabilities for their AI workloads, particularly for large language model deployments. Run:ai’s solutions are already integrated with NVIDIA DGX, NVIDIA DGX SuperPOD, NVIDIA Base Command, NGC containers, and NVIDIA AI Enterprise software, among other products.\nNVIDIA’s accelerated computing platform and Run:ai’s platform will continue to support a broad ecosystem of third-party solutions, giving customers choice and flexibility.\nTogether with Run:ai, NVIDIA will enable customers to have a single fabric that accesses GPU solutions anywhere. Customers can expect to benefit from better GPU utilization, improved management of GPU infrastructure and greater flexibility from the open architecture.",
    "favicon": ""
  },
  {
    "title": "NVIDIA, Global Workstation Manufacturers to Launch Powerful Systems for Generative AI and LLM Development, Content Creation, Data Science",
    "link": "https://nvidianews.nvidia.com/news/nvidia-global-workstation-manufacturers-to-launch-powerful-systems-for-generative-ai-and-llm-development-content-creation-data-science",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW5abTlLTUY5SlRGRlBjemQ1VFJDb0FSaXNBaWdCTWdZZGhKQ01NUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "Desktops Feature NVIDIA RTX 6000 Ada GPUs, NVIDIA Omniverse and NVIDIA AI Enterprise Software\nSIGGRAPH—NVIDIA and global manufacturers today announced powerful new NVIDIA RTX™ workstations designed for development and content creation in the age of generative AI and digitalization.\nThe systems, including those from BOXX, Dell Technologies, HP and Lenovo, are based on NVIDIA RTX 6000 Ada Generation GPUs and incorporate NVIDIA AI Enterprise and NVIDIA Omniverse™ Enterprise software.\nSeparately, NVIDIA also released three new desktop workstation Ada Generation GPUs — the NVIDIA RTX 5000, RTX 4500 and RTX 4000 — to deliver the latest AI, graphics and real-time rendering technology to professionals worldwide.\n“Few workloads are as challenging as generative AI and digitalization applications, which require a full-stack approach to computing,” said Bob Pette, vice president of professional visualization at NVIDIA. “Professionals can now tackle these on a desktop with the latest NVIDIA-powered RTX workstations, enabling them to build vast, digitalized worlds in the new age of generative AI.”\nThe new RTX workstations offer up to four NVIDIA RTX 6000 Ada GPUs, each equipped with 48GB of memory, and a single desktop workstation can provide up to 5,828 TFLOPS of AI performance and 192GB of GPU memory. Depending on user needs, systems can be configured with NVIDIA AI Enterprise or Omniverse Enterprise to power a breadth of demanding generative AI and graphics-intensive workloads.\nNVIDIA AI Enterprise 4.0, announced separately today, now includes NVIDIA NeMo™, an end-to-end framework for building and customizing foundation models for generative AI, NVIDIA RAPIDS™ libraries for data science, as well as frameworks, pretrained models and tools for building common enterprise AI use cases, including recommenders, virtual assistants and cybersecurity solutions.\nOmniverse Enterprise is a platform for industrial digitalization that enables teams to develop interoperable 3D workflows and OpenUSD applications. As an OpenUSD-native platform, Omniverse enables globally distributed teams to collaborate on full-design-fidelity datasets from hundreds of 3D applications.\n“Yurts provides a full-stack generative AI solution aligning with multiple form factors, deployment models and budgets of our customers. We’ve achieved this by leveraging LLMs for various natural language processing tasks and incorporating the RTX 6000 Ada. From private data centers to workstation-sized solutions that fit under a desk, Yurts remains committed to scaling our platform and offering alongside NVIDIA,” said Jason Schnitzer, chief technology officer at Yurts.\nThe new NVIDIA RTX 5000, RTX 4500 and RTX 4000 desktop GPUs feature the latest NVIDIA Ada Lovelace architecture technologies, including:\nNVIDIA CUDA® cores: Up to 2x the single-precision floating point throughput compared to the previous generation.\nThird-generation RT Cores: Up to 2x the throughput of the previous generation with the ability to concurrently run ray tracing with either shading or denoising capabilities.\nFourth-generation Tensor Cores: Up to 2x faster AI training performance than the previous generation with expanded support for the FP8 data format.\nDLSS 3: New levels of realism and interactivity for real-time graphics with the power of AI.\nLarger GPU memory: The RTX 4000 provides 20GB of GDDR6 memory; the RTX 4500 offers 24GB of GDDR6 memory; and the RTX 5000 boasts 32GB of GDDR6 memory — all supporting error-code correction for error-free computing with large 3D models, rendered images, simulations and AI datasets.\nExtended-reality capabilities: Support for high-resolution augmented-reality and virtual-reality devices to deliver the high-performance graphics required for creating stunning AR, VR and mixed-reality content.\n“The NVIDIA RTX 5000 Ada GPU demonstrates NVIDIA’s impressive generational performance improvements — it has significantly increased our efficiency in creating stereo panoramas using Enscape,” said Dan Stine, director of design technology at architectural firm Lake|Flato. “With the performance boost and large frame buffer of RTX 5000 GPUs, our large, complex models look great in virtual reality, which gives our clients a more comfortable and contextual experience.”\nRTX workstations featuring up to four RTX 6000 Ada GPUs, NVIDIA AI Enterprise and NVIDIA Omniverse Enterprise will be available from system builders starting in the fall.\nThe new NVIDIA RTX 5000 GPU is now available and shipping from HP and through global distribution partners such as Leadtek, PNY and Ryoyo Electro starting today. The NVIDIA RTX 4500 and RTX 4000 GPUs will be available in the fall from BOXX, Dell Technologies, HP and Lenovo and through global distribution partners.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "See the Future at GTC 2024: NVIDIA’s Jensen Huang to Unveil Latest Breakthroughs in Accelerated Computing, Generative AI and Robotics",
    "link": "https://nvidianews.nvidia.com/news/see-the-future-at-gtc-2024-nvidias-jensen-huang-to-unveil-latest-breakthroughs-in-accelerated-computing-generative-aiand-robotics",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWtWa3d5ZVdnM1lrNVRObkJpVFJDb0FSaXNBaWdCTWdheE5KYVBtUWs=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-20T08:00:00.000Z",
    "time": "Feb 20",
    "articleType": "regular",
    "content": "AI Leaders and Top Companies From Healthcare, Transportation, Financial Services, Telecom and Agriculture to Participate in First In-Person GTC in Five Years\nNVIDIA today announced it will host its flagship GTC 2024 conference at the San Jose Convention Center from March 18-21. More than 300,000 people are expected to register to attend in person or virtually.\nNVIDIA founder and CEO Jensen Huang will deliver the keynote from the SAP Center on Monday, March 18, at 1 p.m. Pacific time. It will be livestreamed and available on demand. Registration is not required to view the keynote online.\nSince Huang first highlighted machine learning in his 2014 GTC keynote, NVIDIA has been at the forefront of the AI revolution. The company’s platforms have played a crucial role in enabling AI across numerous domains including large language models, biology, cybersecurity, data center and cloud computing, conversational AI, networking, physics, robotics, and quantum, scientific and edge computing.\nThe event’s 900 sessions and over 300 exhibitors will showcase how organizations are deploying NVIDIA platforms to achieve remarkable breakthroughs across industries, including aerospace, agriculture, automotive and transportation, cloud services, financial services, healthcare and life sciences, manufacturing, retail and telecommunications.\n“Generative AI has moved to center stage as governments, industries and organizations everywhere look to harness its transformative capabilities,” Huang said. “GTC has become the world’s most important AI conference because the entire ecosystem is there to share knowledge and advance the state of the art. Come join us.”\nBrad Lightcap, chief operating officer, OpenAI\nSébastien Bubeck, vice president, Microsoft GenAI\nVincent Vanhoucke, distinguished scientist and senior director of robotics, Google DeepMind\nJoelle Pineau, vice president of AI research, Meta\nDr. Fei-Fei Li, professor of computer science and HAI co-director, Stanford University\nDr. Priscilla Chan, cofounder and co-CEO, Chan Zuckerberg Initiative\nStefan Goebel, senior vice president, chief of staff, product engineering and head of strategic engineering partnerships, SAP Labs\nBelinda Neal, chief operating officer for core engineering and head of engineering partnerships, Goldman Sachs\nMoises Hernandez-Fernandez, vice president of machine learning center of excellence, JPMorgan Chase\nShan Jegatheeswaran, vice president and global head of MedTech Digital, Johnson & Johnson\nRodolphe Katra, vice president of AI, Medtronic\nAaron Saunders, CTO, Boston Dynamics\nMore than 1,000 organizations will participate, including Adobe, Amazon, Amgen, Anthropic, Blackrock, Cohere, Databricks, Dell Technologies, Genentech, Getty Images, HPE, Hugging Face, Lockheed Martin, L’Oreal, Lowe’s, Lucasfilm and ILM, Mercedes-Benz, Micron, Mistral AI, Netflix, Oracle, Pixar, Runway, Saudi Aramco, Scale AI, ServiceNow, Siemens, Snowflake, Supermicro, Walt Disney Animation Studios and Zoox.\nRegistration is open at www.nvidia.com/gtc.\nAI, Auto, Robotics on Display\nFrom generative AI to robotics to automotive, GTC attendees can interact with dozens of state-of-the-art demos, see the latest autonomous vehicle technology and explore how generative AI will impact virtually every industry.\nThe Generative AI Pavilion will include a huge, multisensory, interactive installation by world-renowned AI artist Refik Anadol and a demo of Cuebric, a generative AI tool for filmmakers, by Seyhan Lee.\nMany of the world’s leading automotive and robotics companies will show next-generation vehicles and autonomous machines at the event. Vehicles on display will include the new Volvo EX90, Mercedes-Benz Concept CLA Class, Polestar 3, WeRide Robobus and Nuro R3 autonomous delivery vehicle.\nTwenty-five robots ranging from humanoids to industrial manipulators will be on the show floor, from companies including Agility Robotics, Boston Dynamics, Disney and Google DeepMind.\nLeading AI researchers will participate in 200+ sessions, including a panel moderated by Huang on Transforming AI with all eight authors of “Attention Is All You Need,” a seminal research paper on transformers. Other research highlights include:\nInsights from NVIDIA Research with NVIDIA Chief Scientist Bill Dally\nFireside chat with Christian Szegedy, research scientist and cofounder of xAI, and Bojan Tunguz, data scientist at NVIDIA, on AI-based reasoning.\nPresentation by Sébastien Bubeck, vice president of Microsoft GenAI, on the promise of smaller models.\nDiscussion on building practical AI agents that reason and code at scale with Kanjun Qiu, cofounder and CEO of Imbue, and Bryan Catanzaro, vice president of applied deep learning research at NVIDIA.\nConversation on the future of foundation models with Percy Liang, associate professor and director of the Center for Research on Foundation Models at Stanford University and cofounder of Together AI, and Jim Fan, research scientist at NVIDIA.\nGTC presents a rich variety of learning and development opportunities for career professionals, policymakers, educators and students.\nPolicy makers can join a discussion on the challenges of AI regulation with representatives from the U.S. Congress, the National Institute of Standards and Technology, the European Union and NVIDIA.\nAttendees can choose from 20 full-day, instructor-led, hands-on technical workshops, many of which will be available virtually in EMEA and APAC time zones.\nFor the first time at GTC, participants can receive professional certification in generative AI.\nMore than 40 complimentary onsite training labs are included in the GTC “Conference + Training” package.\nTo further support AI in academia, a curated set of sessions will be available\nto help educators integrate NVIDIA technologies into classrooms and courses.\nNVIDIA Inception, a global program designed to nurture cutting-edge startups with more than 18,000 members, will host an interactive pavilion featuring demos from dozens of startups. More than 150 Inception members will participate in GTC through exhibitions, presentation sessions, pitches and more.\nAI Secrets I Wish I Knew\nBuilding Tools for Digital Worlds: Startups Pioneering OpenUSD and Generative AI\nGlobal Strategies: Startups, Venture Capital and Climate Change Solutions\nSeparate sessions for the venture capital community will be offered through the NVIDIA VC Alliance program, including an AI day for VCs and a reverse pitch session, where VCs will pitch startups to join their portfolio.\nNVIDIA will hold a Q&A session for investors on March 19, at 8:30 a.m. Pacific time. The webcast will be available at investor.nvidia.com.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Debuts AI-Enhanced Real-Time Ray Tracing for Games and Apps With New DLSS 3.5",
    "link": "https://blogs.nvidia.com/blog/gamescom-dlss-ray-reconstruction/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWtUVjl0ZVY5TWRsRjJjMnBGVFJDakFSaTBBaWdCTWdZQkJZTHNPUVU=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-22T07:00:00.000Z",
    "time": "Aug 22, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "TSMC and Synopsys Bring Breakthrough NVIDIA Computational Lithography Platform to Production",
    "link": "https://nvidianews.nvidia.com/news/tsmc-synopsys-nvidia-culitho",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXdRV1l0WlhWVGRIQkNjMko1VFJDM0FSaVRBaWdCTWdhaGc0eU5zUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "NVIDIA cuLitho Accelerates Semiconductor Manufacturing’s Most Compute-Intensive Workload by 40-60x, Opens Industry to New Generative AI Algorithms\nGTC—NVIDIA today announced that TSMC and Synopsys are going into production with NVIDIA’s computational lithography platform to accelerate manufacturing and push the limits of physics for the next generation of advanced semiconductor chips.\nTSMC, the world’s leading foundry, and Synopsys, the leader in silicon to systems design solutions, have integrated NVIDIA cuLitho with their software, manufacturing processes and systems to speed chip fabrication, and in the future support the latest-generation NVIDIA Blackwell architecture GPUs.\n“Computational lithography is a cornerstone of chip manufacturing,” said Jensen Huang, founder and CEO of NVIDIA. “Our work on cuLitho, in partnership with TSMC and Synopsys, applies accelerated computing and generative AI to open new frontiers for semiconductor scaling.”\nNVIDIA also introduced new generative AI algorithms that enhance cuLitho, a library for GPU-accelerated computational lithography, dramatically improving the semiconductor manufacturing process over current CPU-based methods.\nSemiconductor Leaders Advance cuLitho Platform\nComputational lithography is the most compute-intensive workload in the semiconductor manufacturing process, consuming tens of billions of hours per year on CPUs. A typical mask set for a chip — a key step in its production — could take 30 million or more hours of CPU compute time, necessitating large data centers within semiconductor foundries. With accelerated computing, 350 NVIDIA H100 systems can now replace 40,000 CPU systems, accelerating production time, while reducing costs, space and power.\n“Our work with NVIDIA to integrate GPU-accelerated computing in the TSMC workflow has resulted in great leaps in performance, dramatic throughput improvement, shortened cycle time and reduced power requirements,” said Dr. C.C. Wei, CEO of TSMC. “We are moving NVIDIA cuLitho into production at TSMC, leveraging this computational lithography technology to drive a critical component of semiconductor scaling.”\nSince its introduction last year, cuLitho has enabled TSMC to open new opportunities for innovative patterning technologies. In testing cuLitho on shared workflows, the companies jointly realized a 45x speedup of curvilinear flows and a nearly 60x improvement on more traditional Manhattan-style flows. These two types of flows differ — with curvilinear the mask shapes are represented by curves, while Manhattan mask shapes are constrained to be either horizontal or vertical.\n“For more than two decades Synopsys Proteus mask synthesis software products have been the production-proven choice for accelerating computational lithography — the most demanding workload in semiconductor manufacturing,” said Sassine Ghazi, president and CEO of Synopsys. “With the move to advanced nodes, computational lithography has dramatically increased in complexity and compute cost. Our collaboration with TSMC and NVIDIA is critical to enabling angstrom-level scaling as we pioneer advanced technologies to reduce turnaround time by orders of magnitude through the power of accelerated computing.”\nSynopsys is the pioneer in delivering advanced techniques for accelerating the performance of computational lithography. Synopsys’ Proteus™ optical proximity correction software running on the NVIDIA cuLitho software library significantly speeds computational workloads compared to current CPU-based methods. With Proteus mask synthesis products, manufacturers like TSMC can achieve exceptional precision, efficiency and speed in proximity correction, model building for correction, and analyzing proximity effects on corrected and uncorrected IC layout patterns, revolutionizing the chip fabrication process.\nBreakthrough Generative AI Support for Computational Lithography\nNVIDIA has developed algorithms to apply generative AI to further enhance the value of the cuLitho platform. The new generative AI workflow delivers an additional 2x speedup on top of the accelerated processes enabled through cuLitho. The application of generative AI enables creation of a near-perfect inverse mask or inverse solution to account for diffraction of light. The final mask is then derived by traditional and physically rigorous methods, speeding up the overall optical proximity correction (OPC) process by a factor of two.\nMany changes in the fab process currently necessitate a revision in OPC, driving up the amount of compute required and creating bottlenecks in the fab development cycle. These costs and bottlenecks are alleviated with the accelerated computing cuLitho provides and generative AI, enabling fabs to allocate available compute capacity and engineering bandwidth to design more novel solutions in development of new technologies for 2nm and beyond.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Launches Generative AI Microservices for Developers to Create and Deploy Generative AI Copilots Across NVIDIA CUDA GPU Installed Base",
    "link": "https://nvidianews.nvidia.com/news/generative-ai-microservices-for-developers",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNWlRVkpaWTFjdFdtOW1TRGRJVFJDbUFSaXdBaWdCTWdrUlVvNEpwYVpyVXdF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "New Catalog of GPU-Accelerated NVIDIA NIM Microservices and Cloud Endpoints for Pretrained AI Models Optimized to Run on Hundreds of Millions of CUDA-Enabled GPUs Across Clouds, Data Centers, Workstations and PCs\nEnterprises Can Use Microservices to Accelerate Data Processing, LLM Customization, Inference, Retrieval-Augmented Generation and Guardrails\nAdopted by Broad AI Ecosystem, Including Leading Application Platform Providers Cadence, CrowdStrike, SAP, ServiceNow and More\nGTC—NVIDIA today launched dozens of enterprise-grade generative AI microservices that businesses can use to create and deploy custom applications on their own platforms while retaining full ownership and control of their intellectual property.\nBuilt on top of the NVIDIA CUDA® platform, the catalog of cloud-native microservices includes NVIDIA NIM microservices for optimized inference on more than two dozen popular AI models from NVIDIA and its partner ecosystem. In addition, NVIDIA accelerated software development kits, libraries and tools can now be accessed as NVIDIA CUDA-X™ microservices for retrieval-augmented generation (RAG), guardrails, data processing, HPC and more. NVIDIA also separately announced over two dozen healthcare NIM and CUDA-X microservices.\nThe curated selection of microservices adds a new layer to NVIDIA’s full-stack computing platform. This layer connects the AI ecosystem of model developers, platform providers and enterprises with a standardized path to run custom AI models optimized for NVIDIA’s CUDA installed base of hundreds of millions of GPUs across clouds, data centers, workstations and PCs.\nAmong the first to access the new NVIDIA generative AI microservices available in NVIDIA AI Enterprise 5.0 are leading application, data and cybersecurity platform providers including Adobe, Cadence, CrowdStrike, Getty Images, SAP, ServiceNow, and Shutterstock.\n“Established enterprise platforms are sitting on a goldmine of data that can be transformed into generative AI copilots,” said Jensen Huang, founder and CEO of NVIDIA. “Created with our partner ecosystem, these containerized AI microservices are the building blocks for enterprises in every industry to become AI companies.”\nNIM Inference Microservices Speed Deployments From Weeks to Minutes\nNIM microservices provide pre-built containers powered by NVIDIA inference software — including Triton Inference Server™ and TensorRT™-LLM — which enable developers to reduce deployment times from weeks to minutes.\nThey provide industry-standard APIs for domains such as language, speech and drug discovery to enable developers to quickly build AI applications using their proprietary data hosted securely in their own infrastructure. These applications can scale on demand, providing flexibility and performance for running generative AI in production on NVIDIA-accelerated computing platforms.\nNIM microservices provide the fastest and highest-performing production AI container for deploying models from NVIDIA, A121, Adept, Cohere, Getty Images, and Shutterstock as well as open models from Google, Hugging Face, Meta, Microsoft, Mistral AI and Stability AI.\nServiceNow today announced that it is using NIM to develop and deploy new domain-specific copilots and other generative AI applications faster and more cost effectively.\nCustomers will be able to access NIM microservices from Amazon SageMaker, Google Kubernetes Engine and Microsoft Azure AI, and integrate with popular AI frameworks like Deepset, LangChain and LlamaIndex.\nCUDA-X Microservices for RAG, Data Processing, Guardrails, HPC\nCUDA-X microservices provide end-to-end building blocks for data preparation, customization and training to speed production AI development across industries.\nTo accelerate AI adoption, enterprises may use CUDA-X microservices including NVIDIA Riva for customizable speech and translation AI, NVIDIA cuOpt™ for routing optimization, as well as NVIDIA Earth-2 for high resolution climate and weather simulations.\nNeMo Retriever™ microservices let developers link their AI applications to their business data — including text, images and visualizations such as bar graphs, line plots and pie charts — to generate highly accurate, contextually relevant responses. With these RAG capabilities, enterprises can offer more data to copilots, chatbots and generative AI productivity tools to elevate accuracy and insight.\nAdditional NVIDIA NeMo™ microservices are coming soon for custom model development. These include NVIDIA NeMo Curator for building clean datasets for training and retrieval, NVIDIA NeMo Customizer for fine-tuning LLMs with domain-specific data, NVIDIA NeMo Evaluator for analyzing AI model performance, as well as NVIDIA NeMo Guardrails for LLMs.\nEcosystem Supercharges Enterprise Platforms With Generative AI Microservices\nIn addition to leading application providers, data, infrastructure and compute platform providers across the NVIDIA ecosystem are working with NVIDIA microservices to bring generative AI to enterprises.\nTop data platform providers including Box, Cloudera, Cohesity, Datastax, Dropbox and NetApp are working with NVIDIA microservices to help customers optimize their RAG pipelines and integrate their proprietary data into generative AI applications. Snowflake leverages NeMo Retriever to harness enterprise data for building AI applications.\nEnterprises can deploy NVIDIA microservices included with NVIDIA AI Enterprise 5.0 across the infrastructure of their choice, such as leading clouds Amazon Web Services (AWS), Google Cloud, Azure and Oracle Cloud Infrastructure.\nNVIDIA microservices are also supported on over 400 NVIDIA-Certified Systems™, including servers and workstations from Cisco, Dell Technologies, Hewlett Packard Enterprise (HPE) , HP, Lenovo and Supermicro. Separately today, HPE announced availability of HPE’s enterprise computing solution for generative AI, with planned integration of NIM and NVIDIA AI Foundation models into HPE’s AI software.\nNVIDIA AI Enterprise microservices are coming to infrastructure software platforms including VMware Private AI Foundation with NVIDIA. Red Hat OpenShift supports NVIDIA NIM microservices to help enterprises more easily integrate generative AI capabilities into their applications with optimized capabilities for security, compliance and controls. Canonical is adding Charmed Kubernetes support for NVIDIA microservices through NVIDIA AI Enterprise.\nNVIDIA’s ecosystem of hundreds of AI and MLOps partners, including Abridge, Anyscale, Dataiku, DataRobot, Glean, H2O.ai, Securiti AI, Scale AI, OctoAI and Weights & Biases, are adding support for NVIDIA microservices through NVIDIA AI Enterprise.\nApache Lucene, Datastax, Faiss, Kinetica, Milvus, Redis, and Weaviate are among the vector search providers working with NVIDIA NeMo Retriever microservices to power responsive RAG capabilities for enterprises.\nDevelopers can experiment with NVIDIA microservices at ai.nvidia.com at no charge. Enterprises can deploy production-grade NIM microservices with NVIDIA AI Enterprise 5.0 running on NVIDIA-Certified Systems and leading cloud platforms.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "SIGGRAPH Special Address: NVIDIA CEO Brings Generative AI to LA Show",
    "link": "https://blogs.nvidia.com/blog/siggraph-2023-special-address/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUdNVTA0YjBOd1NHUlZUakU1VFJDb0FSaXNBaWdCTWdZcFVKQ09xQVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "As generative AI continues to sweep an increasingly digital, hyperconnected world, NVIDIA founder and CEO Jensen Huang made a thunderous return to SIGGRAPH, the world’s premier computer graphics conference.\n“The generative AI era is upon us, the iPhone moment if you will,” Huang told an audience of thousands Tuesday during an in-person special address in Los Angeles.\nNews highlights include the next-generation GH200 Grace Hopper Superchip platform, NVIDIA AI Workbench — a new unified toolkit that introduces simplified model tuning and deployment on NVIDIA AI platforms — and a major upgrade to NVIDIA Omniverse with generative AI and OpenUSD.\nThe announcements are about bringing all of the past decade’s innovations — AI, virtual worlds, acceleration, simulation, collaboration and more — together.\n“Graphics and artificial intelligence are inseparable, graphics needs AI, and AI needs graphics,” Huang said, explaining that AI will learn skills in virtual worlds, and that AI will help create virtual worlds.\nFundamental to AI, Real-Time Graphics\nFive years ago at SIGGRAPH, NVIDIA reinvented graphics by bringing AI and real-time ray tracing to GPUs. But “while we were reinventing computer graphics with artificial intelligence, we were reinventing the GPU altogether for artificial intelligence,” Huang said.\nThe result: increasingly powerful systems such as the NVIDIA HGX H100, which harnesses eight GPUs  — and a total of 1 trillion transistors — that offer dramatic acceleration over CPU-based systems.\n“This is the reason why the world’s data centers are rapidly transitioning to accelerated computing,” Huang told the audience. “The more you buy, the more you save.”\nTo continue AI’s momentum, NVIDIA created the Grace Hopper Superchip, the NVIDIA GH200, which combines a 72-core Grace CPU with a Hopper GPU, and which went into full production in May.\nHuang announced that NVIDIA GH200, which is already in production, will be complemented with an additional version with cutting-edge HBM3e memory.\nHe followed up on that by announcing the next-generation GH200 Grace Hopper superchip platform with the ability to connect multiple GPUs for exceptional performance and easily scalable server design.\nBuilt to handle the world’s most complex generative workloads, spanning large language models, recommender systems and vector databases, the new platform will be available in a wide range of configurations.\nThe dual configuration — which delivers up to 3.5x more memory capacity and 3x more bandwidth than the current generation offering — comprises a single server with 144 Arm Neoverse cores, eight petaflops of AI performance, and 282GB of the latest HBM3e memory technology.\nLeading system manufacturers are expected to deliver systems based on the platform in the second quarter of 2024.\nA packed house at the SIGGRAPH professional graphics conference attended NVIDIA founder and CEO Jensen Huang’s keynote address.\nNVIDIA AI Workbench Speeds Adoption of Custom Generative AI\nTo speed custom adoption of generative AI for the world’s enterprises, Huang announced NVIDIA AI Workbench. It provides developers with a unified, easy-to-use toolkit to quickly create, test and fine-tune generative AI models on a PC or workstation — then scale them to virtually any data center, public cloud or NVIDIA DGX Cloud.\nAI Workbench removes the complexity of getting started with an enterprise AI project. Accessed through a simplified interface running on a local system, it allows developers to fine-tune models from popular repositories such as Hugging Face, GitHub and NGC using custom data. The models can then be shared easily across multiple platforms.\nWhile hundreds of thousands of pretrained models are now available, customizing them with the many open-source tools available can be challenging and time consuming.\n“In order to democratize this ability, we have to make it possible to run pretty much everywhere,” Huang said.\nWith AI Workbench, developers can customize and run generative AI in just a few clicks. It allows them to pull together all necessary enterprise-grade models, frameworks, software development kits and libraries into a unified developer workspace.\n“Everybody can do this,” Huang said.\nLeading AI infrastructure providers — including Dell Technologies, Hewlett Packard Enterprise, HP Inc., Lambda, Lenovo and Supermicro — are embracing AI Workbench for its ability to bring enterprise generative AI capability to wherever developers want to work — including a local device.\nHuang also announced a partnership between NVIDIA and startup Hugging Face, which has 2 million users, that will put generative AI supercomputing at the fingertips of millions of developers building large language models and other advanced AI applications.\nDevelopers will be able to access NVIDIA DGX Cloud AI supercomputing within the Hugging Face platform to train and tune advanced AI models.\n“This is going to be a brand new service to connect the world’s largest AI community to the world’s best training and infrastructure,” Huang said.\nIn a video, Huang showed how AI Workbench and ChatUSD bring it all together: allowing a user to start a project on a GeForce RTX 4090 laptop and scale, seamlessly to a workstation, or the data center  as it grows more complex.\nUsing Jupyter Notebook, a user can prompt the model to generate a picture of Toy Jensen in space. When the model provides a result that doesn’t work, because it’s never seen Toy Jensen, the user can fine-tune the model with eight images of Toy Jensen and then prompt it again to get a correct result.\nThen with AI Workbench, the new model can be deployed to an enterprise application.\nNew NVIDIA Enterprise 4.0 Software Advances AI Deployment\nIn a further step to accelerate the adoption of generative AI, NVIDIA announced the latest version of its enterprise software suite, NVIDIA AI Enterprise 4.0.\nNVIDIA AI Enterprise gives businesses access to the tools needed to adopt generative AI, while also offering the security and API stability required for large-scale enterprise deployments.\nMajor Omniverse Release Converges Generative AI, OpenUSD for Industrial Digitalization\nOffering new foundation applications and services for developers and industrial enterprises to optimize and enhance their 3D pipelines with the OpenUSD framework and generative AI, Huang announced a major release of NVIDIA Omniverse, an OpenUSD-native development platform for building, simulating, and collaborating across tools and virtual worlds.\nHe also announced NVIDIA’s contributions to OpenUSD, the framework and universal interchange for describing, simulating and collaborating across 3D tools.\nUpdates to the Omniverse platform include advancements to Omniverse Kit — the engine for developing native OpenUSD applications and extensions — as well as to the NVIDIA Omniverse Audio2Face foundation app and spatial-computing capabilities.\nCesium, Convai, Move AI, SideFX Houdini and Wonder Dynamics are now connected to Omniverse via OpenUSD.\nAnd expanding their collaboration across Adobe Substance 3D, generative AI and OpenUSD initiatives, Adobe and NVIDIA announced plans to make Adobe Firefly — Adobe’s family of creative generative AI models — available as APIs in Omniverse.\nOmniverse users can now build content, experiences and applications that are compatible with other OpenUSD-based spatial computing platforms such as ARKit and RealityKit.\nHuang announced a broad range of frameworks, resources and services for developers and companies to accelerate the adoption of Universal Scene Description, known as OpenUSD, including contributions such as geospatial data models, metrics assembly and simulation-ready, or SimReady, specifications for OpenUSD.\nHuang also announced four new Omniverse Cloud APIs built by NVIDIA for developers to more seamlessly implement and deploy OpenUSD pipelines and applications.\nChatUSD — Assisting developers and artists working with OpenUSD data and scenes, ChatUSD is a large language model (LLM) agent for generating Python-USD code scripts from text and answering USD knowledge questions.\nRunUSD — a cloud API that translates OpenUSD files into fully path-traced rendered images by checking compatibility of the uploaded files against versions of OpenUSD releases, and generating renders with Omniverse Cloud.\nDeepSearch — an LLM agent enabling fast semantic search through massive databases of untagged assets.\nUSD-GDN Publisher — a one-click service that enables enterprises and software makers to publish high-fidelity, OpenUSD-based experiences to the Omniverse Cloud Graphics Delivery Network (GDN) from an Omniverse-based application such as USD Composer, as well as stream in real time to web browsers and mobile devices.\nThese contributions are an evolution of last week’s announcement of NVIDIA’s co-founding of the Alliance for OpenUSD along with Pixar, Adobe, Apple and Autodesk.\nPowerful New Desktop Systems, Servers\nProviding more computing power for all of this, Huang said NVIDIA and global workstation manufacturers are announcing powerful new RTX workstations for development and content creation in the age of generative AI and digitization.\nThe systems, including those from BOXX, Dell Technologies, HP and Lenovo, are based on NVIDIA RTX 6000 Ada Generation GPUs and incorporate NVIDIA AI Enterprise and NVIDIA Omniverse Enterprise software.\nSeparately, NVIDIA released three new desktop workstation Ada Generation GPUs — the NVIDIA RTX 5000, RTX 4500 and RTX 4000 — to deliver the latest AI, graphics and real-time rendering technology to professionals worldwide.\nHuang also detailed how, together with global data center system manufacturers, NVIDIA is continuing to supercharge generative AI and industrial digitization with new NVIDIA OVX featuring the new NVIDIA L40S GPU, a powerful, universal data center processor design.\nThe powerful new systems will accelerate the most compute-intensive, complex applications, including AI training and inference, 3D design and visualization, video processing and industrial digitalization with the NVIDIA Omniverse platform.\nNVIDIA Research Bringing New Capabilities\nMore innovations are coming, thanks to NVIDIA Research.\nAt the show’s Real Time Live Event, NVIDIA researchers will demonstrate a generative AI workflow that helps artists rapidly create and iterate on materials for 3D scenes, using text or image prompts to generate custom textured materials faster and with finer creative control.\nAnd NVIDIA Research also demo’d how AI can take video conferencing to the next level with new 3D features. NVIDIA Research recently published a paper demonstrating how AI could power a 3D video-conferencing system with minimal capture equipment.\nThe production version of Maxine, now available in NVIDIA Enterprise, allows professionals, teams, creators and others to tap into the power of AI to create high-quaity audio and video effects, even using standard microphone and webcams.\nWatch Huang’s full special address at NVIDIA’s SIGGRAPH event site. where there are also details of labs, presentations and more happening throughout the show.",
    "favicon": ""
  },
  {
    "title": "NVIDIA Eos Revealed: Peek Into Operations of a Top 10 Supercomputer",
    "link": "https://blogs.nvidia.com/blog/eos/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNUJlVlppZEZkVVJsUldlSGREVFJDakFSaTBBaWdCTWdNSk1RUQ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-15T08:00:00.000Z",
    "time": "Feb 15",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Say What? Chat With RTX Brings Custom Chatbot to NVIDIA RTX AI PCs",
    "link": "https://blogs.nvidia.com/blog/chat-with-rtx-available-now/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNWpWRmw2ZEdweU9IZFVkVFJTVFJDb0FSaXNBaWdCTWdPTkVnSQ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-13T08:00:00.000Z",
    "time": "Feb 13",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Project GR00T Foundation Model for Humanoid Robots and Major Isaac Robotics Platform Update",
    "link": "https://nvidianews.nvidia.com/news/foundation-model-isaac-robotics-platform",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUdjRXhwTWt0alFVOXlhRzg0VFJDakFSaTBBaWdCTWdZRk1vNXFLUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Isaac Robotics Platform Now Provides Developers New Robot Training Simulator, Jetson Thor Robot Computer, Generative AI Foundation Models, and CUDA-Accelerated Perception and Manipulation Libraries\nGTC—NVIDIA today announced Project GR00T, a general-purpose foundation model for humanoid robots, designed to further its work driving breakthroughs in robotics and embodied AI.\nAs part of the initiative, the company also unveiled a new computer, Jetson Thor, for humanoid robots based on the NVIDIA Thor system-on-a-chip (SoC), as well as significant upgrades to the NVIDIA Isaac™ robotics platform, including generative AI foundation models and tools for simulation and AI workflow infrastructure.\n“Building foundation models for general humanoid robots is one of the most exciting problems to solve in AI today,” said Jensen Huang, founder and CEO of NVIDIA. “The enabling technologies are coming together for leading roboticists around the world to take giant leaps towards artificial general robotics.”\nRobots powered by GR00T, which stands for Generalist Robot 00 Technology, will be designed to understand natural language and emulate movements by observing human actions — quickly learning coordination, dexterity and other skills in order to navigate, adapt and interact with the real world. In his GTC keynote, Huang demonstrated several such robots completing a variety of tasks.\nJetson Thor was created as a new computing platform capable of performing complex tasks and interacting safely and naturally with people and machines. It has a modular architecture optimized for performance, power and size.\nThe SoC includes a next-generation GPU based on the NVIDIA Blackwell architecture with a transformer engine delivering 800 teraflops of 8-bit floating point AI performance to run multimodal generative AI models like GR00T. With an integrated functional safety processor, a high-performance CPU cluster and 100GB of ethernet bandwidth, it significantly simplifies design and integration efforts.\nNVIDIA is building a comprehensive AI platform for leading humanoid robot companies such as 1X Technologies, Agility Robotics, Apptronik, Boston Dynamics, Figure AI, Fourier Intelligence, Sanctuary AI, Unitree Robotics and XPENG Robotics, among others.\n“We are at an inflection point in history, with human-centric robots like Digit poised to change labor forever. Modern AI will accelerate development, paving the way for robots like Digit to help people in all aspects of daily life,” said Jonathan Hurst, cofounder and chief robot officer at Agility Robotics. “We’re excited to partner with NVIDIA to invest in the computing, simulation tools, machine learning environments and other necessary infrastructure to enable the dream of robots being a part of daily life.”\n“Embodied AI will not only help address some of humanity’s biggest challenges, but also create innovations which are currently beyond our reach or imagination,” said Geordie Rose, cofounder and CEO of Sanctuary AI. “Technology this important shouldn’t be built in silos, which is why we prioritize long-term partners like NVIDIA.”\nMajor Updates to Isaac Platform\nThe Isaac tools that GR00T utilizes are capable of creating new foundation models for any robot embodiment in any environment. Among these tools are Isaac Lab for reinforcement learning, and OSMO, a compute orchestration service.\nEmbodied AI models require massive amounts of real and synthetic data. The new Isaac Lab is a GPU-accelerated, lightweight, performance-optimized application built on Isaac Sim specifically for running thousands of parallel simulations for robot learning.\nTo scale robot development workloads across heterogeneous compute, OSMO coordinates the data generation, model training and software/hardware-in-the-loop workflows across distributed environments.\nNVIDIA also announced Isaac Manipulator and Isaac Perceptor — a collection of robotics pretrained models, libraries and reference hardware.\nIsaac Manipulator offers state-of-the-art dexterity and modular AI capabilities for robotic arms, with a robust collection of foundation models and GPU-accelerated libraries. It provides up to an 80x speedup in path planning and zero-shot perception increases efficiency and throughput, enabling developers to automate a greater number of new robotic tasks. Among early ecosystem partners are Yaskawa, Universal Robots, a Teradyne company, PickNik Robotics, Solomon, READY Robotics and Franka Robotics.\nIsaac Perceptor provides multi-camera, 3D surround-vision capabilities, which are increasingly being used in autonomous mobile robots adopted in manufacturing and fulfillment operations to improve efficiency and worker safety as well as reduce error rates and costs. Early adopters include ArcBest, BYD and KION Group as they aim to achieve new levels of autonomy in material handling operations and more.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Healthcare Launches Generative AI Microservices to Advance Drug Discovery, MedTech and Digital Health",
    "link": "https://nvidianews.nvidia.com/news/healthcare-generative-ai-microservices",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNHpWemd4T0ZGSFF6VkRZMDFQVFJDakFSaTBBaWdCTWdPQllRdw=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "New Catalog of NVIDIA NIM and GPU-Accelerated Microservices for Biology, Chemistry, Imaging and Healthcare Data Runs in Every NVIDIA DGX Cloud\nGTC—NVIDIA today launched more than two dozen new microservices that allow healthcare enterprises worldwide to take advantage of the latest advances in generative AI from anywhere and on any cloud.\nThe new suite of NVIDIA healthcare microservices includes optimized NVIDIA NIM™ AI models and workflows with industry-standard APIs, or application programming interfaces, to serve as building blocks for creating and deploying cloud-native applications. They offer advanced imaging, natural language and speech recognition, and digital biology generation, prediction and simulation.\nAdditionally, NVIDIA accelerated software development kits and tools, including Parabricks®, MONAI, NeMo™, Riva and Metropolis, can now be accessed as NVIDIA CUDA-X™ microservices to accelerate healthcare workflows for drug discovery, medical imaging and genomics analysis.\nThe microservices, 25 of which launched today, can accelerate transformation for healthcare companies as generative AI introduces numerous opportunities for pharmaceutical companies, doctors and hospitals. These include screening for trillions of drug compounds to advance medicine, gathering better patient data to aid early disease detection and implementing smarter digital assistants.\nResearchers, developers and practitioners can use the microservices to easily integrate AI into new and existing applications and run them anywhere — from the cloud to on premises — equipping them with copilot capabilities to enhance their life-saving work.\n“For the first time in history, we can represent the world of biology and chemistry in a computer, making computer-aided drug discovery possible,” said Kimberly Powell, vice president of healthcare at NVIDIA. “By helping healthcare companies easily build and manage AI solutions, we’re enabling them to harness the full power and potential of generative AI.”\nNVIDIA NIM Healthcare Microservices for Inferencing\nThe new suite of healthcare microservices includes NVIDIA NIM, which provides optimized inference for a growing collection of models across imaging, medtech, drug discovery and digital health. These can be used for generative biology and chemistry, and molecular prediction. NIM microservices are available through the NVIDIA AI Enterprise 5.0 software platform.\nThe microservices also include a collection of models for drug discovery, including MolMIM for generative chemistry, ESMFold for protein structure prediction and DiffDock to help researchers understand how drug molecules will interact with targets. The VISTA 3D microservice accelerates the creation of 3D segmentation models. The Universal DeepVariant microservice delivers over 50x speed improvement for variant calling in genomic analysis workflows compared to the vanilla DeepVariant implementation running on CPU.\nCadence, a leading computational software company, is integrating NVIDIA BioNeMo™ microservices for AI-guided molecular discovery and lead optimization into its Orion® molecular design platform, which is used for accelerating drug discovery.\nOrion allows researchers at pharmaceutical companies to generate, search and model data libraries with hundreds of billions of compounds. BioNeMo microservices, such as the MolMIM generative chemistry model and the AlphaFold-2 model for protein folding, substantially augment Orion’s design capabilities.\n“Our pharmaceutical and biotechnology customers require access to accelerated resources for molecular simulation,” said Anthony Nicholls, corporate vice president at Cadence. “By leveraging BioNeMo microservices, researchers can generate molecules that are optimized according to scientists’ specific needs.”\nNearly 50 application providers are using the healthcare microservices, as are biotech and pharma companies and platforms, including Amgen, Astellas, DNA Nexus, Iambic Therapeutics, Recursion and Terray, and medical imaging software makers such as V7.\n\"Generative AI is transforming drug discovery by allowing us to build sophisticated models and seamlessly integrate AI into the antibody design process,” said David M. Reese, executive vice president and chief technology officer at Amgen. “Our team is harnessing this technology to create the next generation of medicines that will bring the most value to patients.”\nImproving Patient and Clinician Interactions\nGenerative AI is changing the future of patient care. Hippocratic AI is developing task-specific Generative AI Healthcare Agents, powered by the company’s safety-focused LLM for healthcare, connected to NVIDIA Avatar Cloud Engine microservices and will utilize NVIDIA NIM for low-latency inferencing and speech recognition.\nThese agents talk to patients on the phone to schedule appointments, conduct pre-operative outreach, perform post-discharge follow-ups and more.\n“With generative AI, we have the opportunity to address some of the most pressing needs of the healthcare industry. We can help mitigate widespread staffing shortages and increase access to high-quality care — all while improving outcomes for patients,” said Munjal Shah, cofounder and CEO of Hippocratic AI. “NVIDIA’s technology stack is critical to achieving the conversational speed and fluidity necessary for patients to naturally build an emotional connection with Hippocratic’s Generative AI Healthcare Agents.”\nAbridge is building an AI-powered clinical conversation platform that generates notes drafts, saving clinicians up to three hours a day. Going from raw audio in noisy environments to draft documentation requires many AI technologies to work together seamlessly. Language identification, transcription, alignment and diarization must all take place within seconds and conversations must be structured according to the sorts of medical information contained in each utterance, and powerful language models must be applied to transform the relevant evidence into summaries. The system turns clinical conversations into high-quality, after-visit documentation in real time.\nFlywheel creates models that can be transformed into microservices. The company’s centralized, cloud-based platform powers biopharma companies, life science organizations, healthcare providers and academic medical centers, helping them identify, curate and train medical imaging data to accelerate time to insight.\n“In this rapidly evolving landscape of healthcare technology, the integration of NVIDIA’s generative AI microservices with Flywheel’s platform represents a transformative leap forward,” said Trent Norris, chief product officer at Flywheel. “By leveraging these advanced tools, we are not only enhancing our capabilities in medical imaging and data management but also driving unprecedented acceleration in medical research and patient care outcomes. Flywheel’s AI Factory powered by NVIDIA’s cutting-edge AI solutions meets healthcare customers where they are, pushing the boundaries of what’s possible in the realm of digital health and biopharma.”\nDevelopers can experiment with NVIDIA AI microservices at ai.nvidia.com and deploy production-grade NIM microservices through NVIDIA AI Enterprise 5.0 running on NVIDIA-Certified Systems™ from providers including Dell Technologies, Hewlett Packard Enterprise, Lenovo and Supermicro, leading public cloud platforms including Amazon Web Services (AWS), Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure, and on NVIDIA DGX™ Cloud.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA RTX 2000 Ada Generation GPU Brings Performance, Versatility for Next Era of AI-Accelerated Design and Visualization",
    "link": "https://blogs.nvidia.com/blog/rtx-2000-ada/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUpRVUZZUzBKVkxYZFJZbWxGVFJDakFSaTBBaWdCTWdhbHBKTE11UVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-12T08:00:00.000Z",
    "time": "Feb 12",
    "articleType": "regular",
    "content": "Generative AI is driving change across industries — and to take advantage of its benefits, businesses must select the right hardware to power their workflows.\nThe new NVIDIA RTX 2000 Ada Generation GPU delivers the latest AI, graphics and compute technology to compact workstations, offering up to 1.5x the performance of the previous-generation RTX A2000 12GB in professional workflows.\nFrom crafting stunning 3D environments to streamlining complex design reviews to refining industrial designs, the card’s capabilities pave the way for an AI-accelerated future, empowering professionals to achieve more without compromising on performance or capabilities.\nModern multi-application workflows, such as AI-powered tools, multi-display setups and high-resolution content, put significant demands on GPU memory. With 16GB of memory in the RTX 2000 Ada, professionals can tap the latest technologies and tools to work faster and better with their data.\nPowered by NVIDIA RTX technology, the new GPU delivers impressive realism in graphics with NVIDIA DLSS, delivering ultra-high-quality, photorealistic ray-traced images more than 3x faster than before. In addition, the RTX 2000 Ada enables an immersive experience for enterprise virtual-reality workflows, such as for product design and engineering design reviews.\nWith its blend of performance, versatility and AI capabilities, the RTX 2000 Ada helps professionals across industries achieve efficiencies.\nArchitects and urban planners can use it to accelerate visualization workflows and structural analysis, enhancing design precision. Product designers and engineers using industrial PCs can iterate rapidly on product designs with fast, photorealistic rendering and AI-powered generative design. Content creators can edit high-resolution videos and images seamlessly, and use AI for realistic visual effects and content creation assistance.\nAnd in vital embedded applications and edge computing, the RTX 2000 Ada can power real-time data processing for medical devices, optimize manufacturing processes with predictive maintenance and enable AI-driven intelligence in retail environments.\nExpanding the Reach of NVIDIA RTX\nAmong the first to tap the power and performance of the RTX 2000 Ada are Dassault Systèmes for its SOLIDWORKS applications, Rob Wolkers Design and Engineering, and WSP.\n“The new RTX 2000 Ada Generation GPU boasts impressive features compared to previous generations, with a compact design that offers exceptional performance and versatility,” said Mark Kauffman, assistant vice president and technical lead at WSP. “Its 16GB of RAM is a game-changer, enabling smooth loading of asset-heavy content, and its ability to run applications like Autodesk 3ds Max, Adobe After Effects and Unreal Engine, as well as support path tracing, expands my creative possibilities.”\n“The new NVIDIA RTX 2000 Ada — with its higher-efficiency, next-generation architecture, low power consumption and large frame buffer — will benefit SOLIDWORKS users,” said Olivier Zegdoun, graphics applications research and development director for SOLIDWORKS at Dassault Systèmes. “It delivers excellent performance for designers and engineers to accelerate the development of innovative product experiences with full-model fidelity, even with larger datasets.”\n“Today’s design and visualization workflows demand more advanced compute and horsepower,” said Rob Wolkers, owner and senior industrial design engineer at Rob Wolkers Design and Engineering. “Equipped with next-generation architecture and a large frame buffer, the RTX 2000 Ada Generation GPU improves productivity in my everyday industrial design and engineering workflows, allowing me to work with large datasets in full fidelity and generate renders with more lighting and reflection scenarios 3x faster.”\nElevating Workflows With Next-Generation RTX Technology\nThe NVIDIA RTX 2000 Ada features the latest technologies in the NVIDIA Ada Lovelace GPU architecture, including:\nThird-generation RT Cores: Up to 1.7x faster ray-tracing performance for high-fidelity, photorealistic rendering.\nFourth-generation Tensor Cores: Up to 1.8x AI throughput over the previous generation, with structured sparsity and FP8 precision to enable higher inference performance for AI-accelerated tools and applications.\nCUDA cores: Up to 1.5x the FP32 throughput of the previous generation for significant performance improvements in graphics and compute workloads.\nPower efficiency: Up to a 2x performance boost across professional graphics, rendering, AI and compute workloads, all within the same 70W of power as the previous generation.\nImmersive workflows: Up to 3x performance for virtual-reality workflows over the previous generation.\n16GB of GPU memory: An expanded canvas enables users to tackle larger projects, along with support for error correction code memory to deliver greater computing accuracy and reliability for mission-critical applications.\nDLSS 3: Delivers a breakthrough in AI-powered graphics, significantly boosting performance by generating additional high-quality frames.\nAV1 encoder: Eighth-generation NVIDIA Encoder, aka NVENC, with AV1 support is 40% more efficient than H.264, enabling new possibilities for broadcasters, streamers and video callers.\nNVIDIA RTX Enterprise Driver Delivers New Features, Adds Support for RTX 2000 Ada\nThe latest RTX Enterprise Driver, available now to download, includes a range of features that enhance graphics workflows, along with support for the RTX 2000 Ada.\nThe AI-based, standard dynamic range to high dynamic range tone-mapping feature, called Video TrueHDR, expands the color range and brightness levels when viewing content in Chrome or Edge browsers. With added support for Video Super Resolution and TrueHDR to the NVIDIA NGX software development kit, video quality of low-resolution sources can be enhanced, and SDR content can easily be converted to HDR.\nAdditional features in this release include:\nTensorRT-LLM, an open-source library that optimizes and accelerates inference performance for the latest large language models on NVIDIA GPUs.\nVideo quality improvement and enhanced coding efficiency to video codecs through bit depth expansion techniques and new low-delay B frame.\nAbility to offload work from the CPU to the GPU with the execute indirect extension NVIDIA API for quicker task completion.\nAbility to display the GPU serial number in the NV Control Panel on desktops for easier registration to the NVIDIA AI Enterprise and NVIDIA Omniverse Enterprise platforms.\nThe NVIDIA RTX 2000 Ada is available now through global distribution partners such as Arrow Electronics, Ingram Micro, Leadtek, PNY, Ryoyo Electro and TD SYNNEX, and will be available from Dell Technologies, HP and Lenovo starting in April.\nSee the NVIDIA RTX 2000 Ada at Dassault Systèmes’ 3DEXPERIENCE World\nStop by the Dell, Lenovo and Z by HP booths at Dassault Systèmes’ 3DEXPERIENCE World, running Feb. 11-14 at the Kay Bailey Hutchison Convention Center in Dallas, to view live demos of Dassault Systèmes SOLIDWORKS applications powered by the NVIDIA RTX 2000 Ada.\nAttend the Z by HP session on Tuesday, Feb. 13, where Wolkers will discuss the workflow used to design NEMO, the supercar of submarines.\nRead our solution brief to discover how NVIDIA RTX GPUs supercharge SOLIDWORKS workflows.",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Earth Climate Digital Twin",
    "link": "https://nvidianews.nvidia.com/news/nvidia-announces-earth-climate-digital-twin",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHplblV4VkU1MlZ6bEJPVVZ5VFJDeUFSaWJBaWdCTWdhaEFZNHRFUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "The Weather Company, Central Weather Administration of Taiwan Among First to Adopt New Earth-2 Cloud APIs, Using AI to Speed Creation of High-Resolution Simulations and Visualization of Global Climate, Weather at Groundbreaking 2-Kilometer Scale\nGTC—To accelerate efforts to combat the $140 billion in economic losses due to extreme weather brought on by climate change, NVIDIA today announced its Earth-2 climate digital twin cloud platform for simulating and visualizing weather and climate at unprecedented scale.\nPart of the NVIDIA CUDA-X™ microservices, announced separately today, Earth-2’s new cloud APIs on NVIDIA DGX Cloud™ allow virtually any user to create AI-powered emulations to speed delivery of interactive, high-resolution simulations ranging from the global atmosphere and local cloud cover to typhoons and turbulence.\nWhen combined with proprietary data owned by companies in the $20 billion climate tech industry, the Earth-2 application programming interfaces help users deliver warnings and updated forecasts in seconds compared to the minutes or hours in traditional CPU-driven modeling.\n“Climate disasters are now normal — historic droughts, catastrophic hurricanes and generational floods appear in the news with alarming frequency,” said Jensen Huang, founder and CEO of NVIDIA. “Earth-2 cloud APIs strive to help us better prepare for — and inspire us to act to moderate — extreme weather.”\nGroundbreaking Generative AI for Climate Tech\nEarth-2’s APIs offer AI models and employ a new NVIDIA generative AI model called CorrDiff, using state-of-the-art diffusion modeling, that generates 12.5x higher resolution images than current numerical models 1,000x faster and 3,000x more energy efficiently. It corrects inaccuracies of coarse-resolution forecasts and synthesizes metrics critical to decision-making. CorrDiff is a first-of-its-kind generative AI model to deliver super-resolution, synthesize new metrics of interest to stakeholders, and learn the physics of fine-scale local weather from high-resolution datasets.\nThe Central Weather Administration of Taiwan plans to use these diffusion models to forecast more precise locations of typhoon landfall. When a typhoon warning is launched, the priority is to minimize casualties by carrying out early evacuations based on quality information generated by relevant agencies, including Taiwan’s National Science and Technology Center for Disaster Reduction (NCDR). In the last decade, the death toll due to typhoons has fallen.\n“Taiwan is a critical component of the global supply chain, and flooding risk analysis and evacuation preparedness are core to our mandate,” said Chia-Ping Cheng, administrator of CWA.\nWith more than 136 typhoons striking the island since 2000, using Earth-2 to mitigate these impacts is key to improving the quality and resolution of disaster informatics, NCDR said.\nAnother key component of Earth-2 cloud APIs is NVIDIA Omniverse™, a computing platform that enables individuals and teams to develop Universal Scene Description (OpenUSD)-based 3D workflows and applications.\nThe Weather Company, a global leader in weather data forecasting and insights, plans to integrate its meteorological data and Weatherverse tools with Omniverse, enabling customers building digital twins to better understand and visualize the impact of actual weather conditions for the first time. The Weather Company also plans to explore the use of NVIDIA score-based generative AI for its Weatherverse services, Weather Engine solution for enterprise-level weather intelligence and new high-resolution weather modeling products.\n“To help effectively address current and future weather- and climate-related challenges, it’s critical now more than ever to incorporate reliable, globally scaled real weather data and insights into digital twin environments to better analyze, plan and simulate the impacts of weather,” said Sheri Bachstein, CEO of The Weather Company. “We’ve worked with NVIDIA for years on GPU acceleration of GRAF, our proprietary weather modeling systems, and we plan to adopt Earth-2 APIs to create higher resolution, energy-efficient simulations at a lower cost.”\nOther early adopters of Earth-2 APIs include weather analytics platform companies like Spire and Meteomatics, which can build on their proprietary data sources and data assimilation to produce accurate forecasts, as well as startups Tomorrow.io, north.io and ClimaSens, which are exploring new solutions for climate tech applications.\nEarth-2 APIs use DGX Cloud to provide full-stack acceleration for climate and weather solutions. This includes optimal AI pipelines for models such as FourCastNet, GraphCast and Deep Learning Weather Prediction. It also includes GPU acceleration of numerical weather prediction models like ICON on the latest NVIDIA Grace Hopper™ systems. Running on NVIDIA DGX GH200, HGX™ H100 and OVX™ supercomputers, Earth-2 may provide a path to simulate and visualize the global climate simulations at unprecedented speed and scale.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Grace Hopper Ignites New Era of AI Supercomputing",
    "link": "https://nvidianews.nvidia.com/news/nvidia-grace-hopper-ignites-new-era-of-ai-supercomputing",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXFkVzFXZGpZNVkwbzNWMU4yVFJDb0FSaXNBaWdCTWdZaGRKQ01LUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-12T07:00:00.000Z",
    "time": "May 12",
    "articleType": "regular",
    "content": "From Climate and Weather to Scientific Exploration, Switzerland’s Alps Supercomputer, France’s EXA1-HE Supercomputer and Others to Deliver 200 Exaflops of AI for Groundbreaking Research Using Energy-Efficient Grace-Based Systems\nISC—Driving a fundamental shift in the high-performance computing industry toward AI-powered systems, NVIDIA today announced nine new supercomputers worldwide are using NVIDIA Grace Hopper™ Superchips to speed scientific research and discovery. Combined, the systems deliver 200 exaflops, or 200 quintillion calculations per second, of energy-efficient AI processing power.\nNew Grace Hopper-based supercomputers coming online include EXA1-HE, in France, from CEA and Eviden; Helios at Academic Computer Centre Cyfronet, in Poland, from Hewlett Packard Enterprise (HPE); Alps at the Swiss National Supercomputing Centre, from HPE; JUPITER at the Jülich Supercomputing Centre, in Germany; DeltaAI at the National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign; and Miyabi at Japan’s Joint Center for Advanced High Performance Computing — established between the Center for Computational Sciences at the University of Tsukuba and the Information Technology Center at the University of Tokyo.\nCEA, the French Alternative Energies and Atomic Energy Commission, and Eviden, an Atos Group company, in April announced the delivery of the EXA1-HE supercomputer, based on Eviden’s BullSequana XH3000 technology. The BullSequana XH3000 architecture offers a new, patented warm-water cooling system, while the EXA1-HE is equipped with 477 compute nodes based on Grace Hopper.\n“AI is accelerating research into climate change, speeding drug discovery and leading to breakthroughs in dozens of other fields,” said Ian Buck, vice president of hyperscale and HPC at NVIDIA. “NVIDIA Grace Hopper-powered systems are becoming an essential part of HPC for their ability to transform industries while driving better energy efficiency.”\nIn addition, Isambard-AI and Isambard 3 from the University of Bristol in the U.K. and systems at the Los Alamos National Laboratory and the Texas Advanced Computing Center in the U.S. join a growing wave of NVIDIA Arm-based supercomputers using Grace CPU Superchips and the Grace Hopper platform.\nThe drive to construct new, more efficient, AI-based supercomputers is accelerating as countries around the world recognize the strategic and cultural importance of sovereign AI — investing in domestically owned and hosted data, infrastructure and workforces to foster innovation.\nBringing together the Arm-based NVIDIA Grace CPU and NVIDIA Hopper™ GPU architectures using NVIDIA NVLink®-C2C interconnect technology, GH200 serves as the engine behind scientific supercomputing centers across the globe. Many centers are planning to go from system installation to real science in months instead of years.\nIsambard-AI phase one consists of an HPE Cray EX2500 supercomputer with 168 NVIDIA GH200 Superchips, making it one of the most efficient supercomputers ever built. When the remaining 5,280 NVIDIA Grace Hopper Superchips arrive at the University of Bristol’s National Composites Centre this summer, it will increase performance by about 32x.\n“Isambard-AI positions the U.K. as a global leader in AI, and will help foster open science innovation both domestically and internationally,” said Simon McIntosh-Smith, professor of high-performance computing at the University of Bristol. “Working with NVIDIA, we delivered phase one of the project in record time, and when completed this summer will see a massive jump in performance to advance data analytics, drug discovery, climate research and many more areas.”\nNVIDIA’s accelerated computing platform comprises NVIDIA Hopper architecture-based GPUs, NVIDIA Grace CPU Superchips, NVIDIA Grace Hopper Superchips, NVIDIA Quantum-2 InfiniBand networking and a full suite of NVIDIA AI and HPC software.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Don’t Pass This Up: Day Passes Now Available on GeForce NOW",
    "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-day-pass-cygames/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW1RbVF0ZUU4M1pHbDZOVzFOVFJETUFSajNBU2dCTWdZWlE0eHByUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-07T08:00:00.000Z",
    "time": "Mar 7",
    "articleType": "regular",
    "content": "Gamers can now seize the day with Day Passes, available to purchase for 24-hour continuous access to powerful cloud gaming with all the benefits of a GeForce NOW Ultimate or Priority membership — no commitment required.\nPublisher Cygames brings its next triple-A title to the cloud. Granblue Fantasy: Relink leads eight new games joining the GeForce NOW library this week.\nPlus, an update for GeForce NOW Windows and macOS adds support for G-SYNC in the cloud. By pairing it with new NVIDIA Reflex support for 60 and 120 frames per second streaming options, Ultimate members can experience ultra-low-latency streaming that’s nearly indistinguishable from using a local PC.\nDay Passes offer access to 24 hours of GeForce RTX-powered cloud gaming. Users can get all the benefits of Ultimate and Priority memberships for a day without committing to longer-term monthly memberships, and choose how and when they access the cloud.\nUltimate Day Pass users can stream at either 4K 120 fps, up to 240 fps, or with ultrawide resolutions. Plus, they can get all the same benefits as gamers using NVIDIA GeForce RTX 40 Series GPUs, with access to NVIDIA DLSS 3 and NVIDIA Reflex technologies for the smoothest gameplay and lowest latency, even on underpowered devices. Both Ultimate and Priority Day Pass users can turn RTX ON in supported games for immersive, cinematic gameplay.\nThe Ultimate Day Pass is available for $7.99 and the Priority Day Pass for $3.99. Twenty-four hours of continuous play begins at purchase. Day Passes are available in limited quantities each day, so grab one before the opportunity passes.\nGoing on a grand adventure.\nCygames, known for developing popular online game Granblue Fantasy, brings their full-fledged action role-playing game to GeForce NOW. Granblue Fantasy: Relink is now available for fans to stream across devices.\nSet in the same universe as the web browser and mobile version of the title, Granblue Fantasy: Relink is an ARPG that features many of the beloved characters from the franchise in an all-new original story. Step into the shoes of a captain leading a Skyfaring crew, alongside a scrappy dragon named Vyrn and a mysterious girl named Lyria, as they navigate the Sky Realm, a world of islands drifting in the clouds.\nSlash, shoot and hex treacherous foes with up to three other gaming buddies. GeForce NOW Priority and Ultimate members can become Skyfarers in the cloud with longer game sessions and faster access to GeForce RTX-class servers.\nIt’s available to stream from the cloud this week, along with the following games:\nThe Thaumaturge (New release on Steam, Mar. 4)\nClassified: France ‘44 (New release on Steam, Mar. 5)\nExpeditions: A MudRunner Game (New release on Steam, Mar. 5)\nWinter Survival (New release on Steam, Mar. 6)\nTaxi Life: A City Driving Simulator (New release on Steam, Mar. 7)\nZoria: Age of Shattering (New release on Steam, Mar. 7)\nWhat are you planning to play this weekend? Let us know on X or in the comments below.\n— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) March 6, 2024",
    "favicon": ""
  },
  {
    "title": "NVIDIA DRIVE Powers Next Generation of Transportation — From Cars and Trucks to Robotaxis and Autonomous Delivery Vehicles",
    "link": "https://nvidianews.nvidia.com/news/nvidia-drive-powers-next-generation-transportation",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTNTbnBJVEU4d1dWRkVWM0p1VFJDb0FSaXJBaWdCTWdhZEk1Yk1JUWs=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "BYD, Hyper, XPENG, Plus, Nuro, Waabi and WeRide Adopt DRIVE Thor; Features New Generative AI Capabilities of Blackwell Architecture\nGTC—NVIDIA today announced that leading companies across the transportation sector have adopted the NVIDIA DRIVE Thor™ centralized car computer to power their next-generation consumer and commercial fleets — from new energy vehicles and trucks to robotaxis, robobuses and last-mile autonomous delivery vehicles.\nDRIVE Thor is an in-vehicle computing platform architected for generative AI applications, which are becoming paramount within the automotive industry. The system, the successor to DRIVE Orin, can deliver feature-rich cockpit capabilities, plus safe and secure highly automated and autonomous driving, all on a centralized platform. This next-generation AV platform will integrate the new NVIDIA Blackwell architecture, designed for transformer, LLM and generative AI workloads, which was announced during NVIDIA founder and CEO Jensen Huang’s keynote at GTC.\n“Accelerated compute has led to transformative breakthroughs, including generative AI, which is redefining autonomy and the global transportation industry at large,” said Xinzhou Wu, vice president of automotive at NVIDIA. “DRIVE Orin continues to be the AI car computer of choice for today’s intelligent fleets, but now we’re seeing mobility leaders looking ahead to bring NVIDIA DRIVE Thor into their next-generation, AI-enabled vehicle roadmaps.”\nNext-Gen AI Fleets Embrace Accelerated Compute\nNVIDIA DRIVE Thor is poised to revolutionize the automotive landscape, ushering in an era where generative AI defines the driving experience. At GTC, several leading EV makers are revealing their next-gen AI vehicle fleets powered by DRIVE Thor:\nBYD, the world’s largest electric vehicle maker, is expanding its ongoing collaboration with NVIDIA from the car to the cloud. In addition to building its next-generation EV fleets on DRIVE Thor, BYD plans to use NVIDIA’s AI infrastructure for cloud-based AI development and training technologies, along with the NVIDIA Isaac™ and NVIDIA Omniverse™ platforms to develop tools and applications for virtual factory planning and retail configurators.\nHyper, a premium luxury brand owned by GAC AION, announced it has selected DRIVE Thor for its next-generation EVs, which will begin production in 2025 with level 4 driving capabilities. Hyper is currently using NVIDIA DRIVE Orin to power its flagship model Hyper GT, which features advanced level 2+ driving capabilities.\nXPENG has also announced it will use the NVIDIA DRIVE Thor platform as the AI brain of its next-generation EV fleets. The next-gen car computer will power the EV maker’s proprietary XNGP AI-assisted driving system, enabling autonomous driving and parking capabilities, driver and passenger monitoring and other functionalities.\nThese EV makers join Li Auto and ZEEKR, which have already announced they’re building their future vehicle roadmap on DRIVE Thor.\nPowering Long-Haul Trucks, Delivery Vehicles, Robotaxis\nBeyond passenger vehicles, DRIVE Thor caters to the diverse needs of other segments where high-performance compute and AI are essential for ensuring safe, secure driving operations, including trucking, robotaxis, goods delivery vehicles and more.\nA number of these mobility providers are leading the charge at GTC, including:\nNuro, which develops level 4 autonomous driving technology for commercial and consumer vehicles, has selected DRIVE Thor to power the Nuro Driver™, an integrated autonomous driving system consisting of Nuro’s proprietary AI-first software and sensors paired with NVIDIA automotive-grade compute and networking hardware. The system will begin testing later this year.\nPlus, a global provider of autonomous driving software solutions, announced that future generations of its level 4 solution, SuperDrive™, will run on the automotive-grade, safety-compliant DRIVE Thor centralized computer. Plus will leverage the compute performance of DRIVE Thor within its autonomous driving system to understand the world around the truck and make safe driving decisions.\nWaabi, which is building AI for self-driving, is leveraging DRIVE Thor to deliver the first generative AI-powered autonomous trucking solution to market. The company plans to integrate DRIVE Thor in its Waabi Driver to power safe and reliable autonomous trucks at scale.\nWeRide, in cooperation with tier 1 partner Lenovo Vehicle Computing, is creating several level 4 autonomous driving solutions for commercial applications built on DRIVE Thor. Integrated within Lenovo’s first autonomous driving domain controller AD1, this solution will be used for a wide range of urban-centered use cases, where functional safety, redundant safety design, fusion and scalability are a must.\nThe Great and Powerful DRIVE Thor\nSlated for production vehicles as early as next year, DRIVE Thor will harness the new NVIDIA Blackwell architecture, which features a generative AI engine and other cutting-edge capabilities, and wields 1,000 teraflops of performance to help ensure safe and secure autonomous machines.\nTo learn the latest on NVIDIA DRIVE, watch NVIDIA’s GTC keynote. Register for GTC to attend sessions from NVIDIA and transportation industry leaders through March 21.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Unveils 6G Research Cloud Platform to Advance Wireless Communications With AI",
    "link": "https://nvidianews.nvidia.com/news/nvidia-unveils-6g-research-cloud-platform-to-advance-wireless-communications-with-ai",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUpTalJ4VmtFMlowaEJhMXBaVFJDb0FSaXNBaWdCTWdZQklJYXZuQVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Ansys, Keysight, Nokia, Samsung Among First to Use NVIDIA Aerial Omniverse Digital Twin, Aerial CUDA-Accelerated RAN and Sionna Neural Radio Framework to Help Realize the Future of Telecommunications\nGTC—NVIDIA today announced a 6G research platform that empowers researchers with a novel approach to develop the next phase of wireless technology.\nThe NVIDIA 6G Research Cloud platform is open, flexible and interconnected, offering researchers a comprehensive suite to advance AI for radio access network (RAN) technology. The platform allows organizations to accelerate the development of 6G technologies that will connect trillions of devices with the cloud infrastructures, laying the foundation for a hyper-intelligent world supported by autonomous vehicles, smart spaces and a wide range of extended reality and immersive education experiences and collaborative robots.\nAnsys, Arm, ETH Zurich, Fujitsu, Keysight, Nokia, Northeastern University, Rohde & Schwarz, Samsung, SoftBank Corp. and Viavi are among its first adopters and ecosystem partners.\n“The massive increase in connected devices and host of new applications in 6G will require a vast leap in wireless spectral efficiency in radio communications,” said Ronnie Vasishta, senior vice president of telecom at NVIDIA. “Key to achieving this will be the use of AI, a software-defined, full-RAN reference stack and next-generation digital twin technology.”\nThe NVIDIA 6G Research Cloud platform consists of three foundational elements:\nNVIDIA Aerial Omniverse Digital Twin for 6G: A reference application and developer sample that enables physically accurate simulations of complete 6G systems, from a single tower to city scale. It incorporates software-defined RAN and user-equipment simulators, along with realistic terrain and object properties. Using the Omniverse Aerial Digital Twin, researchers will be able to simulate and build base-station algorithms based on site-specific data and to train models in real time to improve transmission efficiency.\nNVIDIA Aerial CUDA-Accelerated RAN: A software-defined, full-RAN stack that offers significant flexibility for researchers to customize, program and test 6G networks in real time.\nNVIDIA Sionna Neural Radio Framework: A framework that provides seamless integration with popular frameworks like PyTorch and TensorFlow, leveraging NVIDIA GPUs for generating and capturing data and training AI and machine learning models at scale. This also includes NVIDIA Sionna, the leading link-level research tool for AI/ML-based wireless simulations.\nIndustry-leading researchers can use all elements of the 6G development research cloud platform to advance their work.\n“The future convergence of 6G and AI holds the promise of a transformative technological landscape,” said Charlie Zhang, senior vice president of Samsung Research America. “This will bring seamless connectivity and intelligent systems that will redefine our interactions with the digital world, ushering in an era of unparalleled innovation and connectivity.”\nTesting and simulation will play an essential role in developing the next generation of wireless technology. Leading providers in this space are working with NVIDIA to contribute to the new requirements of AI with 6G.\n“Ansys is committed to advancing the mission of the 6G Research Cloud by seamlessly integrating the cutting-edge Ansys Perceive EM solver into the Omniverse ecosystem,” said Shawn Carpenter, program director of 5G/6G and space at Ansys. “Perceive EM revolutionizes the creation of digital twins for 6G systems. Undoubtedly, the convergence of NVIDIA and Ansys technologies will pave the way toward AI-enabled 6G communication systems.”\n“Access to wireless-specific design tools is limited yet needed to build robust AI,” said Kailash Narayanan, president and general manager of Keysight Communications Solutions Group. “Keysight is pleased to bring its wireless network expertise to enable the next generation of innovation in 6G communications networks.”",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Releases Digital Human Microservices, Paving Way for Future of Generative AI Avatars",
    "link": "https://nvidianews.nvidia.com/news/digital-humans-ace-generative-ai-microservices",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWhUSFJVWTFKaVZIaEdlbTlTVFJDb0FSaXNBaWdCTWdhbE5JQ3RwUU0=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-02T07:00:00.000Z",
    "time": "Jun 2",
    "articleType": "regular",
    "content": "COMPUTEX—NVIDIA today announced the general availability of NVIDIA ACE generative AI microservices to accelerate the next wave of digital humans, as well as new generative AI breakthroughs coming soon to the platform.\nCompanies in customer service, gaming and healthcare are the first to adopt ACE technologies to simplify creating, animating and operating lifelike digital humans across customer service, telehealth, gaming and entertainment.\nThe suite of NVIDIA ACE digital human generative AI technologies now generally available includes:\nNVIDIA Riva ASR, TTS and NMT — for automatic speech recognition, text-to-speech conversion and translation\nNVIDIA Nemotron LLM — for language understanding and contextual response generation\nNVIDIA Audio2Face™ — for realistic facial animation based on audio tracks\nNVIDIA Omniverse RTX — for real-time, path-traced realistic skin and hair\nNVIDIA Audio2Gesture™ — for generating body gestures based on audio tracks, available soon\nNVIDIA Nemotron-3 4.5B — a new small language model (SLM) purpose-built for low-latency, on-device RTX AI PC inference\n“Digital humans will revolutionize industries,” said Jensen Huang, founder and CEO of NVIDIA. “Breakthroughs in multi-modal large language models and neural graphics — delivered by NVIDIA ACE to our ecosystem of developers — are bringing us closer to a future of intent-driven computing, where interacting with computers is as natural as interacting with humans.”\nDigital Humans Come to 100 Million RTX AI PCs\nTo date, NVIDIA has provided ACE as NVIDIA NIM™ microservices for developers to operate in data centers. Now NVIDIA is building ACE PC NIM microservices for deployment across the installed base of 100 million RTX AI PCs and laptops.\nThe new NVIDIA AI Inference Manager software development kit simplifies the deployment of ACE to PCs. It preconfigures the PC with the necessary AI models, engines and dependencies while orchestrating AI inference seamlessly across PCs and the cloud.\nAn updated version of the Covert Protocol tech demo, developed in collaboration with Inworld AI, is being shown at the COMPUTEX trade show. Using Audio2Face and Riva ASR running locally on GeForce RTX™ PCs, the demo allows players to interact and influence digital-human non-playable characters (NPCs) with conversational language to complete their mission.\nDigital Human Ecosystem Expands With Latest ACE Technologies\nACE is making waves with developers building a variety of applications from companies such as Aww Inc., Dell Technologies, Gumption, Hippocratic AI, Inventec, OurPalm, Perfect World Games, Reallusion, ServiceNow, Soulbotix, SoulShell and UneeQ.\nAww Inc., a pioneering virtual human company based in Japan, launched its first virtual celebrity, Imma, in 2018. Imma has since become the face of major global brands in more than 50 countries. Now, Aww Inc. plans to leverage ACE Audio2Face microservices for real-time animation, enabling a highly interactive communication experience with its users.\nPerfect World Games, a game developer and publisher, is adopting ACE in its new mythological wilderness tech demo, Legends. Players can interact with a fully interactive, realistic, multilingual, AI NPC in both English and Mandarin. Using NVIDIA Audio2Face NIM, the character’s audio responses generate realistic facial animation in real time.\nInventec, a major technology company that is investing heavily in AI, is using NVIDIA Audio2Face NIM to enhance its healthcare AI agent within the VRSTATE platform. The integration provides a more engaging, comforting virtual consultation experience. At COMPUTEX, Inventec is showcasing an AI agent that can help patients access information about their health.\nServiceNow, the AI platform for business transformation, recently showcased ACE NIM in a generative AI service agent demo for its Now Assist Gen AI Experience, highlighting the potential for digital avatars to enhance customer and employee interactions across industries including retail, travel and more.\nDell Technologies unveiled its cutting-edge Dell Generative AI Solution for Digital Assistants at Dell Technologies World last month. The offering allows businesses to leverage intelligent digital assistants that engage customers through natural conversations across various industries such as retail, healthcare and customer service.\nNVIDIA Celebrates Digital Human Startups at COMPUTEX 2024\nNVIDIA art teams used generative AI tools built on ACE, including Synthesia and Hour One, to produce a “digital Jensen” avatar that was generated by video from text.\nThe multilingual avatar featured Huang’s unique voice and style, generated by ElevenLabs’ proprietary AI speech and voice technology in Mandarin Chinese and English. NVIDIA also collaborated with Voicemod, an NVIDIA Inception member specializing in AI voice technology, to compose the ending theme song of Huang’s keynote.\nWatch Huang’s COMPUTEX keynote to see the latest in ACE content.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Wide Open: NVIDIA Accelerates Inference on Meta Llama 3",
    "link": "https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVNZMHcwV0ZveGVWQkNUV1JmVFJDakFSaTBBaWdCTWdZTm9wWm5PZ2M=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-04-18T07:00:00.000Z",
    "time": "Apr 18",
    "articleType": "regular",
    "content": "NVIDIA today announced optimizations across all its platforms to accelerate Meta Llama 3, the latest generation of the large language model (LLM).\nThe open model combined with NVIDIA accelerated computing equips developers, researchers and businesses to innovate responsibly across a wide variety of applications.\nMeta engineers trained Llama 3 on computer clusters packing 24,576 NVIDIA H100 Tensor Core GPUs, linked with RoCE and NVIDIA Quantum-2 InfiniBand networks.\nTo further advance the state of the art in generative AI, Meta recently described plans to scale its infrastructure to 350,000 H100 GPUs.\nPutting Llama 3 to Work\nVersions of Llama 3, accelerated on NVIDIA GPUs, are available today for use in the cloud, data center, edge and PC.\nFrom a browser, developers can try Llama 3 at ai.nvidia.com. It’s packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.\nBusinesses can fine-tune Llama 3 with their data using NVIDIA NeMo, an open-source framework for LLMs that’s part of the secure, supported NVIDIA AI Enterprise platform. Custom models can be optimized for inference with NVIDIA TensorRT-LLM and deployed with NVIDIA Triton Inference Server.\nTaking Llama 3 to Devices and PCs\nLlama 3 also runs on NVIDIA Jetson Orin for robotics and edge computing devices, creating interactive agents like those in the Jetson AI Lab.\nWhat’s more, NVIDIA RTX and GeForce RTX GPUs for workstations and PCs speed inference on Llama 3. These systems give developers a target of more than 100 million NVIDIA-accelerated systems worldwide.\nGet Optimal Performance with Llama 3\nBest practices in deploying an LLM for a chatbot involves a balance of low latency, good reading speed and optimal GPU use to reduce costs.\nSuch a service needs to deliver tokens — the rough equivalent of words to an LLM — at about twice a user’s reading speed which is about 10 tokens/second.\nApplying these metrics, a single NVIDIA H200 Tensor Core GPU generated about 3,000 tokens/second — enough to serve about 300 simultaneous users — in an initial test using the version of Llama 3 with 70 billion parameters.\nThat means a single NVIDIA HGX server with eight H200 GPUs could deliver 24,000 tokens/second, further optimizing costs by supporting more than 2,400 users at the same time.\nFor edge devices, the version of Llama 3 with eight billion parameters generated up to 40 tokens/second on Jetson AGX Orin and 15 tokens/second on Jetson Orin Nano.\nAn active open-source contributor, NVIDIA is committed to optimizing community software that helps users address their toughest challenges. Open-source models also promote AI transparency and let users broadly share work on AI safety and resilience.",
    "favicon": ""
  },
  {
    "title": "Picture This: Getty Images Releases Generative AI By iStock Powered by NVIDIA Picasso",
    "link": "https://blogs.nvidia.com/blog/nvidia-picasso-istock-generative-ai/",
    "image": "https://news.google.com/api/attachments/CC8iNkNnNVFMVzVNV2xwUWF6VkpkVU0wVFJDakFSaTBBaWdCTWc0QkVJS0xTU1BnOWJnRk5hclpIQQ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "‘Accelerate Everything,’ NVIDIA CEO Says Ahead of COMPUTEX",
    "link": "https://blogs.nvidia.com/blog/computex-2024-jensen-huang/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXdlWEV6Ym5OTVdVMVFlbXByVFJDNkFSaU9BaWdCTWdhUllZZ3dMUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-02T07:00:00.000Z",
    "time": "Jun 2",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA AI Workbench Speeds Adoption of Custom Generative AI for World’s Enterprises",
    "link": "https://nvidianews.nvidia.com/news/nvidia-ai-workbench-speeds-adoption-of-custom-generative-ai-for-worlds-enterprises",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNWlNMGwwVEU4NFUxcHFWRXRsVFJDakFSaTBBaWdCTWdrSklZTFZ6S09wcGdJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "New Developer Toolkit Introduces Simplified Model Tuning and Deployment on NVIDIA AI Platforms — From PCs and Workstations to Enterprise Data Centers, Public Clouds and NVIDIA DGX Cloud\nSIGGRAPH—NVIDIA today announced NVIDIA AI Workbench, a unified, easy-to-use toolkit that allows developers to quickly create, test and customize pretrained generative AI models on a PC or workstation — then scale them to virtually any data center, public cloud or NVIDIA DGX™ Cloud.\nAI Workbench removes the complexity of getting started with an enterprise AI project. Accessed through a simplified interface running on a local system, it allows developers to customize models from popular repositories like Hugging Face, GitHub and NVIDIA NGC™ using custom data. The models can then be shared easily across multiple platforms.\n“Enterprises around the world are racing to find the right infrastructure and build generative AI models and applications,” said Manuvir Das, vice president of enterprise computing at NVIDIA. “NVIDIA AI Workbench provides a simplified path for cross-organizational teams to create the AI-based applications that are increasingly becoming essential in modern business.”\nA New Era for AI Developers\nWhile hundreds of thousands of pretrained models are now available, customizing them with the many open-source tools can require hunting through multiple online repositories for the right framework, tools and containers, and employing the right skills to customize a model for a specific use case.\nWith NVIDIA AI Workbench, developers can customize and run generative AI in just a few clicks. It allows them to pull together all necessary enterprise-grade models, frameworks, software development kits and libraries from open-source repositories and the NVIDIA AI platform into a unified developer toolkit.\nLeading AI infrastructure providers — including Dell Technologies, Hewlett Packard Enterprise, HP Inc., Lambda, Lenovo and Supermicro — are embracing AI Workbench for its ability to augment their latest generation of multi-GPU-capable desktop workstations, high-end mobile workstations and virtual workstations.\nDevelopers with a Windows or Linux-based NVIDIA RTX™ PC or workstation will also be able to initiate, test and fine-tune enterprise-grade generative AI projects on their local RTX systems, and easily access data center and cloud computing resources to scale as needed.\nNew NVIDIA AI Enterprise 4.0 Software Advances AI Deployment\nTo further accelerate the adoption of generative AI, NVIDIA announced the latest version of its enterprise software platform, NVIDIA AI Enterprise 4.0. It gives businesses the tools needed to adopt generative AI, while also offering the security and API stability required for reliable production deployments.\nNewly supported software and tools in NVIDIA AI Enterprise that help streamline generative AI deployment include:\nNVIDIA NeMo™, a cloud-native framework to build, customize and deploy large language models. With NeMo, NVIDIA AI Enterprise provides end-to-end support for creating and customizing LLM applications.\nNVIDIA Triton™ Management Service, which helps automate and optimize production deployments. It allows enterprises to automatically deploy multiple NVIDIA Triton Inference Server instances in Kubernetes with model orchestration for efficient operation of scalable AI.\nNVIDIA Base Command Manager Essentials cluster management software, which helps enterprises maximize performance and utilization of AI servers across data center, multi-cloud and hybrid-cloud environments.\nNVIDIA AI Enterprise software — which lets users build and run NVIDIA AI-enabled solutions across the cloud, data center and edge — is certified to run on mainstream NVIDIA-Certified Systems™, NVIDIA DGX systems, all major cloud platforms and newly announced NVIDIA RTX workstations.\nLeading software companies ServiceNow and Snowflake, as well as infrastructure provider Dell Technologies, which offers Dell Generative AI Solutions, recently announced they are collaborating with NVIDIA to enable new generative AI solutions and services on their platforms. The integration of NVIDIA AI Enterprise 4.0 and NVIDIA NeMo provides a foundation for production-ready generative AI for customers.\nNVIDIA AI Enterprise 4.0 will be integrated into partner marketplaces, including AWS Marketplace, Google Cloud and Microsoft Azure, as well as through NVIDIA cloud partner Oracle Cloud Infrastructure.\nAdditionally, MLOps providers, including Azure Machine Learning, ClearML, Domino Data Lab, Run:AI, and Weights & Biases, are adding seamless integration with the NVIDIA AI platform to simplify production-grade generative AI model development.\n“Dell Technologies and NVIDIA are committed to helping enterprises build purpose-built AI models to access the immense opportunity of generative AI. With NVIDIA AI Workbench, developers can take advantage of the full Dell Generative AI Solutions portfolio to customize models on PCs, workstations and data center infrastructure.” — Meghana Patwardhan, vice president of commercial client products at Dell Technologies\n“Most enterprises do not have the expertise, budget and data center resources to manage the high complexity of AI software and systems. We look forward to NVIDIA AI Workbench’s potential to simplify generative AI project creation with one-click training and deployment on the HPE GreenLake edge-to-cloud platform.” — Evan Sparks, chief product officer for AI at HPE\n“As a workstation market leader offering the performance and efficiency needed for the most demanding data science and AI models, we have a long history collaborating with NVIDIA. HP is embracing the next generation of high-performance systems, coupled with NVIDIA RTX Ada Generation GPUs and NVIDIA AI Workbench, and bringing the power of generative AI to our enterprise customers and helping move AI workloads between the cloud and locally.” — Jim Nottingham, senior vice president of advanced computing solutions at HP Inc.\n“Lenovo and NVIDIA are helping customers overcome deployment complexities and more easily implement generative AI to deliver transformative services and products to the market. NVIDIA AI Workbench and the Lenovo AI-ready portfolio enable developers to leverage the power of their smart devices and scale across edge-to-cloud infrastructure.” — Rob Herman, vice president and general manager of Lenovo Workstation & Client AI\n“The longstanding VMware and NVIDIA partnership has helped unlock the power of AI for every business by delivering an end-to-end enterprise platform optimized for AI workloads. Together, we are making generative AI more accessible and easier to implement in the enterprise. With AI Workbench, NVIDIA is giving developers a set of powerful tools to help enterprises accelerate gen AI adoption. With the new NVIDIA AI Workbench, development teams can seamlessly move AI workloads from the desktop to production.” — Chris Wolf, vice president of VMware AI Labs",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "US National Science Foundation Launches National AI Research Resource Pilot",
    "link": "https://blogs.nvidia.com/blog/national-ai-research-resource-pilot/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUVOWFo1TUZwMFRHUXpZMUZrVFJDakFSaTBBaWdCTWdrVm9wTDBOQ2VpcndF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-24T08:00:00.000Z",
    "time": "Jan 24",
    "articleType": "regular",
    "content": "In a major stride toward building a shared national research infrastructure, the U.S. National Science Foundation has launched the National Artificial Intelligence Research Resource pilot program with significant support from NVIDIA.\nThe initiative aims to broaden access to the tools needed to power responsible AI discovery and innovation. It was announced Wednesday in partnership with 10 other federal agencies as well as private-sector, nonprofit and philanthropic organizations.\n“The breadth of partners that have come together for this pilot underscores the urgency of developing a National AI Research Resource for the future of AI in America,” said NSF Director Sethuraman Panchanathan. “By investing in AI research through the NAIRR pilot, the United States unleashes discovery and impact and bolsters its global competitiveness.”\nNVIDIA’s commitment of $30 million in technology contributions over two years is a key factor in enlarging the scale of the pilot, fueling the potential for broader achievements and accelerating the momentum toward full-scale implementation.\n“The NAIRR is a vision of a national research infrastructure that will provide access to computing, data, models and software to empower researchers and communities,” said Katie Antypas, director of the Office of Advanced Cyberinfrastructure at the NSF.\n“Our primary goals for the NAIRR pilot are to support fundamental AI research and domain-specific research applying AI, reach broader communities, particularly those currently unable to participate in the AI innovation ecosystem, and refine the design for the future full NAIRR,” Antypas added.\n“AI is increasingly defining our era, and its potential can best be fulfilled with broad access to its transformative capabilities,” said NVIDIA founder and CEO Jensen Huang.\n“Partnerships are really at the core of the NAIRR pilot,” said Tess DeBlanc-Knowles, NSF’s special assistant to the director for artificial intelligence.\n“It’s been incredibly impressive to see this breadth of partners come together in these 90 days, bringing together government, industry, nonprofits and philanthropies,” she added. “Our industry and nonprofit partners are bringing critical expertise and resources, which are essential to advance AI and move forward with trustworthy AI initiatives.”\nNVIDIA’s collaboration with scientific centers aims to significantly scale up educational and workforce training programs, enhancing AI literacy and skill development across the scientific community.\nNVIDIA will harness insights from researchers using its platform, offering an opportunity to refine and enhance the effectiveness of its technology for science, and supporting continuous advancement in AI applications.\n“With NVIDIA AI software and supercomputing, the scientists, researchers and engineers of the extended NSF community will be able to utilize the world’s leading infrastructure to fuel a new generation of innovation,” Huang said.\nThe Foundation for Modern AI\nAccelerating both AI research and research done with AI, NVIDIA’s contributions include NVIDIA DGX Cloud AI supercomputing resources and NVIDIA AI Enterprise software.\nOffering full-stack accelerated computing from systems to software, NVIDIA AI provides the foundation for generative AI, with significant adoption across research and industries.\nBroad Support Across the US Government\nAs part of this national endeavor, the NAIRR pilot brings together a coalition of government partners, showcasing a unified approach to advancing AI research.\nIts partners include the U.S. National Science Foundation, U.S. Department of Agriculture, U.S. Department of Energy, U.S. Department of Veterans Affairs, National Aeronautics and Space Administration, National Institutes of Health, National Institute of Standards and Technology, National Oceanic and Atmospheric Administration, Defense Advanced Research Projects Agency, U.S. Patent and Trade Office and the U.S. Department of Defense.\nThe NAIRR pilot builds on the United States’ rich history of leading large-scale scientific endeavors, such as the creation of the internet, which, in turn, led to the advancement of AI.\nNAIRR promises to drive innovations across various sectors, from healthcare to environmental science, positioning the U.S. at the forefront of global AI advancements.\nThe launch meets a goal outlined in Executive Order 14110, signed by President Biden in October 2023, directing NSF to launch a pilot for the NAIRR within 90 days.\nThe NAIRR pilot will provide access to advanced computing, datasets, models, software, training and user support to U.S.-based researchers and educators.\n“Smaller institutions, rural institutions, institutions serving underrepresented populations are key communities we’re trying to reach with the NAIRR,” said Antypas. “These communities are less likely to have resources to build their own computing or data resources.”\nPaving the Way for Future Investments\nAs the pilot expedites the proof of concept, future investments in the NAIRR will democratize access to AI innovation and support critical work advancing the development of trustworthy AI.\nThe pilot will initially support AI research to advance safe, secure and trustworthy AI as well as the application of AI to challenges in healthcare and environmental and infrastructure sustainability.\nResearchers can apply for initial access to NAIRR pilot resources through the NSF. The NAIRR pilot welcomes additional private-sector and nonprofit partners.\nThose interested are encouraged to reach out to NSF at nairr_pilot@nsf.gov.",
    "favicon": ""
  },
  {
    "title": "NVIDIA and Hugging Face to Connect Millions of Developers to Generative AI Supercomputing",
    "link": "https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXlkRXcyZFVKM1dYSmxTR3RyVFJDb0FSaXNBaWdCTWdhUnNaTFZ1QVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "Integration of NVIDIA DGX Cloud in Hugging Face Platform to Speed LLM Training and Tuning, Simplifies Customizing Models for Nearly Every Industry\nSIGGRAPH—NVIDIA and Hugging Face today announced a partnership that will put generative AI supercomputing at the fingertips of millions of developers building large language models (LLMs) and other advanced AI applications.\nBy giving developers access to NVIDIA DGX™ Cloud AI supercomputing within the Hugging Face platform to train and tune advanced AI models, the combination will help supercharge industry adoption of generative AI using LLMs that are custom-tailored with business data for industry-specific applications, including intelligent chatbots, search and summarization.\n“Researchers and developers are at the heart of generative AI that is transforming every industry,” said Jensen Huang, founder and CEO of NVIDIA. “Hugging Face and NVIDIA are connecting the world’s largest AI community with NVIDIA’s AI computing platform in the world’s leading clouds. Together, NVIDIA AI computing is just a click away for the Hugging Face community.”\nAs part of the collaboration, Hugging Face will offer a new service — called Training Cluster as a Service — to simplify the creation of new and custom generative AI models for the enterprise. Powered by NVIDIA DGX Cloud, the service will be available in the coming months.\n“People around the world are making new connections and discoveries with generative AI tools, and we’re still only in the early days of this technology shift,” said Clément Delangue, co-founder and CEO of Hugging Face. “Our collaboration will bring NVIDIA’s most advanced AI supercomputing to Hugging Face to enable companies to take their AI destiny into their own hands with open source and with speed they need to contribute to what’s coming next.”\nSupercharging LLM Customization and Training Within Hugging Face\nThe Hugging Face platform lets developers build, train and deploy state-of-the-art AI models using open-source resources. Over 15,000 organizations use Hugging Face, and its community has shared over 250,000 models and 50,000 datasets.\nThe DGX Cloud integration with Hugging Face will bring one-click access to NVIDIA’s multi-node AI supercomputing platform. With DGX Cloud, Hugging Face users will be able to connect to NVIDIA AI supercomputing, providing the software and infrastructure needed to rapidly train and tune foundation models with unique data to drive a new wave of enterprise LLM development. With Training Cluster as a Service, powered by DGX Cloud, companies will be able to leverage their unique data for Hugging Face to create uniquely efficient models in record time.\nDGX Cloud Speeds Development and Customization for Massive Models\nEach instance of DGX Cloud features eight NVIDIA H100 or A100 80GB Tensor Core GPUs for a total of 640GB of GPU memory per node. NVIDIA Networking provides a high-performance, low-latency fabric that ensures workloads can scale across clusters of interconnected systems to meet the performance requirements of advanced AI workloads.\nSupport from NVIDIA experts is included with DGX Cloud to help customers optimize their models and quickly resolve development challenges.\nDGX Cloud infrastructure is hosted by leading NVIDIA cloud service provider partners.\nThe NVIDIA DGX Cloud integration with Hugging Face is expected to be available in the coming months.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Announces New Switches Optimized for Trillion-Parameter GPU Computing and AI Infrastructure",
    "link": "https://nvidianews.nvidia.com/news/networking-switches-gpu-computing-ai",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNTBSR2RFUWxwa01VMDJPRk0xVFJDb0FSaXNBaWdCTWdtWm9vNHl0V2FTc0FF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "NVIDIA Quantum-X800 InfiniBand for Highest-Performance AI-Dedicated Infrastructure\nNVIDIA Spectrum-X800 Ethernet for AI-Optimized Networking in Every Data Center\nNVIDIA Software Distributes Computing Across Blackwell, New Switches and BlueField-3 SuperNICs to Boost AI, Data Processing, HPC and Cloud Workloads\nGTC—NVIDIA today announced a new wave of networking switches, the X800 series, designed for massive-scale AI.\nThe world’s first networking platforms capable of end-to-end 800Gb/s throughput, NVIDIA Quantum-X800 InfiniBand and NVIDIA Spectrum™-X800 Ethernet push the boundaries of networking performance for computing and AI workloads. They feature software that further accelerates AI, cloud, data processing and HPC applications in every type of data center, including those that incorporate the newly released NVIDIA Blackwell architecture-based product lineup.\n“NVIDIA Networking is central to the scalability of our AI supercomputing infrastructure,” said Gilad Shainer, senior vice president of Networking at NVIDIA. “NVIDIA X800 switches are end-to-end networking platforms that enable us to achieve trillion-parameter-scale generative AI essential for new AI infrastructures.”\nInitial adopters of Quantum InfiniBand and Spectrum-X Ethernet include Microsoft Azure and Oracle Cloud Infrastructure.\n“AI is a powerful tool to turn data into knowledge. Behind this transformation is the evolution of data centers into high-performance AI engines with increased demands for networking infrastructure,” said Nidhi Chappell, Vice President of AI Infrastructure at Microsoft Azure. “With new integrations of NVIDIA networking solutions, Microsoft Azure will continue to build the infrastructure that pushes the boundaries of cloud AI.”\nCoreWeave is also among early adopters.\nNext Standard for Extreme Performance\nThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the NVIDIA Quantum Q3400 switch and the NVIDIA ConnectX®-8 SuperNIC™, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with NVIDIA’s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the NVIDIA BlueField®-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multi-tenant generative AI clouds and large enterprises.\nSpectrum-X800 optimizes network performance, facilitating faster processing, analysis, and execution of AI workloads, thereby expediting the development, deployment, and time to market of AI solutions. Designed specifically for multi-tenant environments, Spectrum-X800 ensures performance isolation for each tenant's AI workloads to maintain optimal and consistent performance levels, enhancing customer satisfaction and service quality.\nNVIDIA provides a comprehensive suite of network acceleration libraries, software development kits and management software to optimize performance for trillion-parameter AI models.\nThis includes NVIDIA Collective Communications Library (NCCL), which extends GPU parallel computing tasks to the Quantum-X800 network fabric, taking advantage of its powerful In-Network Computing capabilities with SHARPv4 supporting FP8, supercharging performance for large model training and generative AI.\nNVIDIA’s full-stack software approach provides advanced programmability, making data center networks more flexible, reliable and responsive, ultimately increasing overall operational efficiency and supporting the needs of modern applications and services.\nNext year, Quantum-X800 and Spectrum-X800 will be available from a wide range of leading infrastructure and system vendors around the world, including Aivres, DDN, Dell Technologies, Eviden, Hitachi Vantara, Hewlett Packard Enterprise, Lenovo, Supermicro and VAST Data.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Twitch, OBS and NVIDIA to Release Multi-Encode Livestreaming",
    "link": "https://blogs.nvidia.com/blog/twitch-multiencode-av1-livestreaming/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW1Va0ZZTFZCdmRXNXZUVkJTVFJDb0FSaXNBaWdCTWdZbEJJeXVvUVE=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "Twitch, OBS and NVIDIA are leveling up livestreaming technology with the new Twitch Enhanced Broadcasting beta, powered by GeForce RTX GPUs. Available in a few days, streamers will be able to stream multiple encodes concurrently, providing optimal viewing experiences for all viewers.\nToday, many streamers must choose between higher resolution and reliable streaming. High-quality video provides more enjoyable viewing experiences but causes streams to buffer for viewers with low bandwidth or older viewing devices. Streaming lower-bitrate video allows more people to watch the content seamlessly, but introduces artifacts.\nTwitch — the interactive livestreaming platform — provides server-side transcoding for top-performing channels, meaning it will create different versions of the same stream for different bandwidth levels, improving the viewing experience. But the audience of many channels are left with a single stream option.\nTwitch, OBS and NVIDIA have collaborated on a new feature to address this — Twitch Enhanced Broadcasting, releasing in beta later this month. Using the high-quality dedicated encoder (NVENC) in modern GeForce RTX and GTX GPUs, streamers will be able to broadcast up to three resolutions simultaneously at up to 1080p.\nIn the coming months, Enhanced Broadcasting beta testers will be able to experiment with higher-input bit rates, up to 4K resolutions, up to 5 concurrent streams, as well as new codecs. The new codecs include the latest-generation AV1 for GeForce RTX 40 Series GPUs, which provides 40% more encoding efficiency than H.264, and HEVC for previous-generation GeForce GPUs.\nTwitch Enhanced Broadcasting releasing in beta later this month.\nTo simplify set up, Enhanced Broadcasting will automatically configure all OBS encoder settings, including resolution, bit rate and encoding parameters. A server-side algorithm will return the best possible configuration for OBS Studio based on the streamer’s setup, taking the headaches out of tuning settings for the best viewer experiences.\nUsing the dedicated NVENC hardware encoder, streamers can achieve the highest quality video across streaming bitrates, with minimal impact to app and game performance.\nTo further elevate livestreams, download the NVIDIA Broadcast app, free for RTX GPU owners and powered by dedicated AI Tensor Cores, to augment broadcast capabilities for microphones and cameras.",
    "favicon": ""
  },
  {
    "title": "NVIDIA, Global Data Center System Manufacturers to Supercharge Generative AI and Industrial Digitalization",
    "link": "https://nvidianews.nvidia.com/news/nvidia-global-data-center-system-manufacturers-to-supercharge-generative-ai-and-industrial-digitalization",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNTFhVE0zZGtndE9WQlRRVU5FVFJDb0FSaXNBaWdCTWdtTmNvN29QV1pjREFJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "OVX Servers Feature New NVIDIA GPUs to Accelerate Training and Inference, Graphics-Intensive Workloads; Coming Soon From Dell Technologies, Hewlett Packard Enterprise, Lenovo, Supermicro and More\nSIGGRAPH—NVIDIA today announced NVIDIA OVX™ servers featuring the new NVIDIA® L40S GPU, a powerful, universal data center processor designed to accelerate the most compute-intensive, complex applications, including AI training and inference, 3D design and visualization, video processing and industrial digitalization with the NVIDIA Omniverse™ platform.\nThe new GPU powers accelerated computing workloads for generative AI, which is transforming workflows and services across industries, including text, image and video generation, chatbots, game development, product design and healthcare.\n“As generative AI transforms every industry, enterprises are increasingly seeking large-scale compute resources in the data center,” said Bob Pette, vice president of professional visualization at NVIDIA. “OVX systems with NVIDIA L40S GPUs accelerate AI, graphics and video processing workloads, and meet the demanding performance requirements of an ever-increasing set of complex and diverse applications.”\nPowerful Performance for AI and Graphics\nNVIDIA OVX systems will enable up to eight NVIDIA L40S GPUs per server, each equipped with 48GB of memory. Based on the NVIDIA Ada Lovelace GPU architecture, the L40S includes fourth-generation Tensor Cores and an FP8 Transformer Engine, delivering over 1.45 petaflops of tensor processing power. For complex AI workloads with billions of parameters and multiple data modalities — such as text and video — L40S enables up to 1.2x more generative AI inference performance and up to 1.7x training performance compared with the NVIDIA A100 Tensor Core GPU.\nTo power high-fidelity professional visualization workflows like real-time rendering, product design and 3D content creation, the NVIDIA L40S GPU includes 142 third-generation RT Cores that deliver 212 teraflops of ray-tracing performance. This enables creative professionals to create immersive visual experiences and photorealistic content.\nFor computationally demanding workflows, such as engineering and scientific simulations, the NVIDIA L40S includes 18,176 CUDA® cores, delivering nearly 5x the single-precision floating-point (FP32) performance of the NVIDIA A100 GPU to accelerate complex calculations and data-intensive analyses.\nAmong the first cloud service providers to offer L40S instances is CoreWeave, which specializes in large-scale, GPU-accelerated workloads.\n“With the explosion of generative AI, our customers across industries are seeking powerful compute offerings and scale to match the complexity of any workload — from interactive video to AI design and automation,” said Brian Venturo, chief technology officer at CoreWeave. “NVIDIA L40S GPUs will further expand our broad portfolio of NVIDIA solutions, making CoreWeave the first specialized cloud provider to offer these new resources for fast, efficient and cost-effective accelerated computing to power the next wave of generative AI applications.”\nEnterprises deploying L40S GPUs can benefit from NVIDIA AI Enterprise software, which announced a major update today. The software provides production-ready enterprise support and security for over 100 frameworks, pretrained models, toolkits and software, including NVIDIA Modulus for simulations, NVIDIA RAPIDS™ for data science and NVIDIA Triton™ Inference Server for production AI.\nNVIDIA also announced major updates to the Omniverse platform, introducing capabilities and platform enhancements that enable developers to accelerate and advance OpenUSD pipelines and industrial digitalization applications with the power of generative AI. The next generation of NVIDIA OVX systems powering Omniverse Cloud will feature L40S GPUs to deliver the AI and graphics performance needed to supercharge generative AI pipelines and Omniverse workloads.\nThe NVIDIA L40S will be available starting this fall. Global system builders, including ASUS, Dell Technologies, GIGABYTE, HPE, Lenovo, QCT and Supermicro, will soon offer OVX systems that include the NVIDIA L40S GPUs. These servers will help professionals worldwide advance AI and bring generative AI applications like intelligent chatbots, search and summarization tools to users across industries.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Add It to the Toolkit: February Studio Driver and NVIDIA App Beta Now Available",
    "link": "https://blogs.nvidia.com/blog/studio-driver-app-rtx-ai-adobe-premiere-pro/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUNPVjlyYlZVd1MzSnhSVUkzVFJDakFSaTBBaWdCTWdhUlJJenNvUWc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-22T08:00:00.000Z",
    "time": "Feb 22",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA CEO: Every Country Needs Sovereign AI",
    "link": "https://blogs.nvidia.com/blog/world-governments-summit/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNVhPV0pVYjFSRmJUWktWbEY0VFJDM0FSaVRBaWdCTWdrTllwSVZMZWl4Q2dJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-12T08:00:00.000Z",
    "time": "Feb 12",
    "articleType": "regular",
    "content": "Every country needs to own the production of their own intelligence, NVIDIA founder and CEO Jensen Huang told attendees Monday at the World Governments Summit in Dubai.\nHuang, who spoke as part of a fireside chat with the UAE’s Minister of AI, His Excellency Omar Al Olama, described sovereign AI — which emphasizes a country’s ownership over its data and the intelligence it produces — as an enormous opportunity for the world’s leaders.\n“It codifies your culture, your society’s intelligence, your common sense, your history – you own your own data,” Huang told Al Olama during their conversation, a highlight of an event attended by more than 4,000 delegates from 150 countries.\n“We completely subscribe to that vision,” Al Olama said. “That’s why the UAE is moving aggressively on creating large language models and mobilizing compute.”\nHuang’s appearance in the UAE comes as the Gulf State is moving rapidly to transform itself from an energy powerhouse into a global information technology hub.\nDubai is the latest stop for Huang in a global tour that has included meetings with leaders in Canada, France, India, Japan, Malaysia, Singapore and Vietnam over the past six months.\nThe Middle East is poised to reap significant benefits from AI, with PwC projecting a $320 billion boost to the region’s economy by 2030.\nAt Monday’s summit, Huang urged leaders not to be “mystified” by AI. AI’s unprecedented ability to take directions from ordinary humans makes it critical for countries to embrace AI, infusing it with local languages and expertise.\nIn response to Al Olama’s question about how he might approach AI if he were the leader of a developing nation, Huang emphasized the importance of building infrastructure.\n“It’s not that costly, it is also not that hard,” Huang said. “The first thing that I would do, of course, is I would codify the language, the data of your culture into your own large language model.”\nAnd as AI and accelerated computing has developed, NVIDIA GPUs have become a platform for one innovation after another.\n“NVIDIA GPU is the only platform that’s available to everybody on any platform,” Huang said. “This ubiquity has not only democratized AI but facilitated a wave of innovation that spans from cloud computing to autonomous systems and beyond.\nAll of this promises to unleash new kinds of innovations that go beyond what’s traditionally been thought of as information technology.\nHuang even countered advice offered by many visionaries over the years who urged young people to study computer science in order to compete in the information age. No longer.\n“In fact, it’s almost exactly the opposite,” Huang said. “It is our job to create computing technologies that nobody has to program and that the programming language is human: everybody in the world is now a programmer — that is the miracle.”\nIn a move that further underscores the regional momentum behind AI, Moro Hub, a subsidiary of Digital DEWA, the digital arm of the Dubai Electricity and Water Authority, focused on providing cloud services, cybersecurity and smart city solutions, announced Monday it has agreed to build a green data center with NVIDIA.\nIn addition to the fireside chat, the summit featured panels on smart mobility, sustainable development and more, showcasing the latest in AI advancements. Later in the evening, Huang and Al Olama took the stage at the “Get Inspired” ecosystem event, organized by the UAE’s AI Office, featuring 280 attendees including developers, startups and others.\nExplore generative AI sessions and experiences at NVIDIA GTC, the global conference on AI and accelerated computing, running March 18-21 in San Jose, Calif., and online.",
    "favicon": ""
  },
  {
    "title": "NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf",
    "link": "https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXBaVFI0WWs5dFFtMU1VRWx6VFJDb0FSaXJBaWdCTWdZVkFvcXVqUXc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-27T07:00:00.000Z",
    "time": "Mar 27",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "GeForce NOW Delivers 24 A-May-zing Games This Month",
    "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-may-games-list/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNXVTWGRtZEdwMmVVdDJjVlJOVFJDb0FSaXNBaWdCTWdtQkVJaWtsaWN0eVFF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-02T07:00:00.000Z",
    "time": "May 2",
    "articleType": "regular",
    "content": "GeForce NOW brings 24 new games for members this month.\nNinja Theory’s highly anticipated Senua’s Saga: Hellblade II will be coming to the cloud soon — get ready by streaming the first in the series, Hellblade: Senua’s Sacrifice, part of the seven new games joining the GeForce NOW library this week.\nPlus, game across more devices than ever as GeForce NOW adds improved support on Steam Deck this GFN Thursday.\nNo sacrificing frame rates, streaming from the cloud.\nExperience exceptional storytelling in Ninja Theory’s award-winning game Hellblade: Senua’s Sacrifice, available to stream from the cloud this week.\nSet in a dark fantasy world inspired by Norse mythology and Celtic culture, the game follows the journey of Senua, a Pict warrior. Her quest is to reach Helheim, the realm of the dead,to rescue her deceased lover’s soul from the goddess Hela.\nSolve intricate puzzles with observation, engage in melee combat and get pulled deep into Senua’s mind as she grapples with inner demons. Journey through the hauntingly beautiful landscapes of Helheim with ray tracing and high-dynamic range using an Ultimate or Priority membership for the most immersive and stunning visual fidelity.\nThanks to GeForce NOW, nearly any device can perform like a GeForce-powered PC gaming rig. Members who want to stream their favorite PC games to Valve’s Steam Deck now have an easier way to get started.\nMembers can use a new beta installation method to automatically configure GeForce NOW’s browser in Steam Deck’s Gaming Mode. The installation script automatically installs Google Chrome to the device, then adds all the settings needed to help members log into GeForce NOW and stream their favorite games.\nThe latest GeForce NOW update, released last week, also allows members to navigate GeForce NOW on a browser using a gamepad, including on the Steam Deck. That means it’s even easier to find and play supported titles with NVIDIA DLSS technology or real-time ray tracing, regardless of the handheld’s system specs.\nPlus, members can easily play non-Steam games on the Steam Deck thanks to the cloud. This includes games from Battle.net, Epic Games Store, Ubisoft Connect, GOG.com and Xbox, as well as supported PC Game Pass titles. No more worrying about downloads or backend configurations. And with more than 1,900 games supported, there’s always something new to stream.\nMay New Games Be With You\nIf you build it, they will come.\nGet complete creative freedom in Paradox Interactive’s FOUNDRY, an exciting first-person factory-building sandbox game set in an infinite voxel world for expansive, ever-changing landscapes. Build a factory optimized to perfection or an artistic masterpiece, harvest resources, automate ever-growing production lines and manage complex systems to achieve mechanical mastery in FOUNDRY.\nCheck out the list of new games this week:\nHellblade: Senua’s Sacrifice (Steam and Xbox, available on PC Game Pass)\nGray Zone Warfare (New release on Steam, April 30)\nMotoGP24 (New release on Steam, May 2)\nFOUNDRY (New release on Steam, May 2)\nINDIKA (New release on Steam, May 2)\nOrcs Must Die! 3 (New release on Epic Games Store, May 2)\nAnd members can look for the following throughout the rest of the month:\nLittle Kitty, Big City (New release on Steam and Xbox, available on PC Game Pass, May 9)\nShips at Sea (New release on Steam, May 9)\nThe Rogue Prince of Persia  (New release on Steam, May 14)\nMen of War II (New release on Steam, May 15)\nDie by the Blade (New release on Steam, May 16)\nNorland (New release on Steam, May 16)\nGestalt: Steam & Cinder (New release on Steam, May 21)\nSynergy (New release on Steam, May 21)\nSunnySide (New release on Steam, May 21)\nCrown Wars: The Black Prince (New release on Steam, May 23)\nCapes (New release on Steam, May 29)\nHonkai: Star Rail (Epic Games Store)\nApril Showers Brought More Titles\nIn addition to the 19 games announced last month, 20 more joined the GeForce NOW library:\nGigantic: Rampage Edition (New release on Steam, April 9)\nInkbound 1.0 (New release, on Steam, April 9)\nBroken Roads (New release on Steam, April 10)\nInfection Free Zone (New release on Steam, April 11)\nShadow of the Tomb Raider: Definitive Edition (New release on Xbox and available on PC Game Pass, April 11)\nGhostrunner (Epic Games Store, free April 11-18)\nKill It With Fire 2 (New release on Steam, April 16)\nThe Crew Motorfest (New release on Steam, April 18)\nNo Rest for the Wicked (New release on Steam, April 18)\nBellwright (New release on Steam, April 23)\nAge of Water (New release on Steam, April 25)\nFallout 76 (Steam and Xbox, available on PC Game Pass)\nTerra Invicta (Xbox, available on PC Game Pass)\nTomb Raider I-III Remastered (Steam)\nRiot Games has rolled out the League of Legends 14.9 update globally, which adds the Vanguard security software to the game. Since Vanguard doesn’t support virtual machines like GeForce NOW, the game is under maintenance and no longer playable on the cloud gaming platform for the foreseeable future. Work is underway to find a solution for GeForce NOW members.\nLightyear Frontier (Xbox) didn’t make it in April due to technical issues. Stay tuned to GFN Thursday for updates.\nWhat are you planning to play this weekend? Let us know on X or in the comments below.\nwhat are you gaming goals for next month? 🤔\n— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) May 1, 2024",
    "favicon": ""
  },
  {
    "title": "Google Cloud and NVIDIA Expand Partnership to Advance AI Computing, Software and Services",
    "link": "https://nvidianews.nvidia.com/news/google-cloud-and-nvidia-expand-partnership-to-advance-ai-computing-software-and-services",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWxiRlZoTkZCSVpGOVNkSFJqVFJDeUFSaWJBaWdCTWdZSlVaTG9wQWc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-29T07:00:00.000Z",
    "time": "Aug 29, 2023",
    "articleType": "regular",
    "content": "NVIDIA Generative AI Technology Used by Google DeepMind and Google Research Teams Now Optimized and Available to Google Cloud Customers Worldwide\nGoogle Cloud Next — Google Cloud and NVIDIA today announced new AI infrastructure and software for customers to build and deploy massive models for generative AI and speed data science workloads.\nIn a fireside chat at Google Cloud Next, Google Cloud CEO Thomas Kurian and NVIDIA founder and CEO Jensen Huang discussed how the partnership is bringing end-to-end machine learning services to some of the largest AI customers in the world — including by making it easy to run AI supercomputers with Google Cloud offerings built on NVIDIA technologies. The new hardware and software integrations utilize the same NVIDIA technologies employed over the past two years by Google DeepMind and Google research teams.\n“We’re at an inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace,” Huang said. “Our expanded collaboration with Google Cloud will help developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs.”\n“Google Cloud has a long history of innovating in AI to foster and speed innovation for our customers,” Kurian said. “Many of Google’s products are built and served on NVIDIA GPUs, and many of our customers are seeking out NVIDIA accelerated computing to power efficient development of LLMs to advance generative AI.”\nNVIDIA Integrations to Speed AI and Data Science Development\nGoogle’s framework for building massive large language models (LLMs), PaxML, is now optimized for NVIDIA accelerated computing.\nOriginally built to span multiple Google TPU accelerator slices, PaxML now enables developers to use NVIDIA® H100 and A100 Tensor Core GPUs for advanced and fully configurable experimentation and scale. A GPU-optimized PaxML container is available immediately in the NVIDIA NGC™ software catalog. In addition, PaxML runs on JAX, which has been optimized for GPUs leveraging the OpenXLA compiler.\nGoogle DeepMind and other Google researchers are among the first to use PaxML with NVIDIA GPUs for exploratory research.\nThe NVIDIA-optimized container for PaxML will be available immediately on the NVIDIA NGC container registry to researchers, startups and enterprises worldwide that are building the next generation of AI-powered applications.\nAdditionally, the companies announced Google’s integration of serverless Spark with NVIDIA GPUs through Google’s Dataproc service. This will help data scientists speed Apache Spark workloads to prepare data for AI development.\nThese new integrations are the latest in NVIDIA and Google’s extensive history of collaboration. They cross hardware and software announcements, including:\nGoogle Cloud on A3 virtual machines powered by NVIDIA H100 — Google Cloud announced today its purpose-built Google Cloud A3 VMs powered by NVIDIA H100 GPUs will be generally available next month, making NVIDIA’s AI platform more accessible for a broad set of workloads. Compared to the previous generation, A3 VMs offer 3x faster training and significantly improved networking bandwidth.\nNVIDIA H100 GPUs to power Google Cloud’s Vertex AI platform — H100 GPUs are expected to be generally available on VertexAI in the coming weeks, enabling customers to quickly develop generative AI LLMs.\nGoogle Cloud to gain access to NVIDIA DGX™ GH200 — Google Cloud will be one of the first companies in the world to have access to the NVIDIA DGX GH200 AI supercomputer — powered by the NVIDIA Grace Hopper™ Superchip — to explore its capabilities for generative AI workloads.\nNVIDIA DGX Cloud Coming to Google Cloud — NVIDIA DGX Cloud AI supercomputing and software will be available to customers directly from their web browser to provide speed and scale for advanced training workloads.\nNVIDIA AI Enterprise on Google Cloud Marketplace — Users can access NVIDIA AI Enterprise, a secure, cloud native software platform that simplifies developing and deploying enterprise-ready applications including generative AI, speech AI, computer vision, and more.\nGoogle Cloud first to offer NVIDIA L4 GPUs — Earlier this year, Google Cloud became the first cloud provider to offer NVIDIA L4 Tensor Core GPUs with the launch of the G2 VM. NVIDIA customers switching to L4 GPUs from CPUs for AI video workloads can realize up to 120x higher performance with 99% better efficiency. L4 GPUs are used widely for image and text generation, as well as VDI and AI-accelerated audio/video transcoding.\nGoogle Cloud accelerates every organization's ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google's cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.\nSince its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/.\nCertain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, features and availability of our products and technologies, including NVIDIA GPUs, NVIDIA accelerated computing, NVIDIA H100 and A100 Tensor Core GPUs, the NVIDIA DGX GH200 AI supercomputer, NVIDIA DGX Cloud, NVIDIA AI Enterprise and NVIDIA L4 Tensor Core GPUs; NVIDIA’s partnership with Google Cloud, including the benefits, impact, features and availability of Google Cloud offerings built on NVIDIA technologies; the inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace; and NVIDIA’s expanded collaboration with Google Cloud helping developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\nMany of the products and features described herein remain in various stages and will be offered on a when-and-if-available basis. The statements within are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described for our products is subject to change and remains at the sole discretion of NVIDIA. NVIDIA will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX, NGC and NVIDIA Grace Hopper are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.\nGoogle Cloud accelerates every organization's ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google's cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.\nSince its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/.\nCertain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, features and availability of our products and technologies, including NVIDIA GPUs, NVIDIA accelerated computing, NVIDIA H100 and A100 Tensor Core GPUs, the NVIDIA DGX GH200 AI supercomputer, NVIDIA DGX Cloud, NVIDIA AI Enterprise and NVIDIA L4 Tensor Core GPUs; NVIDIA’s partnership with Google Cloud, including the benefits, impact, features and availability of Google Cloud offerings built on NVIDIA technologies; the inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace; and NVIDIA’s expanded collaboration with Google Cloud helping developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\nMany of the products and features described herein remain in various stages and will be offered on a when-and-if-available basis. The statements within are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described for our products is subject to change and remains at the sole discretion of NVIDIA. NVIDIA will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX, NGC and NVIDIA Grace Hopper are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Launches Blackwell-Powered DGX SuperPOD for Generative AI Supercomputing at Trillion-Parameter Scale",
    "link": "https://nvidianews.nvidia.com/news/nvidia-blackwell-dgx-generative-ai-supercomputing",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNU5VRVpLVXpKNGVVUXliMkp2VFJDakFSaTBBaWdCTWdNSmNSQQ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Scales to Tens of Thousands of Grace Blackwell Superchips Using Most Advanced NVIDIA Networking, NVIDIA Full-Stack AI Software, and Storage Features up to 576 Blackwell GPUs Connected as One With NVIDIA NVLink NVIDIA System Experts Speed Deployment for Immediate AI Infrastructure\n​GTC—NVIDIA today announced its next-generation AI supercomputer — the NVIDIA DGX SuperPOD™ powered by NVIDIA GB200 Grace Blackwell Superchips — for processing trillion-parameter models with constant uptime for superscale generative AI training and inference workloads.\nFeaturing a new, highly efficient, liquid-cooled rack-scale architecture, the new DGX SuperPOD is built with NVIDIA DGX™ GB200 systems and provides 11.5 exaflops of AI supercomputing at FP4 precision and 240 terabytes of fast memory — scaling to more with additional racks.\nEach DGX GB200 system features 36 NVIDIA GB200 Superchips — which include 36 NVIDIA Grace CPUs and 72 NVIDIA Blackwell GPUs — connected as one supercomputer via fifth-generation NVIDIA NVLink®. GB200 Superchips deliver up to a 30x performance increase compared to the NVIDIA H100 Tensor Core GPU for large language model inference workloads.\n“NVIDIA DGX AI supercomputers are the factories of the AI industrial revolution,” said Jensen Huang, founder and CEO of NVIDIA. “The new DGX SuperPOD combines the latest advancements in NVIDIA accelerated computing, networking and software to enable every company, industry and country to refine and generate their own AI.”\nThe Grace Blackwell-powered DGX SuperPOD features eight or more DGX GB200 systems and can scale to tens of thousands of GB200 Superchips connected via NVIDIA Quantum InfiniBand. For a massive shared memory space to power next-generation AI models, customers can deploy a configuration that connects the 576 Blackwell GPUs in eight DGX GB200 systems connected via NVLink.\nNew Rack-Scale DGX SuperPOD Architecture for Era of Generative AI\nThe new DGX SuperPOD with DGX GB200 systems features a unified compute fabric. In addition to fifth-generation NVIDIA NVLink, the fabric includes NVIDIA BlueField®-3 DPUs and will support NVIDIA Quantum-X800 InfiniBand networking, announced separately today. This architecture provides up to 1,800 gigabytes per second of bandwidth to each GPU in the platform.\nAdditionally, fourth-generation NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)™ technology provides 14.4 teraflops of In-Network Computing, a 4x increase in the next-generation DGX SuperPOD architecture compared to the prior generation.\nTurnkey Architecture Pairs With Advanced Software for Unprecedented Uptime\nThe new DGX SuperPOD is a complete, data-center-scale AI supercomputer that integrates with high-performance storage from NVIDIA-certified partners to meet the demands of generative AI workloads. Each is built, cabled and tested in the factory to dramatically speed deployment at customer data centers.\nThe Grace Blackwell-powered DGX SuperPOD features intelligent predictive-management capabilities to continuously monitor thousands of data points across hardware and software to predict and intercept sources of downtime and inefficiency — saving time, energy and computing costs.\nThe software can identify areas of concern and plan for maintenance, flexibly adjust compute resources, and automatically save and resume jobs to prevent downtime, even without system administrators present.\nIf the software detects that a replacement component is needed, the cluster will activate standby capacity to ensure work finishes in time. Any required hardware replacements can be scheduled to avoid unplanned downtime.\nNVIDIA DGX B200 Systems Advance AI Supercomputing for Industries\nNVIDIA also unveiled the NVIDIA DGX B200 system, a unified AI supercomputing platform for AI model training, fine-tuning and inference.\nDGX B200 is the sixth generation of air-cooled, traditional rack-mounted DGX designs used by industries worldwide. The new Blackwell architecture DGX B200 system includes eight NVIDIA Blackwell GPUs and two 5th Gen Intel® Xeon® processors. Customers can also build DGX SuperPOD using DGX B200 systems to create AI Centers of Excellence that can power the work of large teams of developers running many different jobs.\nDGX B200 systems include the FP4 precision feature in the new Blackwell architecture, providing up to 144 petaflops of AI performance, a massive 1.4TB of GPU memory and 64TB/s of memory bandwidth. This delivers 15x faster real-time inference for trillion-parameter models over the previous generation.\nDGX B200 systems include advanced networking with eight NVIDIA ConnectX™-7 NICs and two BlueField-3 DPUs. These provide up to 400 gigabits per second bandwidth per connection — delivering fast AI performance with NVIDIA Quantum-2 InfiniBand and NVIDIA Spectrum™-X Ethernet networking platforms.\nSoftware and Expert Support to Scale Production AI\nAll NVIDIA DGX platforms include NVIDIA AI Enterprise software for enterprise-grade development and deployment. DGX customers can accelerate their work with the pretrained NVIDIA foundation models, frameworks, toolkits and new NVIDIA NIM microservices included in the software platform.\nNVIDIA DGX experts and select NVIDIA partners certified to support DGX platforms assist customers throughout every step of deployment, so they can quickly move AI into production. Once systems are operational, DGX experts continue to support customers in optimizing their AI pipelines and infrastructure.\nNVIDIA DGX SuperPOD with DGX GB200 and DGX B200 systems are expected to be available later this year from NVIDIA’s global partners.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Mistral AI and NVIDIA Unveil Mistral NeMo 12B, a Cutting-Edge Enterprise AI Model",
    "link": "https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNU5NVkZGVUdjd1p6YzRURlZwVFJDakFSaTBBaWdCTWdhQkVZeTNUQVE=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-18T07:00:00.000Z",
    "time": "Jul 18",
    "articleType": "regular",
    "content": "Mistral AI and NVIDIA today released a new state-of-the-art language model, Mistral NeMo 12B, that developers can easily customize and deploy for enterprise applications supporting chatbots, multilingual tasks, coding and summarization.\nBy combining Mistral AI’s expertise in training data with NVIDIA’s optimized hardware and software ecosystem, the Mistral NeMo model offers high performance for diverse applications.\n“We are fortunate to collaborate with the NVIDIA team, leveraging their top-tier hardware and software,” said Guillaume Lample, cofounder and chief scientist of Mistral AI. “Together, we have developed a model with unprecedented accuracy, flexibility, high-efficiency and enterprise-grade support and security thanks to NVIDIA AI Enterprise deployment.”\nMistral NeMo was trained on the NVIDIA DGX Cloud AI platform, which offers dedicated, scalable access to the latest NVIDIA architecture.\nNVIDIA TensorRT-LLM for accelerated inference performance on large language models and the NVIDIA NeMo development platform for building custom generative AI models were also used to advance and optimize the process.\nThis collaboration underscores NVIDIA’s commitment to supporting the model-builder ecosystem.\nDelivering Unprecedented Accuracy, Flexibility and Efficiency\nExcelling in multi-turn conversations, math, common sense reasoning, world knowledge and coding, this enterprise-grade AI model delivers precise, reliable performance across diverse tasks.\nWith a 128K context length, Mistral NeMo processes extensive and complex information more coherently and accurately, ensuring contextually relevant outputs.\nReleased under the Apache 2.0 license, which fosters innovation and supports the broader AI community, Mistral NeMo is a 12-billion-parameter model. Additionally, the model uses the FP8 data format for model inference, which reduces memory size and speeds deployment without any degradation to accuracy.\nThat means the model learns tasks better and handles diverse scenarios more effectively, making it ideal for enterprise use cases.\nMistral NeMo comes packaged as an NVIDIA NIM inference microservice, offering performance-optimized inference with NVIDIA TensorRT-LLM engines.\nThis containerized format allows for easy deployment anywhere, providing enhanced flexibility for various applications.\nAs a result, models can be deployed anywhere in minutes, rather than several days.\nNIM features enterprise-grade software that’s part of NVIDIA AI Enterprise, with dedicated feature branches, rigorous validation processes, and enterprise-grade security and support.\nIt includes comprehensive support, direct access to an NVIDIA AI expert and defined service-level agreements, delivering reliable and consistent performance.\nThe open model license allows enterprises to integrate Mistral NeMo into commercial applications seamlessly.\nDesigned to fit on the memory of a single NVIDIA L40S, NVIDIA GeForce RTX 4090 or NVIDIA RTX 4500 GPU, the Mistral NeMo NIM offers high efficiency, low compute cost, and enhanced security and privacy.\nAdvanced Model Development and Customization\nThe combined expertise of Mistral AI and NVIDIA engineers has optimized training and inference for Mistral NeMo.\nTrained with Mistral AI’s expertise, especially on multilinguality, code and multi-turn content, the model benefits from accelerated training on NVIDIA’s full stack.\nIt’s designed for optimal performance, utilizing efficient model parallelism techniques, scalability and mixed precision with Megatron-LM.\nThe model was trained using Megatron-LM, part of NVIDIA NeMo, with 3,072 H100 80GB Tensor Core GPUs on DGX Cloud, composed of NVIDIA AI architecture, including accelerated computing, network fabric and software to increase training efficiency.\nWith the flexibility to run anywhere — cloud, data center or RTX workstation — Mistral NeMo is ready to revolutionize AI applications across various platforms.\nExperience Mistral NeMo as an NVIDIA NIM today via ai.nvidia.com, with a downloadable NIM coming soon.\nSee notice regarding software product information.",
    "favicon": ""
  },
  {
    "title": "Exclusive: Nvidia pursues $30 billion custom chip opportunity with new unit",
    "link": "https://www.reuters.com/technology/nvidia-chases-30-billion-custom-chip-market-with-new-unit-sources-2024-02-09/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUxVME5NV0ZoTFpsazJaWGxTVFJDeEFSaWNBaWdCTWdZQlVJcWtJZ28=-w400-h224-p-df-rw",
    "source": "Reuters",
    "datetime": "2024-02-09T08:00:00.000Z",
    "time": "Feb 9",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "‘We Created a Processor for the Generative AI Era,’ NVIDIA CEO Says",
    "link": "https://blogs.nvidia.com/blog/2024-gtc-keynote/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTRiRlJRYUc1TkxURk1jWFpXVFJDakFSaTBBaWdCTWdheEFxZ1FEUXc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Striking Performance: Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows",
    "link": "https://blogs.nvidia.com/blog/tensorrt-llm-windows-stable-diffusion-rtx/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNW9SV0Y1VDBGVWJtZzNTekJMVFJDakFSaTBBaWdCTWdrUlFwRElwZVk4YUFJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-10-17T07:00:00.000Z",
    "time": "Oct 17, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Wave of EV Makers Choose NVIDIA DRIVE for Automated Driving",
    "link": "https://nvidianews.nvidia.com/news/wave-of-ev-makers-choose-nvidia-drive-for-automated-driving",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVRUV3RsVEZaTFRXOVNTMnRsVFJEQ0FSaURBaWdCTWdhZFpKTE1yUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "Li Auto Selects DRIVE Thor for Next-Gen EVs; GWM, ZEEKR and Xiaomi Develop AI-Driven Cars Powered by NVIDIA DRIVE Orin\nCES — NVIDIA today announced that Li Auto, a pioneer in extended-range electric vehicles (EVs), has selected the NVIDIA DRIVE Thor™ centralized car computer to power its next-generation fleets.\nNVIDIA also announced that EV makers GWM (Great Wall Motor), ZEEKR and Xiaomi have adopted the NVIDIA DRIVE Orin​​™ platform to power their intelligent automated-driving systems.\n“The transportation industry is embracing centralized compute for highly automated and autonomous driving,” said Xinzhou Wu, vice president of automotive at NVIDIA. “The AI car computer of choice for today’s intelligent fleets is NVIDIA DRIVE Orin, with automakers increasingly looking to the advanced capabilities and AI performance of its successor, NVIDIA DRIVE Thor, for their future vehicle roadmaps.”\nDRIVE Thor is a next-generation centralized car computer that integrates a wide range of intelligent functions into a single AI compute platform, delivering autonomous driving and parking capabilities, driver and passenger monitoring, and AI cockpit functionality.\nLi Auto currently uses two DRIVE Orin processors to power its assisted-driving system, AD Max, for its L-series models. The processors, which provide a combined 508 trillion operations per second (TOPS), enable real-time fusing and processing of sensor information — powering full-scenario autonomous driving for navigation on advanced driver-assistance systems (ADAS), full-scenario assisted driving for lane change control (LCC), automated parking and automatic emergency braking (AEB) active safety features.\nThe new AD Max 3.0 upgrade transitions the system to an end-to-end algorithmic architecture dominated by large AI models. It delivers a safer, more comfortable intelligent driving experience using an occupancy network and spatio-temporal trajectory planning and model-predictive control algorithms.\nBuilding the Future of Transportation on DRIVE\nGWM, among China’s leading new energy vehicle makers, has announced that it will build its self-developed, high-end intelligent-driving system, the Coffee Pilot, based on the DRIVE Orin centralized computing platform. Coffee Pilot can support parking, high-speed and urban scenes to achieve full-scenario smart navigation and assisted-driving functions without high-precision maps.\nGWM, which collaborated with NVIDIA to develop this intelligent-driving system, will debut its first model with the system in the first half of the year. Advanced intelligent-driving features, such as Urban Navigate on Autopilot and cross-floor Memory Parking, will be first rolled out in GWM’s WEY models.\n“LLM-driven AI technology will profoundly enhance future mobility as well as the entire automotive industry,” said a GWM spokesperson. “GWM is committed to working with NVIDIA and other industry-leading players to offer greener, smarter mobility for all.”\nZEEKR, the premium EV subsidiary of Geely, has launched the ZEEKR Luxury Sedan, its fourth model to be powered by NVIDIA DRIVE Orin. It features a new full-stack smart driving system, powered by two DRIVE Orin systems-on-a-chip, to deliver intelligent parking and automated operation on high-speed and urban roads. ZEEKR’s full range of models is offered with two sensor options: Lidar + Vision Fusion and Pure Vision. The Navigation ZEEKR Pilot, a highway navigation feature from ZEEKR’s in-house advanced driving assistant system, will be available for use in major Chinese cities once deliveries begin.\n“ZEEKR is proud to work with NVIDIA on innovative, safe autonomous driving capabilities,” said Chen Qi, vice president of ZEEKR. “The new ZEEKR Luxury Sedan’s in-house-developed ADAS system is made possible with the energy-efficient, high-performance NVIDIA DRIVE car computing platform.”\nXiaomi EV, the automotive arm of the technology giant, has announced its first EV, the SU7 sedan, which is built on a dual DRIVE Orin configuration for highway driving functions. Built using Xiaomi’s leading large language model for perception and decision-making, the sedan will be able to seamlessly navigate through Chinese cities, regardless of locale, administrative divisions within the country or type of road. It will come in two versions: one with a driving range of up to 415 miles on a single charge and another with a range of up to 497 miles. The SU7 will be officially launched in the first half of 2024.\nIn production with leading automakers, trucks, robotaxis and shuttles since 2022, DRIVE Orin offers up to 254 TOPS and is scalable to support level 2+ to level 5 self-driving capabilities.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "How Nvidia became an AI giant",
    "link": "https://apnews.com/article/nvidia-artificial-intelligence-ai-gaming-1acc94ebbe6a59f728742ca20b3532cf",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW9aWGxuWkhScE5HcENiSGRQVFJDNkFSaVBBaWdCTWdhdFJKU1BIUWs=-w400-h224-p-df-rw",
    "source": "The Associated Press",
    "datetime": "2024-06-20T07:00:00.000Z",
    "time": "Jun 20",
    "articleType": "regular",
    "content": "Updated 1:53 AM GMT+2, June 21, 2024\nLOS ANGELES (AP) — It all started at a Denny’s in San Jose in 1993.Three engineers — Jensen Huang, Chris Malachowsky and Curtis Priem — gathered at the diner in what is now the heart of Silicon Valley to discuss building a computer chip that would make graphics for video games faster and more realistic. That conversation, and the ones that followed, led to the founding of Nvidia, the tech company that soared through the ranks of the stock market to briefly top Microsoft as the most valuable company in the S&P 500 this week. The company is now worth over $3.2 trillion, with its dominance as a chipmaker cementing Nvidia’s place as the poster child of the artificial intelligence boom — a moment that Huang, Nvidia’s CEO, has dubbed “the next industrial revolution.”On a conference call with analysts last month, Huang predicted that the companies using Nvidia chips would build a new type of data center called “AI factories.”\nHuang added that training AI models is becoming a faster process as they learn to become “multimodal” — able to understand text, speech, images, video and 3-D data — and also “to reason and plan.”\n“People kind of talk about AI as if Jensen just kind of arrived like in the last 18 months, like 24 months ago all of a sudden figured this out,” said Daniel Newman, CEO of The Futurum Group, a tech research firm. “But if you actually go back in time and listen to Jensen talking about accelerated computing, he’s been sharing his vision for more than a decade.”\nThe Santa Clara, California-based tech company’s invention of the graphics processor unit, or GPU, in 1999 helped spark the growth of the PC gaming market and redefined computer graphics. Now Nvidia’s specialized chips are key components that help power different forms of artificial intelligence, including the latest generative AI chatbots such as ChatGPT and Google’s Gemini.\nNvidia’s GPUs are a key factor in the company’s success in artificial intelligence, Newman added.“They took an architecture that was used for a single thing, to maybe enhance gaming, and they figured out how to network these things,” he said. “The GPU became the most compelling architecture for AI, going from gaming, rendering graphics and stuff, to actually using it for data. ... They basically ended up creating a market that didn’t exist, which was GPUs for AI, or GPUs for machine learning.”AI chips are designed to perform artificial intelligence tasks faster and more efficiently. While general-purpose chips like CPUs can also be used for simpler AI tasks, they’re “becoming less and less useful as AI advances,” a 2020 report from Georgetown University’s Center for Security and Emerging Technology found.Tech giants are snapping up Nvidia chips as they wade deeper into AI — a movement that’s enabling cars to drive by themselves, and generating stories, art and music.“Jensen basically has made AI digestible and then Apple will make it consumable,” Newman said.The company carved out an early lead in the hardware and software needed to tailor its technology to AI applications, partly because Huang nudged it into what was still a nascent technology more than a decade ago.\n“Nvidia has been working on different portions of this problem for more than two decades now. They have a deep innovation engine that goes all the way back to the early 2000s,” said Chirag Dekate, a VP analyst at Gartner, a tech research and consulting firm. “What Nvidia did two decades ago is they both identified and they nurtured an adjacent market where they discovered that the same processors, same GPUs that they were using for graphics could be shaped to solve highly parallel tasks.”At the time, he said, AI was only in its infancy. But Nvidia’s understanding that GPUs would be central to the development of AI was “the fundamental breakthrough that was needed,” Dekate said. “Until then, we would have been, I would say, in the analytic Dark Ages,” he said. “The analytics were there, but we could never bring these AI elements to life.”\nAnalysts estimate that Nvidia’s revenue for the fiscal year that ends in January 2025 will reach $119.9 billion — about double its revenue for fiscal 2024 and more than four times its receipts the year before that.“My hypothesis is the kind of exponential growth that we’re seeing with Nvidia today is potentially a pattern that we’re going to see replicated more frequently in the decades to come,” he said. “This is the Golden Age if you will...this is the best time to be an AI engineer.”",
    "favicon": "/favicon-32x32.png"
  },
  {
    "title": "Three’s a Cloud: New Activision and Blizzard Games, Day Passes, G-SYNC Technology Coming to GeForce NOW",
    "link": "https://blogs.nvidia.com/blog/ces-2024-geforce-now-activision-blizzard-day-passes-g-sync/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTZWMjlTUmpFdFpVaG1UMlpGVFJDZkFSaS1BaWdCTWdZQm9wUkl2Z2M=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Accelerates Humanoid Robotics Development",
    "link": "https://nvidianews.nvidia.com/news/nvidia-accelerates-worldwide-humanoid-robotics-development",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNHpkV05rU0VWQmRqa3pOVEZ4VFJDb0FSaXNBaWdCTWdrWkVvQVVFU3VMaUFJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-29T20:35:22.000Z",
    "time": "3 days ago",
    "articleType": "regular",
    "content": "Developers Gain Access to New NVIDIA NIM Microservices for Robotics Simulation in Isaac Lab and Isaac Sim, OSMO Robot Cloud Compute Orchestration Service, Teleoperated Data Capture Workflow and More\nSIGGRAPH—To accelerate humanoid development on a global scale, NVIDIA today announced it is providing the world’s leading robot manufacturers, AI model developers and software makers with a suite of services, models and computing platforms to develop, train and build the next generation of humanoid robotics.\nAmong the offerings are new NVIDIA NIM™ microservices and frameworks for robot simulation and learning, the NVIDIA OSMO orchestration service for running multi-stage robotics workloads, and an AI- and simulation-enabled teleoperation workflow that allows developers to train robots using small amounts of human demonstration data.\n“The next wave of AI is robotics and one of the most exciting developments is humanoid robots,” said Jensen Huang, founder and CEO of NVIDIA. “We’re advancing the entire NVIDIA robotics stack, opening access for worldwide humanoid developers and companies to use the platforms, acceleration libraries and AI models best suited for their needs.”\nAccelerating Development With NVIDIA NIM and OSMO\nNIM microservices provide pre-built containers, powered by NVIDIA inference software, that enable developers to reduce deployment times from weeks to minutes. Two new AI microservices will allow roboticists to enhance simulation workflows for generative physical AI in NVIDIA Isaac Sim™, a reference application for robotics simulation built on the NVIDIA Omniverse™ platform.\nThe MimicGen NIM microservice generates synthetic motion data based on recorded teleoperated data from spatial computing devices like Apple Vision Pro. The Robocasa NIM microservice generates robot tasks and simulation-ready environments in OpenUSD, a universal framework for developing and collaborating within 3D worlds.\nNVIDIA OSMO, available now, is a cloud-native managed service that allows users to orchestrate and scale complex ‌robotics development workflows across distributed computing resources, whether on premises or in the cloud.\nOSMO vastly simplifies robot training and simulation workflows, cutting deployment and development cycle times from months to under a week. Users can visualize and manage a range of tasks — like generating synthetic data, training models, conducting reinforcement learning and implementing software-in-the-loop testing at scale for humanoids, autonomous mobile robots and industrial manipulators.\nAdvancing Data Capture Workflows for Humanoid Robot Developers\nTraining foundation models for humanoid robots requires an incredible amount of data. One way of capturing human demonstration data is using teleoperation, but this is becoming an increasingly expensive and lengthy process.\nAn NVIDIA AI- and Omniverse-enabled teleoperation reference workflow, demonstrated at the SIGGRAPH computer graphics conference, allows researchers and AI developers to generate massive amounts of synthetic motion and perception data from a minimal amount of remotely captured human demonstrations.\nFirst, developers use Apple Vision Pro to capture a small number of teleoperated demonstrations. Then, they simulate the recordings in NVIDIA Isaac Sim and use the MimicGen NIM microservice to generate synthetic datasets from the recordings.\nThe developers train the Project GR00T humanoid foundation model with real and synthetic data, enabling developers to save time and reduce costs. They then use the Robocasa NIM microservice in Isaac Lab, a framework for robot learning, to generate experiences to retrain the robot model. Throughout the workflow, NVIDIA OSMO seamlessly assigns computing jobs to different resources, saving the developers weeks of administrative tasks.\nFourier, a general-purpose robot platform company, sees the benefit of using simulation technology to synthetically generate training data.\n“Developing humanoid robots is extremely complex — requiring an incredible amount of real data, tediously captured from the real world,” said Alex Gu, CEO of Fourier. “NVIDIA’s new simulation and generative AI developer tools will help bootstrap and accelerate our model development workflows.”\nExpanding Access to NVIDIA Humanoid Developer Technologies\nNVIDIA provides three computing platforms to ease humanoid robotics development: NVIDIA AI supercomputers to train the models; NVIDIA Isaac Sim built on Omniverse, where robots can learn and refine their skills in simulated worlds; and NVIDIA Jetson™ Thor humanoid robot computers to run the models. Developers can access and use all — or any part of — the platforms for their specific needs.\n1x, Boston Dynamics, ByteDance Research, Field AI, Figure, Fourier, Galbot, LimX Dynamics, Mentee, Neura Robotics, RobotEra and Skild AI are among the first to join the early-access program.\n“Boston Dynamics and NVIDIA have a long history of close collaboration to push the boundaries of what’s possible in robotics,” said Aaron Saunders, chief technology officer of Boston Dynamics. “We’re really excited to see the fruits of this work accelerating the industry at large, and the early-access program is a fantastic way to access best-in-class technology.”\nDevelopers can join the NVIDIA Humanoid Robot Developer Program now to get access to NVIDIA OSMO and Isaac Lab, and will soon gain access to NVIDIA NIM microservices.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "VMware and NVIDIA Unlock Generative AI for Enterprises",
    "link": "https://nvidianews.nvidia.com/news/vmware-and-nvidia-unlock-generative-ai-for-enterprises",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTJSV3M0Tm1sR1EzcFdXblIwVFJDb0FSaXNBaWdCTWdZTm9wWm5PZ2M=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-22T07:00:00.000Z",
    "time": "Aug 22, 2023",
    "articleType": "regular",
    "content": "New VMware Private AI Foundation With NVIDIA Enables Enterprises to Ready Their Businesses for Generative AI; Platform to Further Support Data Privacy, Security and Control\nVMware Explore—VMware Inc. (NYSE: VMW) and NVIDIA (NASDAQ: NVDA) today announced the expansion of their strategic partnership to ready the hundreds of thousands of enterprises that run on VMware’s cloud infrastructure for the era of generative AI.\nVMware Private AI Foundation with NVIDIA will enable enterprises to customize models and run generative AI applications, including intelligent chatbots, assistants, search and summarization. The platform will be a fully integrated solution featuring generative AI software and accelerated computing from NVIDIA, built on VMware Cloud Foundation and optimized for AI.\n“Generative AI and multi-cloud are the perfect match,” said Raghu Raghuram, CEO, VMware. “Customer data is everywhere — in their data centers, at the edge, and in their clouds. Together with NVIDIA, we’ll empower enterprises to run their generative AI workloads adjacent to their data with confidence while addressing their corporate data privacy, security and control concerns.”\n“Enterprises everywhere are racing to integrate generative AI into their businesses,” said Jensen Huang, founder and CEO, NVIDIA. “Our expanded collaboration with VMware will offer hundreds of thousands of customers — across financial services, healthcare, manufacturing and more — the full-stack software and computing they need to unlock the potential of generative AI using custom applications built with their own data.”\nFull-Stack Computing to Supercharge Generative AI\nTo achieve business benefits faster, enterprises are seeking to streamline development, testing and deployment of generative AI applications. McKinsey estimates that generative AI could add up to $4.4 trillion annually to the global economy.(1)\nVMware Private AI Foundation with NVIDIA will enable enterprises to harness this capability, customizing large language models; producing more secure and private models for their internal usage; and offering generative AI as a service to their users; and, more securely running inference workloads at scale.\nThe platform is expected to include integrated AI tools to empower enterprises to run proven models trained on their private data in a cost-efficient manner. To be built on VMware Cloud Foundation and NVIDIA AI Enterprise software, the platform’s expected benefits will include:\nPrivacy — Will enable customers to easily run AI services adjacent to wherever they have data with an architecture that preserves data privacy and enable secure access.\nChoice — Enterprises will have a wide choice in where to build and run their models — from NVIDIA NeMo™ to Llama 2 and beyond — including leading OEM hardware configurations and, in the future, on public cloud and service provider offerings.\nPerformance — Running on NVIDIA accelerated infrastructure will deliver performance equal to and even exceeding bare metal in some use cases, as proven in recent industry benchmarks.\nData-Center Scale — GPU scaling optimizations in virtualized environments will enable AI workloads to scale across up to 16 vGPUs/GPUs in a single virtual machine and across multiple nodes to speed generative AI model fine-tuning and deployment.\nLower Cost — Will maximize usage of all compute resources across, GPUs, DPUs and CPUs to lower overall costs, and create a pooled resource environment that can be shared efficiently across teams.\nAccelerated Storage — VMware vSAN Express Storage Architecture will provide performance-optimized NVMe storage and supports GPUDirect® storage over RDMA, allowing for direct I/O transfer from storage to GPUs without CPU involvement.\nAccelerated Networking — Deep integration between vSphere and NVIDIA NVSwitch™ technology will further enable multi-GPU models to execute without inter-GPU bottlenecks.\nRapid Deployment and Time to Value — vSphere Deep Learning VM images and image repository will enable fast prototyping capabilities by offering a stable turnkey solution image that includes frameworks and performance-optimized libraries pre-installed.\nThe platform will feature NVIDIA NeMo, an end-to-end, cloud-native framework included in NVIDIA AI Enterprise — the operating system of the NVIDIA AI platform — that allows enterprises to build, customize and deploy generative AI models virtually anywhere. NeMo combines customization frameworks, guardrail toolkits, data curation tools and pretrained models to offer enterprises an easy, cost-effective and fast way to adopt generative AI.\nFor deploying generative AI in production, NeMo uses TensorRT for Large Language Models (TRT-LLM), which accelerates and optimizes inference performance on the latest LLMs on NVIDIA GPUs. With NeMo, VMware Private AI Foundation with NVIDIA will enable enterprises to pull in their own data to build and run custom generative AI models on VMware’s hybrid cloud infrastructure.\nAt VMware Explore 2023, NVIDIA and VMware will highlight how developers within enterprises can use the new NVIDIA AI Workbench to pull community models, like Llama 2, available on Hugging Face, customize them remotely and deploy production-grade generative AI in VMware environments.\nBroad Ecosystem Support for VMware Private AI Foundation With NVIDIA\nVMware Private AI Foundation with NVIDIA will be supported by Dell Technologies, Hewlett Packard Enterprise and Lenovo — which will be among the first to offer systems that supercharge enterprise LLM customization and inference workloads with NVIDIA L40S GPUs, NVIDIA BlueField®-3 DPUs and NVIDIA ConnectX®-7 SmartNICs.\nThe NVIDIA L40S GPU enables up to 1.2x more generative AI inference performance and up to 1.7x more training performance compared with the NVIDIA A100 Tensor Core GPU.\nNVIDIA BlueField-3 DPUs accelerate, offload and isolate the tremendous compute load of virtualization, networking, storage, security and other cloud-native AI services from the GPU or CPU.\nNVIDIA ConnectX-7 SmartNICs deliver smart, accelerated networking for data center infrastructure to boost some of the world’s most demanding AI workloads.\nVMware Private AI Foundation with NVIDIA builds on the companies’ decade-long partnership. Their co-engineering work optimized VMware’s cloud infrastructure to run NVIDIA AI Enterprise with performance comparable to bare metal. Mutual customers further benefit from the resource and infrastructure management and flexibility enabled by VMware Cloud Foundation.\nVMware intends to release VMware Private AI Foundation with NVIDIA in early 2024.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Silicon Volley: Designers Tap Generative AI for a Chip Assist",
    "link": "https://blogs.nvidia.com/blog/llm-semiconductors-chip-nemo/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNWtTVU5ZUW1OTVVEVTFXRWd6VFJDa0FSaTBBaWdCTWdPRmNRZw=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-10-30T07:00:00.000Z",
    "time": "Oct 30, 2023",
    "articleType": "regular",
    "content": "A research paper released today describes ways generative AI can assist one of the most complex engineering efforts: designing semiconductors.\nThe work demonstrates how companies in highly specialized fields can train large language models (LLMs) on their internal data to build assistants that increase productivity.\nFew pursuits are as challenging as semiconductor design. Under a microscope, a state-of-the-art chip like an NVIDIA H100 Tensor Core GPU (above) looks like a well-planned metropolis, built with tens of billions of transistors, connected on streets 10,000x thinner than a human hair.\nMultiple engineering teams coordinate for as long as two years to construct one of these digital megacities.\nSome groups define the chip’s overall architecture, some craft and place a variety of ultra-small circuits, and others test their work. Each job requires specialized methods, software programs and computer languages.\nA Broad Vision for LLMs\n“I believe over time large language models will help all the processes, across the board,” said Mark Ren, an NVIDIA Research director and lead author on the paper.\nBill Dally, NVIDIA’s chief scientist, announced the paper today in a keynote at the International Conference on Computer-Aided Design, an annual gathering of hundreds of engineers working in the field called electronic design automation, or EDA.\n“This effort marks an important first step in applying LLMs to the complex work of designing semiconductors,” said Dally at the event in San Francisco. “It shows how even highly specialized fields can use their internal data to train useful generative AI models.”\nThe paper details how NVIDIA engineers created for their internal use a custom LLM, called ChipNeMo, trained on the company’s internal data to generate and optimize software and assist human designers.\nLong term, engineers hope to apply generative AI to each stage of chip design, potentially reaping significant gains in overall productivity, said Ren, whose career spans more than 20 years in EDA.\nAfter surveying NVIDIA engineers for possible use cases, the research team chose three to start: a chatbot, a code generator and an analysis tool.\nThe latter — a tool that automates the time-consuming tasks of maintaining updated descriptions of known bugs — has been the most well-received so far.\nA prototype chatbot that responds to questions about GPU architecture and design helped many engineers quickly find technical documents in early tests.\nA code generator will help designers write software for a chip design.\nA code generator in development (demonstrated above)  already creates snippets of about 10-20 lines of software in two specialized languages chip designers use. It will be integrated with existing tools, so engineers have a handy assistant for designs in progress.\nCustomizing AI Models With NVIDIA NeMo\nThe paper mainly focuses on the team’s work gathering its design data and using it to create a specialized generative AI model, a process portable to any industry.\nAs its starting point, the team chose a foundation model and customized it with NVIDIA NeMo, a framework for building, customizing and deploying generative AI models that’s included in the NVIDIA AI Enterprise software platform. The selected NeMo model sports 43 billion parameters, a measure of its capability to understand patterns. It was trained using more than a trillion tokens, the words and symbols in text and software.\nChipNeMo provides an example of how one deeply technical team refined a pretrained model with its own data.\nThe team then refined the model in two training rounds, the first using about 24 billion tokens worth of its internal design data and the second on a mix of about 130,000 conversation and design examples.\nThe work is among several examples of research and proofs of concept of generative AI in the semiconductor industry, just beginning to emerge from the lab.\nOne of the most important lessons Ren’s team learned is the value of customizing an LLM.\nOn chip-design tasks, custom ChipNeMo models with as few as 13 billion parameters match or exceed performance of even much larger general-purpose LLMs like LLaMA2 with 70 billion parameters. In some use cases, ChipNeMo models were dramatically better.\nAlong the way, users need to exercise care in what data they collect and how they clean it for use in training, he added.\nFinally, Ren advises users to stay abreast of the latest tools that can speed and simplify the work.\nNVIDIA Research has hundreds of scientists and engineers worldwide focused on topics such as AI, computer graphics, computer vision, self-driving cars and robotics. Other recent projects in semiconductors include using AI to design smaller, faster circuits and to optimize placement of large blocks.\nEnterprises looking to build their own custom LLMs can get started today using NeMo framework available from GitHub and NVIDIA NGC catalog.\nExplore generative AI sessions and experiences at NVIDIA GTC, the global conference on AI and accelerated computing, running March 18-21 in San Jose, Calif., and online.",
    "favicon": ""
  },
  {
    "title": "NVIDIA’s New Ethernet Networking Platform for AI Available Soon From Dell Technologies, Hewlett Packard Enterprise, Lenovo",
    "link": "https://nvidianews.nvidia.com/news/nvidias-new-ethernet-networking-platform-for-ai-available-soon-from-dell-technologies-hewlett-packard-enterprise-lenovo",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXVhR1pDYVhVM1NVYzVVM0puVFJDb0FSaXNBaWdCTWdhWlVwQVRNUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-11-20T08:00:00.000Z",
    "time": "Nov 20, 2023",
    "articleType": "regular",
    "content": "End-to-End Platform Features Latest NVIDIA Spectrum-X Networking, Provides Foundation for Customers to Transform Business With AI\nNVIDIA today announced that Dell Technologies, Hewlett Packard Enterprise and Lenovo will be the first to integrate NVIDIA Spectrum-X™ Ethernet networking technologies for AI into their server lineups to help enterprise customers speed up generative AI workloads.\nPurpose-built for generative AI, Spectrum-X offers enterprises a new class of Ethernet networking that can achieve 1.6x higher networking performance for AI communication versus traditional Ethernet offerings.\nThe new systems coming from three of the top system makers bring together Spectrum-X with NVIDIA Tensor Core GPUs, NVIDIA AI Enterprise software and NVIDIA AI Workbench software to provide enterprises the building blocks to transform their businesses with generative AI.\n“Generative AI and accelerated computing are driving a generational transition as enterprises upgrade their data centers to serve these workloads,” said Jensen Huang, founder and CEO of NVIDIA. “Accelerated networking is the catalyst for a new wave of systems from NVIDIA’s leading server manufacturer partners to speed the shift to the era of generative AI.”\n“Accelerated computing and networking are key to building systems to meet the demands of large language models and generative AI applications,” said Michael Dell, chairman and CEO of Dell Technologies. “Through our collaboration, Dell Technologies and NVIDIA are providing customers with the infrastructure and software needed to quickly and securely extract intelligence from their data.”\n“Generative AI will undoubtedly drive innovation across multiple industries,” said Antonio Neri, president and CEO of HPE. “These powerful new applications will require a fundamentally different architecture to support a variety of dynamic workloads. To enable customers to realize the full potential of generative AI, HPE is partnering with NVIDIA to build systems with the required power, efficiency and scalability to support these applications.”\n“Generative AI can power unprecedented transformation but places unprecedented demands on enterprise infrastructure,” said Yuanqing Yang, chairman and CEO of Lenovo. “Working closely with NVIDIA, Lenovo is building efficient, accelerated systems with the networking, computing and software needed to power modern AI applications.”\nNetworking Purpose-Built to Accelerate AI\nFor peak AI workload efficiency, Spectrum-X combines the extreme performance of the Spectrum-4 Ethernet switch; the NVIDIA BlueField®-3 SuperNIC, a new class of network accelerators for supercharging hyperscale AI workloads; as well as acceleration software. Spectrum-X complements BlueField-3 DPUs, the world’s most advanced infrastructure computing platform.\nSpectrum-4 is the world’s first 51Tb/sec Ethernet switch for AI, providing highly effective data throughput at scale and under load while minimizing network congestion for multi-tenant, AI cloud workloads. Its intelligent, fine-tuned routing technology enables maximum utilization of network infrastructure at all times.\nBlueField-3 SuperNICs are designed for network-intensive, massively parallel computing, offering up to 400Gb/s RDMA over Converged Ethernet (RoCE) network connectivity between GPU servers and boosting performance for AI training and inference traffic on the east-west network inside the cluster. They also enable secure, multi-tenant data center environments, ensuring deterministic and isolated performance between tenant jobs. Boasting a power-efficient, half-height, half-length PCIe form factor, BlueField-3 SuperNICs are ideal for enterprise-class servers.\nAcceleration software powering Spectrum-X features NVIDIA software development kits such as Cumulus Linux, Pure SONiC and NetQ — which together drive the platform’s breakthrough performance — and the NVIDIA DOCA™ software framework, which is at the heart of BlueField.\nNVIDIA AI Enterprise provides frameworks, pretrained models and development tools for secure, stable and supported production AI. NVIDIA AI Workbench allows developers to quickly create, test and customize pretrained generative AI models on a PC or workstation — then scale them to virtually any data center or cloud.\nNVIDIA Israel-1 Supercomputer Powered by Spectrum-X\nSpectrum-X also enables the NVIDIA Israel-1 supercomputer, a reference architecture for next-generation AI systems. Israel-1 is a collaboration with Dell Technologies, using Dell PowerEdge XE9680 servers powered by the NVIDIA HGX™ H100 eight-GPU platform and BlueField-3 DPUs and SuperNICs with Spectrum-4 switches.\nNew systems from Dell, HPE and Lenovo featuring the complete NVIDIA AI stack are expected in the first quarter of next year.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Hewlett Packard Enterprise and NVIDIA Announce ‘NVIDIA AI Computing by HPE’ to Accelerate Generative AI Industrial Revolution",
    "link": "https://nvidianews.nvidia.com/news/hpe-nvidia-ai-computing-generative-ai",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUZaVGxuWDJ0aWVESk1kblZYVFJDakFSaTBBaWdCTWdrQnNaVEVPbWN0emdF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-18T07:00:00.000Z",
    "time": "Jun 18",
    "articleType": "regular",
    "content": "New Lineup Features First-of-Its-Kind Turnkey, Private-Cloud AI Solution Including Sustainable Accelerated Computing with Full Lifecycle Services to Streamline Time to Value with AI\nHPE Discover 2024—Hewlett Packard Enterprise (NYSE: HPE) and NVIDIA today announced NVIDIA AI Computing by HPE, a portfolio of co-developed AI solutions and joint go-to-market integrations that enable enterprises to accelerate adoption of generative AI.\nAmong the portfolio’s key offerings is HPE Private Cloud AI, a first-of-its-kind solution that provides the deepest integration to date of NVIDIA AI computing, networking and software with HPE’s AI storage, compute and the HPE GreenLake cloud. The offering enables enterprises of every size to gain an energy-efficient, fast and flexible path for sustainably developing and deploying generative AI applications. Powered by the new OpsRamp AI copilot that helps IT operations improve workload and IT efficiency, HPE Private Cloud AI includes a self-service cloud experience with full lifecycle management and is available in four right-sized configurations to support a broad range of AI workloads and use cases.\nAll NVIDIA AI Computing by HPE offerings and services will be available through a joint go-to-market strategy that spans sales teams and channel partners, training and a global network of system integrators — including Deloitte, HCLTech, Infosys, TCS and Wipro — that can help enterprises across a variety of industries run complex AI workloads.\nAnnounced during the HPE Discover keynote by HPE President and CEO Antonio Neri, who was joined by NVIDIA founder and CEO Jensen Huang, NVIDIA AI Computing by HPE marks the expansion of a decades-long partnership and reflects the substantial commitment of time and resources from each company.\n“Generative AI holds immense potential for enterprise transformation, but the complexities of fragmented AI technology contain too many risks and barriers that hamper large-scale enterprise adoption and can jeopardize a company’s most valuable asset — its proprietary data,” said Neri. “To unleash the immense potential of generative AI in the enterprise, HPE and NVIDIA co-developed a turnkey private cloud for AI that will enable enterprises to focus their resources on developing new AI use cases that can boost productivity and unlock new revenue streams.”\n“Generative AI and accelerated computing are fueling a fundamental transformation as every industry races to join the industrial revolution,” said Huang. “Never before have NVIDIA and HPE integrated our technologies so deeply — combining the entire NVIDIA AI computing stack along with HPE’s private cloud technology — to equip enterprise clients and AI professionals with the most advanced computing infrastructure and services to expand the frontier of AI.”\nHPE and NVIDIA co-developed Private Cloud AI portfolio\nHPE Private Cloud AI delivers a unique, cloud-based experience to accelerate innovation and return on investment while managing enterprise risk from AI. The solution offers:\n●       Support for inference, fine-tuning and RAG AI workloads that utilize proprietary data.\n●       Enterprise control for data privacy, security, transparency and governance requirements.\n●       Cloud experience with ITOps and AIOps capabilities to increase productivity.\n●       Fast path to consume flexibly to meet future AI opportunities and growth.\nCurated AI and data software stack in HPE Private Cloud AI\nThe foundation of the AI and data software stack starts with the NVIDIA AI Enterprise software platform, which includes NVIDIA NIM™ inference microservices.\nNVIDIA AI Enterprise accelerates data science pipelines and streamlines development and deployment of production-grade copilots and other GenAI applications. Included with NVIDIA AI Enterprise, NVIDIA NIM delivers easy-to-use microservices for optimized AI model inferencing offering a smooth transition from prototype to secure deployment of AI models in a variety of use cases.\nComplementing NVIDIA AI Enterprise and NVIDIA NIM, HPE AI Essentials software delivers a ready to run set of curated AI and data foundation tools with a unified control plane that provide adaptable solutions, ongoing enterprise support and trusted AI services, such as data and model compliance and extensible features that ensure AI pipelines are in compliance, explainable and reproducible throughout the AI lifecycle.\nTo deliver optimal performance for the AI and data software stack, HPE Private Cloud AI delivers a fully integrated AI infrastructure stack that includes NVIDIA Spectrum-X™ Ethernet networking, HPE GreenLake for File Storage and HPE ProLiant servers with support for NVIDIA L40S, NVIDIA H100 NVL Tensor Core GPUs and the NVIDIA GH200 NVL2 platform.\nCloud experience enabled by HPE GreenLake cloud\nHPE Private Cloud AI offers a self-service cloud experience enabled by HPE GreenLake cloud. Through a single, platform-based control plane, HPE Greenlake cloud services provide manageability and observability to automate, orchestrate and manage endpoints, workloads and data across hybrid environments. This includes sustainability metrics for workloads and endpoints.\nHPE GreenLake cloud and OpsRamp AI infrastructure observability and copilot assistant\nOpsRamp's IT operations are integrated with HPE GreenLake cloud to deliver observability and AIOps to all HPE products and services. OpsRamp now provides observability for the end-to-end NVIDIA accelerated computing stack, including NVIDIA NIM and AI software, NVIDIA Tensor Core GPUs and AI clusters as well as NVIDIA Quantum InfiniBand and NVIDIA Spectrum Ethernet switches. IT administrators can gain insights to identify anomalies and monitor their AI infrastructure and workloads across hybrid, multi-cloud environments.\nThe new OpsRamp operations copilot utilizes NVIDIA’s accelerated computing platform to analyze large datasets for insights with a conversational assistant, boosting productivity for operations management. OpsRamp will also integrate with CrowdStrike APIs so customers can see a unified service map view of endpoint security across their entire infrastructure and applications.\nAccelerate time to value with AI — expanded collaboration with global system integrators\nTo advance the time to value for enterprises to develop industry-focused AI solutions and use cases with clear business benefits, Deloitte, HCLTech, Infosys, TCS and Wipro announced their support of the NVIDIA AI Computing by HPE portfolio and HPE Private Cloud AI as part of their strategic AI solutions and services.\nHPE adds support for NVIDIA’s latest GPUs, CPUs and Superchips\nHPE Cray XD670 supports eight NVIDIA H200 NVL Tensor Core GPUs and is ideal for LLM builders.\nHPE ProLiant DL384 Gen12 server with NVIDIA GH200 NVL2 is ideal for LLM consumers using larger models or RAG.\nHPE ProLiant DL380a Gen12 server support for up to eight NVIDIA H200 NVL Tensor Core GPUs is ideal for LLM users looking for flexibility to scale their GenAI workloads.\nHPE will be time-to-market to support the NVIDIA GB200 NVL72 / NVL2, as well as the new NVIDIA Blackwell, NVIDIA Rubin and NVIDIA Vera architectures.\nHigh-density file storage certified for NVIDIA DGX BasePOD and NVIDIA OVX systems\nHPE GreenLake for File Storage has achieved NVIDIA DGX BasePOD certification and NVIDIA OVX™ storage validation, providing customers with a proven enterprise file storage solution for accelerating AI, GenAI and GPU-intensive workloads at scale. HPE will be a time-to-market partner on upcoming NVIDIA reference architecture storage certification programs.\nHPE Private Cloud AI is expected to be generally available in the fall.\nHPE ProLiant DL380a Gen12 server with NVIDIA H200 NVL Tensor Core GPUs is expected to be generally available in the fall.\nHPE ProLiant DL384 Gen12 server with dual NVIDIA GH200 NVL2 is expected to be generally available in the fall.\nHPE Cray XD670 server with NVIDIA H200 NVL is expected to be generally available in the summer.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Isaac Taps Generative AI for Manufacturing and Logistics Applications",
    "link": "https://blogs.nvidia.com/blog/isaac-generative-ai-manufacturing-logistics/",
    "image": "https://news.google.com/api/attachments/CC8iMkNnNXVjVW96TFhoS1NFRm9halJDVFJDakFSaTBBaWdCTWdzQllJYWhNR2FodEZpSm9n=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "The NVIDIA Isaac robotics platform is tapping into the latest generative AI and advanced simulation technologies to accelerate AI-enabled robotics.\nAt GTC today, NVIDIA announced Isaac Manipulator and Isaac Perceptor — a collection of foundation models, robotics tools and GPU-accelerated libraries.\nOn stage before a crowd of 10,000-plus, NVIDIA founder and CEO Jensen Huang demonstrated Project GR00T, which stands for Generalist Robot 00 Technology, a general-purpose foundation model for humanoid robot learning. Project GR00T leverages various tools from the NVIDIA Isaac robotics platform to create AI for humanoid robots.\n“Building foundation models for general humanoid robots is one of the most exciting problems to solve in AI today,” said Huang. “The enabling technologies are coming together for leading roboticists around the world to take giant leaps toward artificial general robotics.”\nNVIDIA also announced a new computer for humanoid robots based on the NVIDIA Thor system-on-a-chip, and new tools for the NVIDIA Isaac robotics platform, including Isaac Lab for robot learning and NVIDIA OSMO for hybrid-cloud workflow orchestration, which are instrumental in the development of Project GR00T and foundation models for robots.\nIntroducing Isaac Manipulator for Robotic Arms\nNVIDIA Isaac Manipulator offers a collection of state-of-the-art motion generation and modular AI capabilities for robotic arms, with a robust collection of foundation models and GPU-accelerated libraries.\nRobotics developers can use combinations of software components customized for specific tasks to perceive and interact with surroundings, enabling the building of scalable and repeatable workflows for dynamic manipulation tasks by accelerating AI model training and task programming.\n“Incorporating new tools for foundation model generation into the Isaac platform accelerates the development of smarter, more flexible robots that can be generalized to do many tasks,” said Deepu Talla, vice president of robotics and edge computing at NVIDIA.\nLeading robotics companies Yaskawa, Solomon, PickNik Robotics, READY Robotics, Franka Robotics and Universal Robots, a Teradyne company, are partnering with NVIDIA to bring Isaac Manipulator to their customers.\n“By bringing NVIDIA AI tools and capabilities to Yaskawa’s automation solutions, we’re pushing the boundaries of where robots can be deployed across industries,“ said Masahiro Ogawa, President, Yaskawa. “This will significantly influence various industries.”\nNVIDIA is introducing foundation models to augment existing robot manipulation systems. These will help develop robots to sense, adapt and reprogram for varied environments and applications in smart manufacturing, handling pick-and-place tasks, machine tending and assembly with the following:\nFoundationPose is a pioneering foundation model for 6D pose estimation and tracking of previously unseen objects.\ncuMotion taps into the parallel processing of NVIDIA GPUs for solving robot motion planning problems at industrial scale by running many trajectory optimizations at the same time to provide the best solution.\nFoundationGrasp is a transformer-based model that can make dense grasp predictions for unknown 3D objects.\nSyntheticaDETR is an object detection model for indoor environments that allows faster detection, rendering and training with new objects.\nIntroducing Isaac Perceptor for Autonomous Mobile Robots Visual AI\nManufacturing and fulfillment operations are adopting autonomous mobile robots (AMRs) to improve efficiency and worker safety as well as to reduce error rates and costs.\nIsaac Perceptor provides multi-camera, 360-degree vision capabilities, offering early industry partners  such as ArcBest, BYD and KION Group advanced visual AI for their AMR installations that assist in material handling operations.\nThe NVIDIA Nova Orin DevKit — created in collaboration with Segway Robotics and Leopard Imaging — allows companies to quickly develop, evaluate and deploy Isaac Perceptor.\n“ArcBest is collaborating with NVIDIA to bring leading-edge machine vision technology into the logistics space,” said Michael Newcity, chief innovation officer of ArcBest and president of ArcBest Technologies. “Using the Isaac Perceptor platform in our Vaux Smart Autonomy AMR forklifts and reach trucks enables better perception, semantic-aware navigation and 3D mapping for obstacle detection in material handling processes across warehouses, distribution centers and manufacturing facilities.”\nProject GR00T for Humanoid Robotics Development Takes a Bow\nDemonstrated at GTC, GR00T-powered humanoid robots can take multimodal instructions — text, video and demonstrations — as well as their previous interactions to produce the desired action for the robot. GR00T was shown on four humanoid robots from different companies, including Agility Robotics, Apptronik, Fourier Intelligence and Unitree Robotics. Agility Robotics and Unitree Robotics are members of the NVIDIA Inception program for cutting-edge startups.\nHumanoid robots are complex systems that require heterogeneous computing to meet the needs of high-frequency low-level controls, sensor fusion and perception, task planning and human-robot interaction. NVIDIA unveiled a new Jetson Thor-based computer for humanoid robots, built on the NVIDIA Thor SoC.\nJetson Thor includes a next-generation GPU based on the NVIDIA Blackwell Architecture with a transformer engine delivering 800 teraflops of 8-bit floating point AI performance to run multimodal generative AI models like GR00T. With an integrated functional safety processor, a high-performance CPU cluster and 100GB of ethernet bandwidth, it significantly simplifies design and integration efforts.\nProject GR00T uses Isaac tools that are available to robotics developers for building and testing foundation models. These include Isaac Lab, a new lightweight simulation app built in Isaac Sim to train this humanoid robot model at scale, and OSMO, a cloud workflow orchestration platform for managing the training and simulation workloads.\nAccelerating Robot Learning With Isaac Lab\nRobots that require advanced locomotion skills, whether with walking or grasping, need to use deep reinforcement learning in a simulated environment and be trained repeatedly in a virtual environment to learn skills. However, this utility becomes more useful when the model transfers to the real robot deployment, which has been demonstrated with Project GR00T.\nAs the successor to Isaac Gym, Isaac Lab benefits from NVIDIA Omniverse technologies for physics-informed, photorealistic, perception-based reinforcement learning tasks. Isaac Lab is an open-source, performance-optimized application for robot learning built on the Isaac Sim platform. It incorporates reinforcement learning APIs and a developer-friendly tasking framework.\nEnabling Cloud-Native Robotics Workflow Scheduling With NVIDIA OSMO\nNVIDIA OSMO scales workloads across distributed environments. For robotics workloads with complex multi-stage and multi-container workflows, the platform provides a location-agnostic deployment option and dataset management and traceability features for deployed models.\n“Boston Dynamics employs a range of machine learning, reinforcement learning and AI technologies to power our robots,” said Pat Marion, machine learning and perception lead at Boston Dynamics. “To effectively manage the large training workloads, we’re using NVIDIA OSMO, an infrastructure solution that lets our machine learning engineers streamline their workflows and dedicate their expertise to tackling the hard robotics problems.”\nOSMO supports GR00T, for example, by concurrently running models on NVIDIA DGX for training and NVIDIA OVX servers for live reinforcement learning in simulation. This workload involves generating and training models iteratively in a loop. OSMO’s ability to manage and schedule workloads across distributed environments allows for the seamless coordination of DGX and OVX systems, enabling efficient and iterative model development. Once the model is ready for testing and validation, OSMO can uniquely orchestrate software-in-the-loop workflows on OVX (x86-64) as well as hardware-in-the-loop workflows with NVIDIA Jetson (aarch64) compute resources.\nSupporting the ROS Ecosystem of Developers\nNVIDIA joined the Open Source Robotics Alliance (OSRA) as a founding member and platinum sponsor. OSRA is a new initiative by Open Source Robotics Foundation to foster collaboration, innovation and technical guidance in the robotics community by supporting several open-source robotics projects, including the Robot Operating System (ROS).\n“The increasing capability of autonomous robots is driving a rise in demand for more powerful but still energy-efficient onboard computing,” said Vanessa Yamzon Orsi, CEO of Open Robotics. “The ROS community is experiencing this demand firsthand, and our users are increasingly taking advantage of advanced accelerated computing hardware from industry leaders such as NVIDIA.”",
    "favicon": ""
  },
  {
    "title": "NVIDIA Digital Human Technologies Bring AI Characters to Life",
    "link": "https://nvidianews.nvidia.com/news/nvidia-digital-human-technologies-bring-ai-characters-to-life-6900750",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUViRjk1VjE5MVNEaDZNekYyVFJDb0FSaXNBaWdCTWdrRlVZNmxvcWd0Q1FJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Leading AI Developers Use Suite of NVIDIA Technologies to Create Lifelike Avatars and Dynamic Characters for Everything From Games to Healthcare, Financial Services and Retail Applications\nGDC—NVIDIA announced today that leading AI application developers across a wide range of industries are using NVIDIA digital human technologies to create lifelike avatars for commercial applications and dynamic game characters. The results are on display at GTC, the global AI conference held this week in San Jose, Calif., and can be seen in technology demonstrations from Hippocratic AI, Inworld AI, UneeQ and more.\nNVIDIA Avatar Cloud Engine (ACE) for speech and animation, NVIDIA NeMo™ for language, and NVIDIA RTX™ for ray-traced rendering are the building blocks that enable developers to create digital humans capable of AI-powered natural language interactions, making conversations more realistic and engaging.\n“NVIDIA offers developers a world-class set of AI-powered technologies for digital human creation,” said John Spitzer, vice president of developer and performance technologies at NVIDIA. “These technologies may power the complex animations and conversational speech required to make digital interactions feel real.”\nThe digital human technologies suite includes language, speech, animation and graphics powered by AI:\nNVIDIA ACE — technologies that help developers bring digital humans to life with facial animation powered by NVIDIA Audio2Face™ and speech powered by NVIDIA Riva automatic speech recognition (ASR) and text-to-speech (TTS). ACE microservices are flexible in allowing models to run across cloud and PC depending on the local GPU capabilities to help ensure the user receives the best experience.\nNVIDIA NeMo — an end-to-end platform that enables developers to deliver enterprise-ready generative AI models with precise data curation, cutting-edge customization, retrieval-augmented generation and accelerated performance.\nNVIDIA RTX — a collection of rendering technologies, such as RTX Global Illumination (RTXGI) and DLSS 3.5, that enable real-time path tracing in games and applications.\nBuilding Blocks for Digital Humans and Virtual Assistants\nTo showcase the new capabilities of its digital human technologies, NVIDIA worked across industries with leading developers, such as Hippocratic AI, Inworld AI and UneeQ, on a series of new demonstrations.\nHippocratic AI has created a safety-focused, LLM-powered, task-specific Healthcare Agent. The agent calls patients on the phone, follows up on care coordination tasks, delivers preoperative instructions, performs post-discharge management and much more. For GTC, NVIDIA collaborated with Hippocratic AI to extend its solution to use NVIDIA ACE microservices, NVIDIA Audio2Face along with NVIDIA Animation graph and NVIDIA Omniverse™ Streamer Client to show the potential of a generative AI healthcare agent avatar.\n“Our digital assistants provide helpful, timely and accurate information to patients worldwide,” said Munjal Shah, cofounder and CEO of Hippocratic AI. “NVIDIA ACE technologies bring them to life with cutting-edge visuals and realistic animations that help better connect to patients.”\nUneeQ is an autonomous digital human platform specialized in creating AI-powered avatars for customer service and interactive applications. Its digital humans represent brands online, communicating with customers in real time to give them confidence in their purchases. UneeQ integrated the NVIDIA Audio2Face microservice into its platform and combined it with Synanim ML to create highly realistic avatars for a better customer experience and engagement.\n“UneeQ combines NVIDIA animation AI with our own Synanim ML synthetic animation technology to deliver real-time digital human interactions that are emotionally responsive and deliver dynamic experiences powered by conversational AI,” said Danny Tomsett, founder and CEO of UneeQ.\nBringing Dynamic Non-Playable Characters to Games\nNVIDIA ACE is a suite of technologies designed to bring game characters to life. Covert Protocol is a new technology demonstration, created by Inworld AI in partnership with NVIDIA, that pushes the boundary of what character interactions in games can be. Inworld’s AI engine has integrated NVIDIA Riva for accurate speech-to-text and NVIDIA Audio2Face to deliver lifelike facial performances.\nInworld’s AI engine takes a multimodal approach to the performance of non-playable characters (NPCs), bringing together cognition, perception and behavior systems for an immersive narrative with stunning RTX-rendered characters set in a beautifully crafted environment.\n“The combination of NVIDIA ACE microservices and the Inworld Engine enables developers to create digital characters that can drive dynamic narratives, opening new possibilities for how gamers can decipher, deduce and play,” said Kylan Gibbs, CEO of Inworld AI.\nGame publishers worldwide are evaluating how NVIDIA ACE can improve the gaming experience.\nDevelopers Across Healthcare, Gaming, Financial Services, Media & Entertainment and Retail Embrace ACE\nTop game and digital human developers are pioneering ways ACE and generative AI technologies can be used to transform interactions between players and NPCs in games and applications.\nDevelopers and platforms embracing ACE include Convai, Cyber Agent, Data Monsters, Deloitte, Hippocratic AI, IGOODI, Inworld AI, Media.Monks, miHoYo, NetEase Games, Perfect World, Openstream, OurPalm, Quantiphi, Rakuten Securities, Slalom, SoftServe, Tencent, Top Health Tech, Ubisoft, UneeQ and Unions Avatars.\nMore information on NVIDIA ACE is available at https://developer.nvidia.com/ace. Platform developers can incorporate the full suite of digital human technologies or individual microservices into their product offerings.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Say It Again: ChatRTX Adds New AI Models, Features in Latest Update",
    "link": "https://blogs.nvidia.com/blog/ai-decoded-chatrtx-update/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVJRMDE0V2xKQk5XNUNXRGhoVFJDakFSaTBBaWdCTWdZcGhJeHdQUVU=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-01T07:00:00.000Z",
    "time": "May 1",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Generative AI Is Opening the Next Era of Drug Discovery and Design",
    "link": "https://blogs.nvidia.com/blog/drug-discovery-bionemo-generative-ai/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVRMVEZZTkZvdFIzaDFTa1ZoVFJDakFSaTBBaWdCTWdZQkVJNkFHQWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Sharper Image: GeForce NOW Update Delivers Stunning Visuals to Android Devices",
    "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-android-higher-resolutions/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTZlSFpMT1ZKVk56STBZa3RWVFJDakFSaTBBaWdCTWdhZFU0NXNKUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-25T08:00:00.000Z",
    "time": "Jan 25",
    "articleType": "regular",
    "content": "This GFN Thursday levels up PC gaming on mobile with higher-resolution support on Android devices.\nThis week also brings 10 new games to the GeForce NOW library, including Enshrouded.\nGeForce NOW transforms nearly any device into a high-powered PC gaming rig, and members streaming on Android can now access that power from the palms of their hands. The GeForce NOW Android app, rolling out now to members, unlocks a new level of visual quality for Ultimate members gaming on mobile, with improved support for streaming up to 1440p resolution at 120 frames per second.\nExplore the vibrant neon landscapes of Cyberpunk 2077, stream triple-A titles like Baldur’s Gate 3 and Monster Hunter: World, and play the latest releases in the cloud, including Prince of Persia: The Lost Crown and Exoprimal — all on the go with higher resolutions for more immersive gameplay.\nUltimate members can stream these and over 1,800 titles from the GeForce NOW library on select 120Hz Android phones and tablets at pixel-perfect quality. Plus, they can take gameplay even further with eight-hour sessions and tap GeForce RTX 4080-powered servers for faster access to their gaming libraries.\n“We fight because we choose to.”\nLead a team of specialists through a story-driven campaign set in the SG-1 universe of Stargate: Timekeepers from Slitherine Ltd. Sneak characters behind enemy lines, use their unique skills, craft the perfect plan to unravel a time-loop mystery, and defeat the Goa’uld threat. It’s available to stream from the cloud this week.\nMore titles joining the cloud this week include:\nStargate: Timekeepers (New release on Steam, Jan. 23)\nEnshrouded (New release on Steam, Jan. 24)\nFirefighting Simulator – The Squad (Steam)\nMetal: Hellsinger (Xbox, available on the Microsoft Store)\nRoad 96: Mile 0 (Xbox, available on the Microsoft Store)\nShadow Tactics: Blades of the Shogun (Steam)\nShadow Tactics: Blades of the Shogun – Aiko’s Choice (Steam)\nSolasta: Crown of the Magister (Steam)\nTails Noir (Xbox, available on the Microsoft Store)\nGames from Spike Chunsoft will be removed from the GeForce NOW library at the request of the publisher. Fourteen titles are leaving on Friday, Feb. 2, so be sure to catch them before they go:\nConception PLUS: Maidens of the Twelve Stars\nDanganronpa Another Episode: Ultra Despair Girls\nRe: ZERO – Starting Life in Another World – The Prophecy of the Throne\nShiren the Wanderer: The Tower of Fortune and the Dice of Fate\nZero Escape: The Nonary Games\nWhat are you planning to play this weekend? Let us know on X or in the comments below.\noh, you dropped this: 💚📱🌩️\n— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) January 24, 2024",
    "favicon": ""
  },
  {
    "title": "AWS and NVIDIA Announce Strategic Collaboration to Offer New Supercomputing Infrastructure, Software and Services for Generative AI",
    "link": "https://nvidianews.nvidia.com/news/aws-nvidia-strategic-collaboration-for-generative-ai",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNU5Uemt4WWpFeFNIQmhVemhaVFJDb0FSaXNBaWdCTWdNUmNoQQ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-11-28T08:00:00.000Z",
    "time": "Nov 28, 2023",
    "articleType": "regular",
    "content": "AWS to offer first cloud AI supercomputer with NVIDIA Grace Hopper Superchip and AWS UltraCluster scalability\nNVIDIA DGX Cloud—first to feature NVIDIA GH200 NVL32—coming to AWS\nCompanies partner on Project Ceiba—the world’s fastest GPU-powered AI supercomputer and newest NVIDIA DGX Cloud supercomputer for NVIDIA AI R&D and custom model development\nNew Amazon EC2 instances powered by NVIDIA GH200, H200, L40S and L4 GPUs supercharge generative AI, HPC, design and simulation workloads\nNVIDIA software on AWS—NeMo LLM framework, NeMo Retriever and BioNeMo—to boost generative AI development for custom models, semantic retrieval and drug discovery\nAWS re:Invent—Amazon Web Services, Inc. (AWS), an Amazon.com, Inc. company (NASDAQ: AMZN), and NVIDIA (NASDAQ: NVDA) today announced an expansion of their strategic collaboration to deliver the most-advanced infrastructure, software and services to power customers’ generative artificial intelligence (AI) innovations.\nThe companies will bring together the best of NVIDIA and AWS technologies—from NVIDIA’s newest multi-node systems featuring next-generation GPUs, CPUs and AI software, to AWS Nitro System advanced virtualization and security, Elastic Fabric Adapter (EFA) interconnect, and UltraCluster scalability—that are ideal for training foundation models and building generative AI applications.\nThe expanded collaboration builds on a longstanding relationship that has fueled the generative AI era by offering early machine learning (ML) pioneers the compute performance required to advance the state-of-the-art in these technologies.\nAs part of the expanded collaboration to supercharge generative AI across all industries:\nAWS will be the first cloud provider to bring NVIDIA® GH200 Grace Hopper Superchips with new multi-node NVLink™ technology to the cloud. The new NVIDIA GH200 NVL32 multi-node platform connects 32 Grace Hopper Superchips with NVIDIA NVLink and NVSwitch™ technologies into one instance. The platform will be available on Amazon Elastic Compute Cloud (Amazon EC2) instances connected with Amazon’s powerful networking (EFA), supported by advanced virtualization (AWS Nitro System), and hyper-scale clustering (Amazon EC2 UltraClusters), enabling joint customers to scale to thousands of GH200 Superchips.\nNVIDIA and AWS will collaborate to host NVIDIA DGX™ Cloud—NVIDIA’s AI-training-as-a-service—on AWS. It will be the first DGX Cloud featuring GH200 NVL32, providing developers the largest shared memory in a single instance. DGX Cloud on AWS will accelerate training of cutting-edge generative AI and large language models that can reach beyond 1 trillion parameters.\nNVIDIA and AWS are partnering on Project Ceiba to design the world’s fastest GPU-powered AI supercomputer—an at-scale system with GH200 NVL32 and Amazon EFA interconnect hosted by AWS for NVIDIA’s own research and development team. This first-of-its-kind supercomputer—featuring 16,384 NVIDIA GH200 Superchips and capable of processing 65 exaflops of AI—will be used by NVIDIA to propel its next wave of generative AI innovation.\nAWS will introduce three additional new Amazon EC2 instances: P5e instances, powered by NVIDIA H200 Tensor Core GPUs, for large-scale and cutting-edge generative AI and HPC workloads, and G6 and G6e instances, powered by NVIDIA L4 GPUs and NVIDIA L40S GPUs, respectively, for a wide set of applications such as AI fine-tuning, inference, graphics and video workloads. G6e instances are particularly suitable for developing 3D workflows, digital twins and other applications using NVIDIA Omniverse™, a platform for connecting and building generative AI-enabled 3D applications.\n“AWS and NVIDIA have collaborated for more than 13 years, beginning with the world’s first GPU cloud instance. Today, we offer the widest range of NVIDIA GPU solutions for workloads including graphics, gaming, high performance computing, machine learning, and now, generative AI,” said Adam Selipsky, CEO at AWS. “We continue to innovate with NVIDIA to make AWS the best place to run GPUs, combining next-gen NVIDIA Grace Hopper Superchips with AWS’s EFA powerful networking, EC2 UltraClusters’ hyper-scale clustering, and Nitro’s advanced virtualization capabilities.”\n“Generative AI is transforming cloud workloads and putting accelerated computing at the foundation of diverse content generation,” said Jensen Huang, founder and CEO of NVIDIA. “Driven by a common mission to deliver cost-effective state-of-the-art generative AI to every customer, NVIDIA and AWS are collaborating across the entire computing stack, spanning AI infrastructure, acceleration libraries, foundation models, to generative AI services.”\nNew Amazon EC2 Instances Combine State-of-the-Art from NVIDIA and AWS\nAWS will be the first cloud provider to offer NVIDIA GH200 Grace Hopper Superchips with multi-node NVLink technology. Each GH200 Superchip combines an Arm-based Grace CPU with an NVIDIA Hopper™ architecture GPU on the same module. A single Amazon EC2 instance with GH200 NVL32 can provide up to 20 TB of shared memory to power terabyte-scale workloads.\nThese instances will take advantage of AWS’s third-generation Elastic Fabric Adapter (EFA) interconnect, providing up to 400 Gbps per Superchip of low-latency, high-bandwidth networking throughput, enabling customers to scale to thousands of GH200 Superchips in EC2 UltraClusters.\nAWS instances with GH200 NVL32 will provide customers on-demand access to supercomputer-class performance, which is critical for large-scale AI/ML workloads that need to be distributed across multiple nodes for complex generative AI workloads—spanning FMs, recommender systems, and vector databases.\nNVIDIA GH200-powered EC2 instances will feature 4.5 TB of HBM3e memory—a 7.2x increase compared to current generation H100-powered EC2 P5d instances—allowing customers to run larger models, while improving training performance. Additionally, CPU-to-GPU memory interconnect provides up to 7x higher bandwidth than PCIe, enabling chip-to-chip communications that extend the total memory available for applications.\nAWS instances with GH200 NVL32 will be the first AI infrastructure on AWS to feature liquid cooling to help ensure densely-packed server racks can efficiently operate at maximum performance.\nEC2 instances with GH200 NVL32 will also benefit from the AWS Nitro System, the underlying platform for next-generation EC2 instances. The Nitro System offloads I/O for functions from the host CPU/GPU to specialized hardware to deliver more consistent performance, while its enhanced security protects customer code and data during processing.\nAWS First to Host NVIDIA DGX Cloud Powered by Grace Hopper\nAWS will team up with NVIDIA to host NVIDIA DGX Cloud powered by GH200 NVL32 NVLink infrastructure. NVIDIA DGX Cloud is an AI supercomputing service that gives enterprises fast access to multi-node supercomputing for training the most complex LLM and generative AI models, with integrated NVIDIA AI Enterprise software, and direct access to NVIDIA AI experts.\nMassive Project Ceiba Supercomputer to Supercharge NVIDIA’s AI Development\nThe Project Ceiba supercomputer that AWS and NVIDIA are collaborating on will be integrated with AWS services, such as Amazon Virtual Private Cloud (VPC) encrypted networking and Amazon Elastic Block Store high-performance block storage, giving NVIDIA access to a comprehensive set of AWS capabilities.\nNVIDIA will use the supercomputer for research and development to advance AI for LLMs, graphics and simulation, digital biology, robotics, self-driving cars, Earth-2 climate prediction and more.\nNVIDIA and AWS Supercharge Generative AI, HPC, Design and Simulation\nTo power the development, training and inference of the largest LLMs, AWS P5e instances will feature NVIDIA’s latest H200 GPUs that offer 141 GB of HBM3e GPU memory, which is 1.8x larger and 1.4x faster than H100 GPUs. This boost in GPU memory, along with up to 3,200 Gbps of EFA networking enabled by the AWS Nitro System, will enable customers to continue to build, train and deploy their cutting-edge models on AWS.\nTo deliver cost-effective, energy-efficient solutions for video, AI and graphics workloads, AWS announced new Amazon EC2 G6e instances featuring NVIDIA L40S GPUs and G6 instances powered by L4 GPUs. The new offerings can help startups, enterprises and researchers meet their AI and high-fidelity graphics needs.\nG6e instances are built to handle complex workloads such as generative AI and digital twin applications. Using NVIDIA Omniverse, photorealistic 3D simulations can be developed, contextualized and enhanced using real-time data from services such as AWS IoT TwinMaker, intelligent chatbots, assistants, search and summarization. Amazon Robotics and Amazon Fulfillment Centers will be able to integrate digital twins built with NVIDIA Omniverse and AWS IoT TwinMaker to optimize warehouse design and flow, train more intelligent robot assistants and improve deliveries to customers.\nL40S GPUs deliver up to 1.45 petaflops of FP8 performance and feature Ray Tracing cores that offer up to 209 teraflops of ray-tracing performance. L4 GPUs featured in G6 instances will deliver a lower-cost, energy-efficient solution for deploying AI models for natural language processing, language translation, AI video and image analysis, speech recognition, and personalization. L40S GPUs also accelerate graphics workloads, such as creating and rendering real-time, cinematic-quality graphics and game streaming. All three instances will be available in the coming year.\nNVIDIA Software on AWS Boosts Generative AI Development\nIn addition, NVIDIA announced software on AWS to boost generative AI development. NVIDIA NeMo™ Retriever microservice offers new tools to create highly accurate chatbots and summarization tools using accelerated semantic retrieval. NVIDIA BioNeMo™, available on Amazon SageMaker now and coming to AWS on NVIDIA DGX Cloud, enables pharmaceutical companies to speed drug discovery by simplifying and accelerating the training of models using their own data.\nNVIDIA software on AWS is helping Amazon bring new innovations to its services and operations. AWS is using the NVIDIA NeMo framework to train select next-generation Amazon Titan LLMs. Amazon Robotics has begun leveraging NVIDIA Omniverse Isaac to build digital twins for automating, optimizing and planning its autonomous warehouses in virtual environments before deploying them into the real world.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models",
    "link": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/",
    "image": "https://news.google.com/api/attachments/CC8iI0NnNUpRazFSV21rdGQwRm5VV04zVFJDakFSaTBBaWdCTWdB=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-14T07:00:00.000Z",
    "time": "Jun 14",
    "articleType": "regular",
    "content": "NVIDIA today announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry.\nHigh-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access.\nThrough a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.\nThe Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library.\nNemotron-4 340B can be downloaded now from the NVIDIA NGC catalog and from Hugging Face, where developers can also use the Train on DGX Cloud service to easily fine-tune open AI models. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.\nNavigating Nemotron to Generate Synthetic Data\nLLMs can help developers generate synthetic training data in scenarios where access to large, diverse labeled datasets is limited.\nThe Nemotron-4 340B Instruct model creates diverse synthetic data that mimics the characteristics of real-world data, helping improve data quality to increase the performance and robustness of custom LLMs across various domains.\nThen, to boost the quality of the AI-generated data, developers can use the Nemotron-4 340B Reward model to filter for high-quality responses. Nemotron-4 340B Reward grades responses on five attributes: helpfulness, correctness, coherence, complexity and verbosity. It’s currently first place on the Hugging Face RewardBench leaderboard, created by AI2, for evaluating the capabilities, safety and pitfalls of reward models.\nIn this synthetic data generation pipeline, (1) the Nemotron-4 340B Instruct model is first used to produce synthetic text-based output. An evaluator model, (2) Nemotron-4 340B Reward, then assesses this generated text — providing feedback that guides iterative improvements and ensures the synthetic data is accurate, relevant and aligned with specific requirements.\nResearchers can also create their own instruct or reward models by customizing the Nemotron-4 340B Base model using their proprietary data, combined with the included HelpSteer2 dataset.\nFine-Tuning With NeMo, Optimizing for Inference With TensorRT-LLM\nUsing open-source NVIDIA NeMo and NVIDIA TensorRT-LLM, developers can optimize the efficiency of their instruct and reward models to generate synthetic data and to score responses.\nAll Nemotron-4 340B models are optimized with TensorRT-LLM to take advantage of tensor parallelism, a type of model parallelism in which individual weight matrices are split across multiple GPUs and servers, enabling efficient inference at scale.\nNemotron-4 340B Base, trained on 9 trillion tokens, can be customized using the NeMo framework to adapt to specific use cases or domains. This fine-tuning process benefits from extensive pretraining data and yields more accurate outputs for specific downstream tasks.\nA variety of customization methods are available through the NeMo framework, including supervised fine-tuning and parameter-efficient fine-tuning methods such as low-rank adaptation, or LoRA.\nTo boost model quality, developers can align their models with NeMo Aligner and datasets annotated by Nemotron-4 340B Reward. Alignment is a key step in training LLMs, where a model’s behavior is fine-tuned using algorithms like reinforcement learning from human feedback (RLHF) to ensure its outputs are safe, accurate, contextually appropriate and consistent with its intended goals.\nBusinesses seeking enterprise-grade support and security for production environments can also access NeMo and TensorRT-LLM through the cloud-native NVIDIA AI Enterprise software platform, which provides accelerated and efficient runtimes for generative AI foundation models.\nEvaluating Model Security and Getting Started\nThe Nemotron-4 340B Instruct model underwent extensive safety evaluation, including adversarial tests, and performed well across a wide range of risk indicators. Users should still perform careful evaluation of the model’s outputs to ensure the synthetically generated data is suitable, safe and accurate for their use case.\nDownload Nemotron-4 340B models via NVIDIA NGC and Hugging Face. For more details, read the research papers on the model and dataset.\nSee notice regarding software product information.",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Generative AI Models and NIM Microservices for OpenUSD Language, Geometry, Physics and Materials",
    "link": "https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHlabEk0ZFhwTWFUQnlVa1ZIVFJDb0FSaXNBaWdCTWdZSklZaU55UU0=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-29T20:35:22.000Z",
    "time": "3 days ago",
    "articleType": "regular",
    "content": "New Services Accelerate Universal Scene Description-Based Workflows and Development of Industrial Digital Twins and Robotics\nSIGGRAPH—NVIDIA today announced major advancements to Universal Scene Description, or OpenUSD, that will expand adoption of the universal 3D data interchange framework to robotics, industrial design and engineering, and accelerate developers’ abilities to build highly accurate virtual worlds for the next evolution of AI.\nThrough new OpenUSD-based generative AI and NVIDIA-accelerated development frameworks built on the NVIDIA Omniverse™ platform, more industries can now develop applications for visualizing industrial design and engineering projects, and for simulating environments to build the next wave of physical AI and robots.\nThe new offerings include NVIDIA NIM™ microservices for AI models that can generate OpenUSD language to answer user queries, generate OpenUSD Python code, apply materials to 3D objects, and understand 3D space and physics to help accelerate digital twin development. In addition, new USD connectors to robotics and industrial simulation data formats and developer tools let users stream massive, fully NVIDIA RTX™ ray-traced datasets to Apple Vision Pro.\n“The generative AI boom for heavy industries is here,” said Rev Lebaredian, vice president of Omniverse and simulation technology at NVIDIA. “Until recently, digital worlds have been primarily used by creative industries; now, with the enhancements and accessibility NVIDIA NIM microservices are bringing to OpenUSD, industries of all kinds can build physically based virtual worlds and digital twins to drive innovation while preparing for the next wave of AI: robotics.”\nGenerative AI Comes to USD With NVIDIA NIM\nThe world’s first generative AI models for OpenUSD development, developed by NVIDIA, will be available as NVIDIA NIM microservices. The models enable developers to incorporate generative AI copilots and agents into USD workflows, broadening the possibilities in 3D worlds and helping speed the adoption of USD across a new range of industrial sectors, like manufacturing, automotive and robotics.\nThe microservices available in preview are:\nUSD Code NIM microservice — answers general knowledge OpenUSD questions and automatically generates OpenUSD-Python code based on text prompts that can then be inputted into an OpenUSD viewing app, such as usdview from Pixar, or an NVIDIA Omniverse Kit-based application to visualize the corresponding 3D data.\nUSD Search NIM microservice — enables developers to search through massive libraries of OpenUSD, 3D and image data using natural language or image inputs.\nUSD Validate NIM microservice — checks the compatibility of uploaded files against OpenUSD release versions and generates a fully RTX-rendered, path-traced image, powered by NVIDIA Omniverse Cloud APIs, or application programming interfaces.\nNewly announced microservices that will be available soon are:\nUSD Layout NIM microservice — enables users to assemble OpenUSD-based scenes from a series of text prompts based on spatial intelligence.\nUSD SmartMaterial NIM microservice — predicts and applies a realistic material to a computer-aided design object.\nfVDB Mesh Generation NIM microservice — generates an OpenUSD-based mesh, rendered by Omniverse Cloud APIs, from point-cloud data.\nfVDB Physics Super-Res NIM microservice — performs AI super resolution on a frame or sequence of frames to generate an OpenUSD-based, high-resolution physics simulation.\nfVDB NeRF-XL NIM microservice — generates large-scale neural radiance fields in OpenUSD using Omniverse Cloud APIs.\nFoxconn, a global manufacturing leader with more than 170 factories worldwide, is already benefiting from NVIDIA’s computing platform, using NIM microservices and Omniverse to create a digital twin of a factory under development.\n“Digital twins will help us accelerate the next wave of industrial manufacturing and autonomous machines,” said Zhe Shi, chief digital officer and head of the Smart Manufacturing platform at Foxconn. “NVIDIA Omniverse and the new NIM microservices will democratize the ability to develop digital twins and help our teams build physically based virtual factories faster than ever.”\nWPP, a world leader in marketing and communications services company, is an early adopter of USD Search and USD Code NIM microservices, implementing them in its generative AI-enabled content creation pipeline, built on NVIDIA Omniverse, for customers such as The Coca-Cola Company.\n“The beauty of the innovation is how compatible it is with the way we work, and that it leverages open standards — accelerating not only future work, but allowing us to continue to build on and extend the usefulness of all our previous investments in standards like OpenUSD,” said Stephan Pretorius, chief technology officer at WPP. “Using NVIDIA NIM microservices with NVIDIA Omniverse has made it possible for us to launch innovative new production tools with companies like The Coca-Cola Company at unprecedented speed.”\nUSD Connectors Bring Generative AI to More Industries\nA series of new USD connectors for robotics data formats and streaming to Apple Vision Pro opens the portals of OpenUSD interoperability and advanced authoring to more industries.\nNVIDIA and Siemens, a global leader in industrial automation and software, are extending their collaboration to facilitate more industrial workloads using OpenUSD. Siemens will integrate OpenUSD pipelines with its Simcenter portfolio of simulation technologies to support evidence-based decision-making and collaboration among key stakeholders.\nThis integration enables high-fidelity, real-time, photorealistic visualization of complex simulation data, providing deeper insights into a product’s performance within its real-world operating environment. The work will build on Siemens’ efforts to incorporate Omniverse into its Teamcenter Product Lifecycle Management portfolio.\nNVIDIA also released a connector from Unified Robotics Description Format to OpenUSD, letting roboticists seamlessly bring their robot data across applications, including for design, simulation and reinforcement learning.\nTo further advance OpenUSD ecosystem expansion, NVIDIA announced the OpenUSD Exchange software development kit, enabling developers to build their own robust OpenUSD data connectors.\n“OpenUSD is revolutionizing the way we create and interact with 3D content,” said Steve May, chief technology officer of Pixar and chairman of the Alliance for OpenUSD (AOUSD). “Now, with these new services and APIs for OpenUSD built by NVIDIA, we expect to see accelerated growth and adoption of USD, paving the way for new users and industries to more easily engage with our ecosystem.”\nLast year, NVIDIA cofounded the AOUSD along with Pixar, Adobe, Apple and Autodesk. Through AOUSD, NVIDIA and other collaborators have announced a new OpenUSD release, progress on an OpenUSD core specification and new members.\nThe USD Search, USD Code and USD Validate NIM microservices are available in preview on the NVIDIA API catalog. The OpenUSD to URDF connector is now available with NVIDIA Isaac Sim™.\nDevelopers can get started integrating generative AI into OpenUSD workflows with new Omniverse developer tools and a reference workflow for building a generative AI-enabled synthetic data pipeline with OpenUSD.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "The Creative AI: NVIDIA Studio Unveils New RTX- and AI-Accelerated Tools and Systems for Creators",
    "link": "https://blogs.nvidia.com/blog/studio-rtx-hdr-video-twitch-obs-istock-getty-super-laptop-desktop/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUlWMGRUV0Y4MFVGbDZRVlJCVFJDakFSaTBBaWdCTWdhQklJaVhIQWs=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-01-08T08:00:00.000Z",
    "time": "Jan 8",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Releases Major Omniverse Upgrade With Generative AI and OpenUSD",
    "link": "https://nvidianews.nvidia.com/news/nvidia-releases-major-omniverse-upgrade-with-generative-ai-and-openusd",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHhZMEZSZERSamNtVlZhelpHVFJDb0FSaXNBaWdCTWdhaFU0eU5MUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "New Platform Updates, Connections to Adobe Firefly, OpenUSD to RealityKit, Ada-Generation Systems Accelerate Interoperable 3D Workflows and Industrial Digitalization\nSIGGRAPH—NVIDIA today announced a major release of its NVIDIA Omniverse™ platform, offering new foundation applications and services for developers and industrial enterprises to optimize and enhance their 3D pipelines with the OpenUSD framework and generative AI.\nThe update to Omniverse, an OpenUSD-native software platform for connecting, describing and simulating across 3D tools and applications, accelerates the creation of virtual worlds and advanced workflows for industrial digitalization. Cesium, Convai, Move AI, SideFX Houdini and Wonder Dynamics are now connected to Omniverse via OpenUSD.\nKey highlights from the platform update include advancements to Omniverse Kit — the engine for developing native OpenUSD applications and extensions — as well as to the NVIDIA Omniverse Audio2Face™ foundation app and spatial-computing capabilities.\n“Industrial enterprises are racing to digitalize their workflows, increasing the demand for OpenUSD-enabled, connected, interoperable, 3D software ecosystems,” said Rev Lebaredian, vice president of Omniverse and simulation technology at NVIDIA. “The latest Omniverse update lets developers tap generative AI through OpenUSD to enhance their tools, and it allows enterprises to build larger, more complex world-scale simulations as digital testing grounds for their industrial applications.”\nUpdates to Omniverse Kit include:\nNew modular app building — A new Omniverse Kit Extension Registry, a central repository for accessing, sharing and managing Omniverse extensions, lets developers easily turn functionality on and off in their application, making it easier than ever to build custom apps from over 600 core Omniverse extensions provided by NVIDIA.\nNew developer templates and resources — New application and experience templates provide developers getting started with OpenUSD and Omniverse a major headstart with minimal coding.\nBoosted efficiency and user experience — New rendering optimizations take full advantage of the NVIDIA Ada Lovelace architecture enhancements in NVIDIA RTX™ GPUs with DLSS 3 technology fully integrated into the Omniverse RTX Renderer, and a new AI denoiser enables real-time 4K path tracing of massive industrial scenes.\nNative RTX-powered spatial integration — New extended-reality (XR) developer tools let users build spatial-computing options natively into their Omniverse-based applications, giving users the flexibility to experience their 3D projects and virtual worlds however they like.\nThese platform updates are showcased in Omniverse foundation applications, which are fully customizable reference applications that creators, enterprises and developers can copy, extend or enhance. Upgraded applications include:\nOmniverse USD Composer — Lets 3D users assemble large-scale, OpenUSD-based scenes.\nOmniverse Audio2Face — Provides access to generative AI application programming interfaces that create realistic facial animations and gestures from only an audio file, and now includes multilingual support and a new female base model.\nNVIDIA also announced a broad range of frameworks, resources and services for developers and companies to accelerate the adoption of Universal Scene Description, known as OpenUSD.\nIn addition, the company announced new Omniverse Cloud APIs, built by NVIDIA, for developers to more seamlessly implement and deploy OpenUSD pipelines and applications. For example, ChatUSD is a large language model copilot for developers that can answer USD knowledge questions or generate Python-USD code scripts.\nNew Omniverse connections enabled by OpenUSD are now available, opening more opportunities for industrial enterprises to break data silos in their complex production pipelines.\nExpanding their collaboration across Adobe Substance 3D, generative AI and OpenUSD initiatives, Adobe and NVIDIA announced plans to make Adobe Firefly — Adobe’s family of creative generative AI models — available as APIs in Omniverse, enabling developers and creators to enhance their design processes.\nWonder Dynamics is connected to Omniverse with new OpenUSD export support through its AI platform Wonder Studio, which automatically animates, lights and composes computer-generated characters into live-action scenes. New OpenUSD export support will enable artists to generate and export a complete 3D scene — all from a single camera video.\nLuma AI’s reality-capture models in USDZ format can be readily imported to Omniverse. Tools from avatar company Convai and character-engine company Inworld AI are connected to Omniverse. With AI tools like Convai, creators can add characters in their digital twin environments that can provide relevant information about the environment and objects, be a tour guide or be a virtual robot. Move AI enables single-camera motion capture with the Move One app, which can be used to generate 3D character animations that can then be exported to OpenUSD and used in Omniverse.\nOmniverse users can now build content, experiences and applications that are compatible with other OpenUSD-based spatial computing platforms such as ARKit and RealityKit. Plus, new support for the Khronos Group’s OpenXR open standard expands Omniverse use to more headsets from manufacturers such as HTC VIVE, Magic Leap and Varjo.\nUsers of SideFX Houdini can also now load Houdini Digital Assets directly into the Omniverse viewport, making Houdini-based connected workflows more seamless. The Cesium extension for Omniverse, called Cesium for Omniverse, enables 3D Tiles, an open standard for streaming massive geospatial datasets in virtual worlds, including those supported by OpenUSD. CGI.Backgrounds now has several ultra-high-definition HDRi maps available in USD Composer. The Cadence DataCenter Design Software™, now available via Omniverse, helps users see computational fluid dynamics simulations in the complete context of their digital twin. With the Cadence data center extension, users can plan, test and validate design and operational considerations before implementing them. And the Blackshark.AI world digital twin platform is now connected to Omniverse.\nCustomers Using Omniverse for Digitalization\nCustomers are using Omniverse for tasks ranging from simulating robots to training AI models and improving animation.\nBoston Dynamics AI Institute is using Omniverse to simulate robots and their interactions to enable the design of novel robotics and control systems. Continental, one of the leading companies in automotive and industrialization of autonomous systems, is using Omniverse in its mobile robots business to generate physically accurate synthetic data at scale to train computer-vision AI models and perform system-integration testing.\nVolvo Cars has transitioned its digital twin to be OpenUSD-based, using Omniverse to create immersive visualizations to help customers make online purchasing decisions.\nMarks Design, a brand design and experience agency, is using Omniverse and OpenUSD to streamline collaboration and improve its animation, visualization and rendering workflows.\nNew Omniverse Systems and Partners\nNVIDIA is collaborating with global systems manufacturers to bring RTX workstations optimally configured for Omniverse to millions of designers, architects and engineers. The new systems feature up to four NVIDIA RTX 6000 Ada Generation GPUs, bundled with NVIDIA Omniverse Enterprise software, to accelerate OpenUSD world-building, generative AI-enhanced collaborative design and other industrial digitalization applications.\nOmniverse users can also take advantage of the new NVIDIA L40S GPU, a powerful, universal data center GPU that accelerates the most graphics-intensive workloads.\nThe latest Omniverse release is now available in beta to download for free and coming soon to Omniverse Enterprise.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Brings Business Intelligence to Chatbots, Copilots and Summarization Tools With Enterprise-Grade Generative AI Microservice",
    "link": "https://nvidianews.nvidia.com/news/nemo-retriever-generative-ai-microservice",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDNObk5ZTkdvMmRHeG1UbE5HVFJDakFSaTBBaWdCTWdhVlJJck5xUWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-11-28T08:00:00.000Z",
    "time": "Nov 28, 2023",
    "articleType": "regular",
    "content": "Cadence, Dropbox, SAP, ServiceNow First to Access NVIDIA NeMo Retriever to Optimize Semantic Retrieval for Accurate AI Inference\nAWS re:Invent—NVIDIA today announced a generative AI microservice that lets enterprises connect custom large language models to enterprise data to deliver highly accurate responses for their AI applications.\nNVIDIA NeMo™ Retriever — a new offering in the NVIDIA NeMo family of frameworks and tools for building, customizing and deploying generative AI models — helps organizations enhance their generative AI applications with enterprise-grade retrieval-augmented generation (RAG) capabilities.\nAs a semantic-retrieval microservice, NeMo Retriever helps generative AI applications provide more accurate responses through NVIDIA-optimized algorithms. Developers using the microservice can connect their AI applications to business data wherever it resides across clouds and data centers. It adds NVIDIA-optimized RAG capabilities to AI foundries and is part of the NVIDIA AI Enterprise software platform, available in AWS Marketplace.\nCadence, Dropbox, SAP and ServiceNow are among the pioneers working with NVIDIA to build production-ready RAG capabilities into their custom generative AI applications and services.\n“Generative AI applications with RAG capabilities are the next killer app of the enterprise,” said Jensen Huang, founder and CEO of NVIDIA. “With NVIDIA NeMo Retriever, developers can create customized generative AI chatbots, copilots and summarization tools that can access their business data to transform productivity with accurate and valuable generative AI intelligence.”\nGlobal Leaders Enhance LLM Accuracy With NeMo Retriever\nElectronic systems design leader Cadence serves companies across hyperscale computing, 5G communications, automotive, mobile, aerospace, consumer and healthcare markets. It is working with NVIDIA to develop RAG features for generative AI applications in industrial electronics design.\n“Generative AI introduces innovative approaches to address customer needs, such as tools to uncover potential flaws early in the design process,” said Anirudh Devgan, president and CEO of Cadence. “Our researchers are working with NVIDIA to use NeMo Retriever to further boost the accuracy and relevance of generative AI applications to reveal issues and help customers get high-quality products to market faster.”\nCracking the Code for Accurate Generative AI Applications\nUnlike open-source RAG toolkits, NeMo Retriever supports production-ready generative AI with commercially viable models, API stability, security patches and enterprise support.\nNVIDIA-optimized algorithms power the highest accuracy results in Retriever’s embedding models. The optimized embedding models capture relationships between words, enabling LLMs to process and analyze textual data.\nUsing NeMo Retriever, enterprises can connect their LLMs to multiple data sources and knowledge bases, so that users can easily interact with data and receive accurate, up-to-date answers using simple, conversational prompts. Businesses using Retriever-powered applications can allow users to securely gain access to information spanning numerous data modalities, such as text, PDFs, images and videos.\nEnterprises can use NeMo Retriever to achieve more accurate results with less training, speeding time to market and supporting energy efficiency in the development of generative AI applications.\nReliable, Simple, Secure Deployment With NVIDIA AI Enterprise\nCompanies can deploy NeMo Retriever-powered applications to run during inference on NVIDIA-accelerated computing on virtually any data center or cloud. NVIDIA AI Enterprise supports accelerated, high-performance inference with NVIDIA NeMo, NVIDIA Triton Inference Server™, NVIDIA TensorRT™, NVIDIA TensorRT-LLM and other NVIDIA AI software.\nTo maximize inference performance, developers can run their models on NVIDIA GH200 Grace Hopper Superchips with TensorRT-LLM software.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Launches Cloud Quantum-Computer Simulation Microservices",
    "link": "https://nvidianews.nvidia.com/news/nvidia-launches-cloud-quantum-computer-simulation-microservices",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDJUWEpPWWtwb2VtcHZUWGxXVFJDakFSaTBBaWdCTWdhaFpJak5MUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Available Through Major Cloud Providers, NVIDIA’s Quantum Simulation Platform to Help Scientists Advance Quantum Computing and Algorithm Research\nGTC—NVIDIA today launched a cloud service that allows researchers and developers to push the boundaries of quantum computing exploration in key scientific domains, including chemistry, biology and materials science.\nNVIDIA Quantum Cloud is based on the company’s open-source CUDA-Q™ quantum computing platform, which is used by three-quarters of the companies deploying quantum processing units, or QPUs. As a microservice, it lets users for the first time build and test in the cloud new quantum algorithms and applications — including powerful simulators and tools for hybrid quantum-classical programming.\n“Quantum computing presents the next revolutionary frontier of computing and it’s going to require the world’s most brilliant minds to bring this future one step closer,” said Tim Costa, director of HPC and quantum computing at NVIDIA. “NVIDIA Quantum Cloud breaks down the barriers to explore this transformative technology and lets every scientist in the world harness the power of quantum computing and bring their ideas closer to reality.”\nQuantum Cloud features powerful capabilities and third-party software integrations to accelerate scientific exploration, including:\nThe Generative Quantum Eigensolver, developed in a collaboration with the University of Toronto, leverages large language models (LLMs) to enable a quantum computer to find the ground-state energy of a molecule more quickly.\nClassiq’s integration with CUDA-Q allows quantum researchers to generate large, sophisticated quantum programs, as well as to deeply analyze and execute quantum circuits.\nQC Ware Promethium tackles complex quantum chemistry problems such as molecular simulation.\nNVIDIA has more than 160 partners in its quantum computing ecosystem. Leading cloud service providers are integrating Quantum Cloud into their offerings, including Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure, as are many leading quantum companies, such as IQM Quantum Computers, OQC, ORCA Computing, qBraid and Quantinuum.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Introduces Generative AI Foundry Service on Microsoft Azure for Enterprises and Startups Worldwide",
    "link": "https://nvidianews.nvidia.com/news/nvidia-introduces-generative-ai-foundry-service-on-microsoft-azure-for-enterprises-and-startups-worldwide",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNVRhbGswZG5GS1lXb3dNV1Z1VFJDb0FSaXNBaWdCTWdrUllvek1zV1kxTWdF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-11-15T08:00:00.000Z",
    "time": "Nov 15, 2023",
    "articleType": "regular",
    "content": "SAP, Amdocs, Getty Images Among First to Build Custom LLMs With NVIDIA AI Foundation Models, Train on NVIDIA DGX Cloud, Deploy With NVIDIA AI Enterprise Software\nMicrosoft Ignite—NVIDIA today introduced an AI foundry service to supercharge the development and tuning of custom generative AI applications for enterprises and startups deploying on Microsoft Azure.\nThe NVIDIA AI foundry service pulls together three elements — a collection of NVIDIA AI Foundation Models, NVIDIA NeMo™ framework and tools, and NVIDIA DGX™ Cloud AI supercomputing services — that give enterprises an end-to-end solution for creating custom generative AI models. Businesses can then deploy their customized models with NVIDIA AI Enterprise software to power generative AI applications, including intelligent search, summarization and content generation.\nIndustry leaders SAP SE, Amdocs and Getty Images are among the pioneers building custom models using the service.\n“Enterprises need custom models to perform specialized skills trained on the proprietary DNA of their company — their data,” said Jensen Huang, founder and CEO of NVIDIA. “NVIDIA’s AI foundry service combines our generative AI model technologies, LLM training expertise and giant-scale AI factory. We built this in Microsoft Azure so enterprises worldwide can connect their custom model with Microsoft’s world-leading cloud services.”\n“Our partnership with NVIDIA spans every layer of the Copilot stack — from silicon to software — as we innovate together for this new age of AI,” said Satya Nadella, chairman and CEO of Microsoft. “With NVIDIA’s generative AI foundry service on Microsoft Azure, we’re providing new capabilities for enterprises and startups to build and deploy AI applications on our cloud.”\nIndustry Leaders Building Tailored, Timely LLMs\nNVIDIA’s AI foundry service can be used to customize models for generative AI-powered applications across industries, including enterprise software, telecommunications and media. Once ready to deploy, enterprises can use a technique called retrieval-augmented generation (RAG) to connect their models with their enterprise data and access new insights.\nAs the first customer of NVIDIA DGX Cloud on Microsoft Azure, SAP plans to use the service and optimized RAG workflow with NVIDIA DGX Cloud and NVIDIA AI Enterprise software running on Azure to help customize and deploy Joule®, its new natural language generative AI copilot.\n“Joule draws on SAP’s unique position at the nexus of business and technology, and builds on our relevant, reliable and responsible approach to Business AI,” said Christian Klein, CEO and member of the Executive Board of SAP SE. “In partnership with NVIDIA, Joule can help customers unlock the potential of generative AI for their business by automating time-consuming tasks and quickly analyzing data to deliver more intelligent, personalized experiences.”\nAmdocs, a leading provider of software and services to communications and media companies, is optimizing models for the Amdocs amAIz framework to speed adoption of generative AI applications and services for telcos globally.\n“Generative AI technology presents an incredible opportunity for service providers to reinvent the way they engage with customers,” said Shuky Sheffer, president and CEO at Amdocs. “Leveraging NVIDIA’s and Microsoft's technology to power the Amdocs amAlz framework will bring new GenAI-powered applications to customers faster and enable them to benefit from the immense potential of generative AI, while also providing enterprise-grade security, reliability and performance.”\nCurated, Optimized Models for Custom Generative AI\nCustomers using the NVIDIA foundry service can choose from several NVIDIA AI Foundation models, including a new family of NVIDIA Nemotron-3 8B models hosted in the Azure AI model catalog. Developers can also access the Nemotron-3 8B models on the NVIDIA NGC™ catalog, as well as community models such as Meta’s Llama 2 models optimized for NVIDIA for accelerated computing, which are also coming soon to the Azure AI model catalog.\nOptimized with 8 billion parameters, the Nemotron-3 8B family includes versions tuned for different use cases and have multilingual capabilities for building custom enterprise generative AI applications.\nNVIDIA DGX Cloud Now Available on Microsoft Azure Marketplace\nNVIDIA DGX Cloud AI supercomputing is available today on Azure Marketplace. It features instances customers can rent, scaling to thousands of NVIDIA Tensor Core GPUs, and comes with NVIDIA AI Enterprise software, including NeMo, to speed LLM customization.\nThe addition of DGX Cloud on the Azure Marketplace enables Azure customers to use their existing Microsoft Azure Consumption Commitment credits to speed model development with NVIDIA AI supercomputing and software.\nNVIDIA AI Enterprise software is now integrated into Azure Machine Learning, adding NVIDIA’s platform of secure, stable and supported AI and data science software. This brings NeMo and NVIDIA Triton Inference Server™ to Azure’s enterprise-grade AI service.\nNVIDIA AI Enterprise is also available on Azure Marketplace, providing businesses worldwide with broad options for production-ready AI development and deployment of custom generative AI applications.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Instant Latte: NVIDIA Gen AI Research Brews 3D Shapes in Under a Second",
    "link": "https://blogs.nvidia.com/blog/latte-3d-generative-ai-research/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNWpPRmh6U1VOWmNVWTRVM1pEVFJDakFSaTBBaWdCTWdrQmdaQ2p0aVl0ekFF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-21T07:00:00.000Z",
    "time": "Mar 21",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA BioNeMo Expands Computer-Aided Drug Discovery With New Foundation Models",
    "link": "https://blogs.nvidia.com/blog/bionemo-ai-drug-discovery-foundation-models-microservices/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNTJOa2htUzNsNFdtRjFORzVRVFJDakFSaTBBaWdCTWdrQkFJSkxES1dvc2dB=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Computer Industry Joins NVIDIA to Build AI Factories and Data Centers for the Next Industrial Revolution",
    "link": "https://nvidianews.nvidia.com/news/computer-industry-ai-factories-data-centers",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUtjRk16TmpGYVQzTTRaREU1VFJDb0FSaXNBaWdCTWdZSllvNUhOZ1k=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-02T07:00:00.000Z",
    "time": "Jun 2",
    "articleType": "regular",
    "content": "Top Computer Manufacturers Unveil Array of Blackwell-Powered Systems Featuring Grace CPUs, NVIDIA Networking and Infrastructure\nBroad Portfolios Encompass Cloud, On-Premises, Embedded and Edge AI Systems\nOfferings Range From Single to Multi-GPUs, x86 to Grace, Air to Liquid Cooling\nCOMPUTEX—NVIDIA and the world’s top computer manufacturers today unveiled an array of NVIDIA Blackwell architecture-powered systems featuring Grace CPUs, NVIDIA networking and infrastructure for enterprises to build AI factories and data centers to drive the next wave of generative AI breakthroughs.\nDuring his COMPUTEX keynote, NVIDIA founder and CEO Jensen Huang announced that ASRock Rack, ASUS, GIGABYTE, Ingrasys, Inventec, Pegatron, QCT, Supermicro, Wistron and Wiwynn will deliver cloud, on-premises, embedded and edge AI systems using NVIDIA GPUs and networking.\n“The next industrial revolution has begun. Companies and countries are partnering with NVIDIA to shift the trillion-dollar traditional data centers to accelerated computing and build a new type of data center — AI factories — to produce a new commodity: artificial intelligence,” said Huang. “From server, networking and infrastructure manufacturers to software developers, the whole industry is gearing up for Blackwell to accelerate AI-powered innovation for every field.”\nTo address applications of all types, the offerings will range from single to multi-GPUs, x86- to Grace-based processors, and air- to liquid-cooling technology.\nAdditionally, to speed up the development of systems of different sizes and configurations, the NVIDIA MGX™ modular reference design platform now supports NVIDIA Blackwell products. This includes the new NVIDIA GB200 NVL2 platform, built to deliver unparalleled performance for mainstream large language model inference, retrieval-augmented generation and data processing.\nGB200 NVL2 is ideally suited for emerging market opportunities such as data analytics, on which companies spend tens of billions of dollars annually. Taking advantage of high-bandwidth memory performance provided by NVLink®-C2C interconnects and dedicated decompression engines in the Blackwell architecture speeds up data processing by up to 18x, with 8x better energy efficiency compared to using x86 CPUs.\nModular Reference Architecture for Accelerated Computing\nTo meet the diverse accelerated computing needs of the world’s data centers, NVIDIA MGX provides computer manufacturers with a reference architecture to quickly and cost-effectively build more than 100 system design configurations.\nManufacturers start with a basic system architecture for their server chassis, and then select their GPU, DPU and CPU to address different workloads. To date, more than 90 systems from over 25 partners have been released or are in development that leverage the MGX reference architecture, up from 14 systems from six partners last year. Using MGX can help slash development costs by up to three-quarters and reduce development time by two-thirds, to just six months.\nAMD and Intel are supporting the MGX architecture with plans to deliver, for the first time, their own CPU host processor module designs. This includes the next-generation AMD Turin platform and the Intel® Xeon® 6 processor with P-cores (formerly codenamed Granite Rapids). Any server system builder can use these reference designs to save development time while ensuring consistency in design and performance.\nNVIDIA’s latest platform, the GB200 NVL2, also leverages MGX and Blackwell. Its scale-out, single-node design enables a wide variety of system configurations and networking options to seamlessly integrate accelerated computing into existing data center infrastructure.\nThe GB200 NVL2 joins the Blackwell product lineup, which also includes NVIDIA Blackwell Tensor Core GPUs, GB200 Grace Blackwell Superchips and the GB200 NVL72.\nNVIDIA’s comprehensive partner ecosystem includes TSMC, the world’s leading semiconductor manufacturer and an NVIDIA foundry partner, as well as global electronics makers, which provide key components to create AI factories. These include manufacturing innovations such as server racks, power delivery, cooling solutions and more from companies such as Amphenol, Asia Vital Components (AVC), Cooler Master, Colder Products Company (CPC), Danfoss, Delta Electronics and LITEON.\nAs a result, new data center infrastructure can quickly be developed and deployed to meet the needs of the world’s enterprises — and further accelerated by Blackwell technology, NVIDIA Quantum-2 or Quantum-X800 InfiniBand networking, NVIDIA Spectrum™-X Ethernet networking and NVIDIA BlueField®-3 DPUs — in servers from leading systems makers Dell Technologies, Hewlett Packard Enterprise and Lenovo.\nEnterprises can also access the NVIDIA AI Enterprise software platform, which includes NVIDIA NIM™ inference microservices, to create and run production-grade generative AI applications.\nHuang also announced during his keynote that Taiwan's leading companies are rapidly adopting Blackwell to bring the power of AI to their own businesses.\nTaiwan’s leading medical center, Chang Gung Memorial Hospital, plans to use the NVIDIA Blackwell computing platform to advance biomedical research and accelerate imaging and language applications to improve clinical workflows, ultimately enhancing patient care.\nFoxconn, one of the world’s largest makers of electronics, is planning to use NVIDIA Grace Blackwell to develop smart solution platforms for AI-powered electric vehicle and robotics platforms, as well as a growing number of language-based generative AI services to provide more personalized experiences to its customers.\nR. Adam Norwitt, president and CEO at Amphenol: “NVIDIA’s groundbreaking AI systems require advanced interconnect solutions, and Amphenol is proud to be supplying critical components. As an important partner in NVIDIA’s rich ecosystem, we are able to provide highly complex and efficient interconnect products for Blackwell accelerators to help deliver cutting-edge performance.”\nSpencer Shen, chairman and CEO at AVC: “AVC plays a key role in NVIDIA products, providing efficient cooling for its AI hardware, including the latest Grace Blackwell Superchip. As AI models and workloads continue to grow, reliable thermal management is important to handle intensive AI computing — and we’re with NVIDIA every step of the way.”\nJonney Shih, chairman at ASUS: “ASUS is working with NVIDIA to take enterprise AI to new heights with our powerful server lineup, which we’ll be showcasing at COMPUTEX. Using NVIDIA’s MGX and Blackwell platforms, we’re able to craft tailored data center solutions built to handle customer workloads across training, inference, data analytics and HPC.”\nJanel Wittmayer, president of Dover Corporation’s CPC: “CPC’s innovative, purpose-built connector technology enables the easy and reliable connection of liquid-cooled NVIDIA GPUs in AI systems. With a shared vision of performance and quality, CPC has the capacity and expertise to supply critical technological components to support NVIDIA’s incredible growth and progress. Our connectors are central to maintaining the integrity of temperature-sensitive products, which is important when AI systems are running compute-intensive tasks. We are excited to be part of the NVIDIA ecosystem and bring our technology to new applications.”\nAndy Lin, CEO at Cooler Master: “As the demand for accelerated computing continues to soar, so does demand for solutions that effectively meet energy standards for enterprises leveraging cutting-edge accelerators. As a pioneer in thermal management solutions, Cooler Master is helping unlock the full potential of the NVIDIA Blackwell platform, which will deliver incredible performance to customers.”\nKim Fausing, CEO at Danfoss: “Danfoss’ focus on innovative, high-performance quick disconnect and fluid power designs makes our couplings valuable for enabling efficient, reliable and safe operation in data centers. As a vital part of NVIDIA’s AI ecosystem, our work together enables data centers to meet surging AI demands while minimizing environmental impact.”\nPing Cheng, chairman and CEO at Delta Electronics: “The ubiquitous demand for computing power has ignited a new era of accelerated performance capabilities. Through our advanced cooling and power systems, Delta has developed innovative solutions capable of enabling NVIDIA’s Blackwell platform to operate at peak performance levels, while maintaining energy and thermal efficiency.”\nEtay Lee, vice president and general manager at GIGABYTE: “With our collaboration spanning nearly three decades, GIGABYTE has a deep commitment to supporting NVIDIA technologies across GPUs, CPUs, DPUs and high-speed networking. For enterprises to achieve even greater performance and energy efficiency for the compute-intensive workloads, we’re bringing to market a broad range of Blackwell-based systems.”\nYoung Liu, chairman and CEO at Hon Hai Technology Group: “As generative AI transforms industries, Foxconn stands ready with cutting-edge solutions to meet the most diverse and demanding computing needs. Not only do we use the latest Blackwell platform in our own servers, but we also help provide the key components to NVIDIA, giving our customers faster time-to-market.”\nJack Tsai, president at Inventec: “For nearly half a century, Inventec has been designing and manufacturing electronic products and components — the lifeblood of our business. Through our NVIDIA MGX rack-based solution powered by the NVIDIA Grace Blackwell Superchip, we’re helping customers enter a new realm of AI capability and performance.”\nAnson Chiu, president at LITEON Technology: “In pursuit of greener and more sustainable data centers, power management and cooling solutions are taking center stage. With the launch of the NVIDIA Blackwell platform, LITEON is releasing multiple liquid-cooling solutions that enable NVIDIA partners to unlock the future of highly efficient, environmentally friendly data centers.”\nBarry Lam, chairman at Quanta Computer: “We stand at the center of an AI-driven world, where innovation is accelerating like never before. NVIDIA Blackwell is not just an engine; it is the spark igniting this industrial revolution. When defining the next era of generative AI, Quanta proudly joins NVIDIA on this amazing journey. Together, we will shape and define a new chapter of AI.”\nCharles Liang, president and CEO at Supermicro: “Our building-block architecture and rack-scale, liquid-cooling solutions, combined with our in-house engineering and global production capacity of 5,000 racks per month, enable us to quickly deliver a wide range of game-changing NVIDIA AI platform-based products to AI factories worldwide. Our liquid-cooled or air-cooled high-performance systems with rack-scale design, optimized for all products based on the NVIDIA Blackwell architecture, will give customers an incredible choice of platforms to meet their needs for next-level computing, as well as a major leap into the future of AI.”\nC.C. Wei, CEO at TSMC: “TSMC works closely with NVIDIA to push the limits of semiconductor innovation that enables them to realize their visions for AI. Our industry-leading semiconductor manufacturing technologies helped shape NVIDIA’s groundbreaking GPUs, including those based on the Blackwell architecture.”\nJeff Lin, CEO at Wistron: “As a key manufacturing partner, Wistron has been on an incredible journey alongside NVIDIA delivering GPU computing technologies and AI cloud solutions to customers. Now we’re working with NVIDIA's latest GPU architectures and reference designs, such as Blackwell and MGX, to quickly bring tremendous new AI computing products to market.”\nWilliam Lin, president at Wiwynn: “Wiwynn is focused on helping customers address the rising demand for massive computing power and advanced cooling solutions in the era of generative AI. With our latest lineup based on the NVIDIA Grace Blackwell and MGX platforms, we’re building optimized, rack-level, liquid-cooled AI servers tailored specifically for the demanding workloads of hyperscale cloud providers and enterprises.”",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA and Alphabet’s Intrinsic Put Next-Gen Robotics Within Grasp",
    "link": "https://blogs.nvidia.com/blog/alphabet-intrinsic-robotics-isaac-manipulator/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNTRUelJGY214dVEyUk1ibk5QVFJDakFSaTBBaWdCTWdrQmdJcGdxMmRGTXdF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-05-06T07:00:00.000Z",
    "time": "May 6",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "For Your Edification: Shutterstock Releases Generative 3D, Getty Images Upgrades Service Powered by NVIDIA",
    "link": "https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUpiWGhwV2t0NWVVMXlSR1pEVFJDa0FSaXpBaWdCTWdrQkFJTEluT1pFc3dB=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-29T20:32:46.000Z",
    "time": "3 days ago",
    "articleType": "regular",
    "content": "Designers and artists have new and improved ways to boost their productivity with generative AI trained on licensed data.\nShutterstock, a leading platform for creative content, launched its Generative 3D service in commercial beta. It lets creators quickly prototype 3D assets and generate 360 HDRi backgrounds that light scenes, using just text or image prompts.\nGetty Images, a premier visual content creator and marketplace, turbocharged its Generative AI by Getty Images service so it creates images twice as fast, improves output quality, brings advanced controls and enables fine-tuning.\nThe services are built with NVIDIA’s visual AI foundry using NVIDIA Edify, a multimodal generative AI architecture. The AI models are then optimized and packaged for maximum performance with NVIDIA NIM, a set of accelerated microservices for AI inference.\nEdify enables service providers to train responsible generative models on their licensed data and scale them quickly with NVIDIA DGX Cloud, the cloud-first way to get the best of NVIDIA AI.\nGenerative AI Speeds 3D Modeling\nAvailable now for enterprises in commercial beta, Shutterstock’s service lets designers and artists quickly create 3D objects that help them prototype or populate virtual environments. For example, tapping generative AI, they can quickly create the silverware and plates on a dining room table so they can focus on designing the characters around it.\nThe 3D assets the service generates are ready to edit using digital content creation tools, and available in a variety of popular file formats. Their clean geometry and layout gives artists an advanced starting point for adding their own flair.\nAn example of a 3D mesh from Shutterstock Generative 3D.\nThe AI model first delivers a preview of a single asset in as little as 10 seconds. If users like it, the preview can be turned into a higher-quality 3D asset, complete with physically based rendering materials like concrete, wood or leather.\nAt this year’s SIGGRAPH computer graphics conference, designers will see just how fast they can make their ideas come to life.\nShutterstock will demo a workflow in Blender that lets artists generate objects directly within their 3D environment. In the Shutterstock booth at SIGGRAPH, HP will show 3D prints and physical prototypes of the kinds of assets attendees can design on the show floor using Generative 3D.\nShutterstock is also working with global marketing and communications services company WPP to bring ideas to life with Edify 3D generation for virtual production (see video below).\nExplore Generative 3D by Shutterstock on the company’s website, or test-drive the application programming interface (API) at build.nvidia.com/.\nLighting a virtual scene with accurate reflections can be a complicated task. Creatives need to operate expensive 360-degree camera rigs and go on set to create backgrounds from scratch, or search vast libraries for something that approximates what they want.\nWith Shutterstock’s Generative 3D service, users can now simply describe the exact environment they need in text or with an image, and out comes a high-dynamic-range panoramic image, aka 360 HDRi, in brilliant 16K resolution. (See video below.)\nWant that beautiful new sports car shown in a desert, a tropical beach or maybe on a winding mountain road? With generative AI, designers can shift gears fast.\nThree companies plan to integrate Shutterstock’s 360 HDRi APIs directly into their workflows — WPP, CGI studio Katana and Dassault Systèmes, developer of the 3DEXCITE applications for creating high-end visualizations and 3D content for virtual worlds.\nExamples from Generative AI by Getty Images.\nGreat Images Get a Custom Fit\nGenerative AI by Getty Images has upgraded to a more powerful Edify AI model with a portfolio of new features that let artists control image composition and style.\nWant a red beach ball floating above that perfect shot of a coral reef in Fiji? Getty Images’ service can get it done in a snap.\nThe new model is twice as fast, boosts image quality and prompt accuracy, and lets users control camera settings like the depth of field or focal length of a shot. Users can generate four images in about six seconds and scale them up to 4K resolution.\nAn example of the camera controls in Generative AI by Getty Images.\nIn addition, the commercially safe foundational model now serves as the basis for a fine-tuning capability that lets companies customize the AI with their own data. That lets them generate images tailored to the creative style of their specific brands.\nNew controls in the service support the use of a sketch or depth map to guide the composition or structure of an image.\nCreatives at Omnicom, a global leader in marketing and sales solutions, are using Getty Images’ service to streamline advertising workflows and safely create on-brand content. The collaboration with Getty Images is part of Omnicom’s strategy to infuse generative AI into every facet of its business, helping teams move from ideas to outcomes faster.\nGenerative AI by Getty Images is available through the Getty Images and iStock websites, and via an API.\nFor more about NVIDIA’s offerings, read about the AI foundry for visual generative AI built on NVIDIA DGX Cloud, and try it on ai.nvidia.com.\nTo get the big picture, listen to NVIDIA founder and CEO Jensen Huang in two fireside chats at SIGGRAPH.\nSee notice regarding software product information.",
    "favicon": ""
  },
  {
    "title": "Cisco and NVIDIA to Help Enterprises Quickly and Easily Deploy and Manage Secure AI Infrastructure",
    "link": "https://nvidianews.nvidia.com/news/cisco-nvidia-secure-ai-infrastructure",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNVFhV0YxUm1Gc1JEWndYMU0zVFJDb0FSaXNBaWdCTWdrUm9wWklQaWRGN2dF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-02-06T08:00:00.000Z",
    "time": "Feb 6",
    "articleType": "regular",
    "content": "Companies to offer enterprises simplified cloud-based and on-premises AI infrastructure, networking and software, including infrastructure management, secure AI infrastructure, observable end-to-end AI solutions and access to NVIDIA AI Enterprise software that supports the building and deployment of advanced AI and generative AI workloads.\nCisco and NVIDIA’s purpose-built Ethernet networking-based solutions will be sold through Cisco’s vast global channel, offering professional services and support through key partners who are committed to helping businesses deploy their GPU clusters via Ethernet infrastructure.\nThe collaboration has attracted key customers like ClusterPower, a cloud services provider in Europe, to help drive data center operations with innovative AI/ML solutions that are foundational for its client infrastructure and services.\nCISCO LIVE—Cisco and NVIDIA today announced plans to deliver AI infrastructure solutions for the data center that are easy to deploy and manage, enabling the massive computing power that enterprises need to succeed in the AI era.\n“AI is fundamentally changing how we work and live, and history has shown that a shift of this magnitude is going to require enterprises to rethink and re-architect their infrastructures,” said Chuck Robbins, Chair and CEO, Cisco. “Strengthening our great partnership with NVIDIA is going to arm enterprises with the technology and the expertise they need to build, deploy, manage, and secure AI solutions at scale.”\n“Companies everywhere are racing to transform their businesses with generative AI,” said Jensen Huang, founder and CEO of NVIDIA. “Working closely with Cisco, we’re making it easier than ever for enterprises to obtain the infrastructure they need to benefit from AI, the most powerful technology force of our lifetime.”\nCisco, with its industry-leading expertise in Ethernet networking and extensive partner ecosystem, together with NVIDIA, the inventor of the GPU that fueled the AI boom, share a vision and commitment to help customers navigate the transitions for AI with highly secure Ethernet-based infrastructure.\nCisco and NVIDIA have offered a broad range of integrated product solutions over the past several years across Webex collaboration devices and data center compute environments to enable hybrid workforces with flexible workspaces, AI-powered meetings and virtual desktop infrastructure. The companies are now deepening their partnership in the data center to enable enterprise customers with scalable and automated AI cluster management, automated troubleshooting, best-in-class customer experiences, and more. Highlights include:\nCisco and NVIDIA Integrated Data Center Solutions Available Today\nNVIDIA’s newest Tensor Core GPUs are available in Cisco’s M7 generation of UCS rack and blade servers, including Cisco UCS X-Series and UCS X-Series Direct, to enable optimal performance across a broad array of AI and data-intensive workloads in the data center and at the edge.\nNVIDIA AI Enterprise, which includes software frameworks, pretrained models and development tools for more secure, stable and supported production AI is now available on Cisco’s global price list.\nJointly validated reference architectures through Cisco Validated Designs (CVDs) make it simple to deploy and manage AI clusters at any scale in a wide array of use cases spanning virtualized and containerized environments, with both converged and hyperconverged options. CVDs for FlexPod and FlashStack for Generative AI Inferencing with NVIDIA AI Enterprise will be available this month, with more to follow.\nSupporting Cisco Networking Cloud: Cisco simplified AI infrastructure management and operations through both on-premises and cloud-based management with Cisco Nexus Dashboard and Cisco Intersight.\nDigital Experience Monitoring: With AI workloads and data in the public cloud, on premises and across multiple data centers, ThousandEyes provides Digital Experience Monitoring to provide AI-driven insights and automated remediation of problems that occur anywhere across the cloud to on-premises networks.\nThe Cisco Observability Platform uses AI capabilities to contextualize and correlate real-time telemetry across domains, so organizations can better attain the visibility, insights and actions to improve digital experiences.\nPartners Reducing Risks: As organizations plan to successfully adopt AI and automation, they will look to Cisco’s global ecosystem of partners to advise, support and guide them.\n“Building AI compute-based environments can be daunting task for organizations, and underscores the need for simple, adaptable data center infrastructure that delivers optimized performance,” said Vladimir Ester, chief technology officer and cofounder of ClusterPower. “Cisco and NVIDIA are bringing together the innovations to help enterprises support demand for more computing power in the data center and navigate the transitions for AI with secure and observable infrastructure.”\n“World Wide Technology (WWT) is excited to see NVIDIA and Cisco combining resources to bring proven Ethernet-based AI solutions to market,” said Neil Anderson, vice president of Cloud & Infrastructure Solutions at WWT. “Ethernet has the broadest scale in our client base, and this partnership and resulting offers will accelerate our ability to deliver AI solutions to our clients. Both Cisco and NVIDIA are already a significant part of WWT’s AI Proving Ground lab where we help clients select and operationalize AI architectures so they can more quickly turn their data into insights and action.”\n“As enterprises look to transform their businesses with AI, they must understand the unique demands that AI workloads place on data center infrastructure,” said Vijay Bhagavath, vice president of Cloud & Data Center Networks at IDC Research. “The Cisco and NVIDIA partnership brings together two trusted brands with complementary technologies to enable customers to realize the full potential of AI with a wide range of performance-optimized Ethernet-based infrastructure.”\nAvailability: 2Q Calendar Year; Solutions sold through Cisco Channel partners",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Why GPUs Are Great for AI",
    "link": "https://blogs.nvidia.com/blog/why-gpus-are-great-for-ai/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWhTWGd5VEVaUWVYTXljbDlQVFJDa0FSaTBBaWdCTWdhbEJJYXRrUW8=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-12-04T08:00:00.000Z",
    "time": "Dec 4, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "NVIDIA Announces Omniverse Microservices to Supercharge Physical AI",
    "link": "https://nvidianews.nvidia.com/news/omniverse-microservices-physical-ai",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUZOa3RMWm1OUmJXTlRSWFF5VFJDb0FSaXNBaWdCTWdrQkVJeUZPcVVscUFJ=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-17T07:00:00.000Z",
    "time": "Jun 17",
    "articleType": "regular",
    "content": "NVIDIA Omniverse Cloud Sensor RTX Generates Synthetic Data to Speed AI Development of Autonomous Vehicles, Robotic Arms, Mobile Robots, Humanoids and Smart Spaces\nCVPR—NVIDIA today announced NVIDIA Omniverse Cloud Sensor RTX™, a set of microservices that enable physically accurate sensor simulation to accelerate the development of fully autonomous machines of every kind.\nSensors, which comprise a growing, multibillion-dollar industry, provide autonomous vehicles, humanoids, industrial manipulators, mobile robots and smart spaces with the data needed to comprehend the physical world and make informed decisions. With NVIDIA Omniverse Cloud Sensor RTX, developers can test sensor perception and associated AI software at scale in physically accurate, realistic virtual environments before real-world deployment — enhancing safety while saving time and costs.\n“Developing safe and reliable autonomous machines powered by generative physical AI requires training and testing in physically based virtual worlds,” said Rev Lebaredian, vice president of Omniverse and simulation technology at NVIDIA. “NVIDIA Omniverse Cloud Sensor RTX microservices will enable developers to easily build large-scale digital twins of factories, cities and even Earth — helping accelerate the next wave of AI.”\nBuilt on the OpenUSD framework and powered by NVIDIA RTX™ ray-tracing and neural-rendering technologies, Omniverse Cloud Sensor RTX accelerates the creation of simulated environments by combining real-world data from videos, cameras, radar and lidar with synthetic data.\nEven for scenarios with limited real-world data, the microservices can be used to simulate a broad range of activities, such as whether a robotic arm is operating correctly, an airport luggage carousel is functional, a tree branch is blocking a roadway, a factory conveyor belt is in motion, or a robot or person is nearby.\nResearch Wins Drive Real-World Deployment\nThe Omniverse Cloud Sensor RTX announcement comes at the same time as NVIDIA’s first-place win at the Computer Vision and Pattern Recognition conference’s Autonomous Grand Challenge for End-to-End Driving at Scale.\nNVIDIA researchers’ winning workflow can be replicated in high-fidelity simulated environments with Omniverse Cloud Sensor RTX — giving autonomous vehicle (AV) simulation developers the ability to test self-driving scenarios in physically accurate environments before deploying AVs in the real world.\nForetellix and MathWorks are among the first software developers to which NVIDIA is providing Omniverse Cloud Sensor RTX access for AV development.\nOmniverse Cloud Sensor RTX will also enable sensor manufacturers to validate and integrate digital twins of their sensors in virtual environments, reducing the time needed for physical prototyping.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Oracle and NVIDIA to Deliver Sovereign AI Worldwide",
    "link": "https://nvidianews.nvidia.com/news/oracle-nvidia-sovereign-ai",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVJTMjQzU2tWRlJVaHhSR3hMVFJDb0FSaXNBaWdCTWdZUm9aYlZ1QWM=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "Oracle and NVIDIA collaborate to deliver accelerated computing and generative AI services that establish digital sovereignty and manage proprietary national and personal data\nOracle adopts NVIDIA Grace Blackwell across OCI Supercluster, OCI Compute, and NVIDIA DGX Cloud on OCI\nGTC—Oracle and NVIDIA today announced an expanded collaboration to deliver sovereign AI solutions to customers around the world. Oracle’s distributed cloud, AI infrastructure, and generative AI services, combined with NVIDIA’s accelerated computing and generative AI software, are enabling governments and enterprises to deploy AI factories.\nThese AI factories can run cloud services locally, and within a country’s or organization’s secure premises with a range of operational controls, supporting sovereign goals of diversifying and boosting economic growth.\n“As AI reshapes business, industry, and policy around the world, countries and organizations need to strengthen their digital sovereignty in order to protect their most valuable data,” said Safra Catz, CEO of Oracle. “Our continued collaboration with NVIDIA and our unique ability to deploy cloud regions quickly and locally will ensure societies can take advantage of AI without compromising their security.”\n“In an era where innovation will be driven by generative AI, data sovereignty is a cultural and economic imperative,” said Jensen Huang, founder and CEO of NVIDIA. “Oracle’s integrated cloud applications and infrastructure, combined with NVIDIA accelerated computing and generative AI services, create the flexibility and security nations and regions require to control their own destiny.”\nTurnkey Solutions to Help Customers Meet Data Sovereignty\nThe combination of NVIDIA’s full-stack AI platform with Oracle’s Enterprise AI – deployable across OCI Dedicated Region, Oracle Alloy, Oracle EU Sovereign Cloud, and Oracle Government Cloud – offers customers a state-of-the-art AI solution that provides greater control over operations, location, and security to help support digital sovereignty.\nCountries across the globe are increasingly investing in AI infrastructure that can support their cultural and economic ambitions. Across 66 cloud regions in 26 countries, customers can access more than 100 cloud and AI services spanning infrastructure and applications to support IT migration, modernization, and innovation.\nThe companies’ combined offerings can be deployed via the public cloud or in a customer’s data center in specific locations, with flexible operational controls. Oracle is the only hyperscaler capable of delivering AI and full cloud services locally, anywhere. OCI services and pricing are consistent across deployment types to simplify planning, portability, and management.\nOracle’s cloud services leverage a range of NVIDIA’s stack, including NVIDIA accelerated computing infrastructure and the NVIDIA AI Enterprise software platform, including newly announced NVIDIA NIM™ inference microservices, which are built on the foundation of NVIDIA inference software such as NVIDIA TensorRT™, NVIDIA TensorRT-LLM, and NVIDIA Triton Inference Server™.\nAvaloq, a leader in wealth management technology, selected OCI Dedicated Region to bring a complete OCI cloud region into its own data center.\n“OCI Dedicated Region aligns with our commitment to ensure maximum control over data residency while providing access to the latest cloud infrastructure,” said Martin Büchi, chief technology officer at Avaloq. “This supports us as we continue to drive the digital transformation of banks and wealth managers.”\nTEAM IM, a leading New Zealand information management services provider, chose Oracle Alloy to build New Zealand’s first locally owned and operated hyperscale cloud known as TEAM Cloud.\n“Organizations in New Zealand are increasingly eager to harness the power of the cloud while safeguarding the integrity of their data within their own shores by leveraging a unique hyperscale cloud solution,” said Ian Rogers, chief executive officer of TEAM IM. “With Oracle Alloy and the possibility of integrating the NVIDIA AI platform into our cloud services, we’ve been able to become a cloud services provider that can assist public sector, commercial and iwi organizations in navigating the intricacies of the digital landscape and optimizing their digital transformations.”\ne& UAE, telecom arm of e& group, is collaborating with Oracle to enhance its AI capabilities and intends to deploy NVIDIA H100 Tensor Core GPU clusters within its OCI Dedicated Region.\n“OCI will enable us to deploy NVIDIA H100 GPU clusters within our own OCI Dedicated Region, hosted at e& UAE data centers,” said Khalid Murshed, chief technology and information officer (CTIO) of e& UAE. “This type of localization will allow us to accelerate AI innovation across the UAE and helps us develop new Gen AI applications and use cases at scale. This is in line with e& UAE’s transformation efforts to pioneer innovation and shape the future of technology with our focus on driving excellence in AI to provide unparalleled customer experiences.”\nOCI Supercluster and OCI Compute Boosted with NVIDIA Grace Blackwell\nTo help customers address the ever-increasing needs of AI models, Oracle plans to take advantage of the latest NVIDIA Grace Blackwell computing platform, announced today at GTC, across OCI Supercluster and OCI Compute. OCI Supercluster will become significantly faster with new OCI Compute bare metal instances, ultra-low-latency RDMA networking, and high-performance storage. OCI Compute will adopt both the NVIDIA GB200 Grace Blackwell Superchip and the NVIDIA Blackwell B200 Tensor Core GPU.\nThe NVIDIA GB200 Grace™ Blackwell Superchip will power a new era of computing. GB200 delivers up to 30X faster real-time large language model (LLM) inference, 25X lower TCO, and requires 25X less energy compared to the previous generation of GPUs, supercharging AI training, data processing, and engineering design and simulation. NVIDIA Blackwell B200 Tensor Core GPUs are designed for the most demanding AI, data analytics, and high-performance computing (HPC) workloads.\nNVIDIA NIM and CUDA-X™ microservices, including NVIDIA NeMo Retriever for retrieval- augmented generation (RAG) inference deployments, will also help OCI customers bring more insight and accuracy to their generative AI copilots and other productivity tools using their own data.\nNVIDIA Grace Blackwell Comes to DGX Cloud on OCI\nTo meet escalating customer demand for increasingly complex AI models, the companies are adding NVIDIA Grace Blackwell to NVIDIA DGX™ Cloud on OCI. Customers will be able to access new GB200 NVL72-based instances through this co-engineered supercomputing service designed for energy-efficient training and inference in an era of trillion-parameter LLMs.\nThe full DGX Cloud cluster buildout will include more than 20,000 GB200 accelerators and NVIDIA CX8 InfiniBand networking, providing a highly scalable and performant cloud infrastructure. The cluster will consist of 72 Blackwell GPUs NVL72 and 36 Grace CPUs with fifth-generation NVLink™.\nRead more about AI innovators running on OCI",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Light Bulb Moment: NVIDIA CEO Sees Bright Future for AI-Powered Electric Grid",
    "link": "https://blogs.nvidia.com/blog/ai-electric-grid-edison/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVpWbk5XVm1aRE5sVlJabFZ6VFJDa0FSaTBBaWdCTWdZZFlZenROUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-18T07:00:00.000Z",
    "time": "Jun 18",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "GTC 2024 News",
    "link": "https://nvidianews.nvidia.com/online-press-kit/gtc-2024-news",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXZWV2xZVkhjNVUzQXhaV1pCVFJDb0FSaXNBaWdCTWdZbFJJN1BLUWc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T21:01:43.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "TSMC and Synopsys Bring Breakthrough NVIDIA Computational Lithography Platform to Production\nNVIDIA Blackwell Platform Arrives to Power a New Era of Computing\nNVIDIA Launches Blackwell-Powered DGX SuperPOD for Generative AI Supercomputing at Trillion-Parameter Scale\nNVIDIA Announces New Switches Optimized for Trillion-Parameter GPU Computing and AI Infrastructure\nAWS and NVIDIA Extend Collaboration to Advance Generative AI Innovation\nMicrosoft and NVIDIA Announce Major Integrations to Accelerate Generative AI for Enterprises Everywhere\nNVIDIA Announces Earth Climate Digital Twin\nNVIDIA Healthcare Launches Generative AI Microservices to Advance Drug Discovery, MedTech and Digital Health\nNVIDIA Launches Generative AI Microservices for Developers to Create and Deploy Generative AI Copilots Across NVIDIA CUDA GPU Installed Base\nNVIDIA Announces Omniverse Cloud APIs to Power Wave of Industrial Digital Twin Software Tools\nNVIDIA DRIVE Powers Next Generation of Transportation — From Cars and Trucks to Robotaxis and Autonomous Delivery Vehicles\nOracle and NVIDIA to Deliver Sovereign AI Worldwide\nGoogle Cloud and NVIDIA Expand Partnership to Scale AI Development\nSAP and NVIDIA to Accelerate Generative AI Adoption Across Enterprise Applications Powering Global Industries\nNVIDIA Announces Project GR00T Foundation Model for Humanoid Robots and Major Isaac Robotics Platform Update\nNVIDIA Unveils 6G Research Cloud Platform to Advance Wireless Communications With AI\nNVIDIA Powers Japan’s ABCI-Q Supercomputer for Quantum Research\nNVIDIA Launches Cloud Quantum-Computer Simulation Microservices\nNVIDIA BioNeMo Expands Computer-Aided Drug Discovery With New Foundation Models\nSafe and Found: NVIDIA Generative AI Microservices Help Enterprises Detect and Address Software Security Issues in Seconds\nAt Your Microservice: NVIDIA Smooths Businesses’ Journey to Generative AI\nNVIDIA and Siemens Bring Immersive Visualization and Generative AI to Industrial Design and Manufacturing\nNVIDIA Omniverse Expands Worlds Using Apple Vision Pro\nNVIDIA Supercharges Autonomous System Development with Omniverse Cloud APIs\nStaying in Sync: NVIDIA Combines Digital Twins With Real-Time AI for Industrial Automation\nNVIDIA Isaac Taps Generative AI for Manufacturing and Logistics Applications\nJohnson & Johnson MedTech Works With NVIDIA to Broaden AI’s Reach in Surgery\nNVIDIA Edify Unlocks 3D Generative AI, New Image Controls for Visual Content Providers\nNVIDIA Maxine Developer Platform to Transform $10 Billion Video Conferencing Industry\nFrom Atoms to Supercomputers: NVIDIA, Partners Scale Quantum Computing\nNew NVIDIA Storage Partner Validation Program Streamlines Enterprise AI Deployments\nNVIDIA Unveils Digital Blueprint for Building Next-Gen Data Centers\nGenerative AI Developers Harness NVIDIA Technologies to Transform In-Vehicle Experiences\nAll Eyes on AI: Automotive Tech on Full Display at GTC 2024\nAll Aboard: NVIDIA Scores 23 World Records for Route Optimization\nBNY Mellon, First Global Bank to Deploy AI Supercomputer Powered by NVIDIA DGX SuperPOD With DGX H100\nMake It So: Software Speeds Journey to Post-Quantum Cryptography\nGeneration Sensation: New Generative AI and RTX Tools Boost Content Creation\nNVIDIA Celebrates Americas Partners Driving AI-Powered Transformation\nClimate Pioneers: 3 Startups Harnessing NVIDIA’s AI and Earth-2 Platforms\nSecure by Design: NVIDIA AIOps Partner Ecosystem Blends AI for Businesses\nAI Decoded From GTC: The Latest Developer Tools and Apps Accelerating AI on PC and Workstation\n‘You Transformed the World,’ NVIDIA CEO Tells Researchers Behind Landmark AI Paper\nInstant Latte: NVIDIA Gen AI Research Brews 3D Shapes in Under a Second\nKeynote Deck and Show Highlights",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA Omniverse Opens Portals to Vast Worlds of OpenUSD",
    "link": "https://nvidianews.nvidia.com/news/nvidia-omniverse-opens-portals-to-vast-worlds-of-openusd",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXhhbFl0UjFsS2FYZzRObEZuVFJDb0FSaXNBaWdCTWdZRmtaS01zUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "New Omniverse Cloud APIs Help Developers Adopt OpenUSD; Generative AI Model ChatUSD LLM Converses in USD; RunUSD Translates USD to Interactive Graphics, DeepSearch LLM Enables Semantic 3D Search\nSIGGRAPH—NVIDIA today announced a broad range of frameworks, resources and services for developers and companies to accelerate the adoption of Universal Scene Description, known as OpenUSD.\nNVIDIA is advancing the development of OpenUSD — a 3D framework enabling interoperability between software tools and data types for the building of virtual worlds — through NVIDIA Omniverse™ and a new portfolio of technologies and cloud application programming interfaces (APIs) — including ChatUSD and RunUSD — along with a new NVIDIA OpenUSD Developer Program.\nThese investments in OpenUSD expand on NVIDIA’s co-founding of the Alliance for OpenUSD (AOUSD) — an organization announced last week that will standardize OpenUSD specifications — along with Pixar, Adobe, Apple and Autodesk.\n“Just as HTML ignited a major computing revolution of the 2D internet, OpenUSD will spark the era of collaborative 3D and industrial digitalization,” said Jensen Huang, founder and CEO of NVIDIA. “NVIDIA is putting our full force behind the advancement and adoption of OpenUSD through our development of NVIDIA Omniverse and generative AI.”\nOpenUSD Goes to the Cloud\nNVIDIA announced four new Omniverse Cloud APIs built by NVIDIA for developers to more seamlessly implement and deploy OpenUSD pipelines and applications.\nChatUSD — a large language model (LLM) copilot for developers that can answer USD knowledge questions or generate Python-USD code scripts. ChatUSD is fine-tuned using USD functions and Python-USD code snippets from NVIDIA.\nRunUSD — a cloud API that translates OpenUSD files into fully path-traced rendered images by checking compatibility of the uploaded files against versions of OpenUSD releases, and generating renders with Omniverse Cloud. A demo of the API is currently available for developers in the NVIDIA OpenUSD Developer Program.\nDeepSearch — an LLM agent enabling fast semantic search through massive databases of untagged assets.\nUSD-GDN Publisher — a one-click service that enables enterprises and software makers to publish high-fidelity, OpenUSD-based experiences to the Omniverse Cloud Graphics Delivery Network (GDN) from an Omniverse-based application such as USD Composer, as well as stream in real time to web browsers and mobile devices.\nOpenUSD was invented to better connect film and animation pipelines. Industrial applications — such as building interoperable manufacturing design pipelines, creating physically accurate real-time digital twins of factories, or training and validating autonomous vehicles — require different demands of the 3D framework.\nTo enable these highly complex industrial and perception AI workloads, NVIDIA is developing NVIDIA Omniverse, the OpenUSD-native software platform for developing applications, as well as technologies that include geospatial data models, metrics assembly and simulation-ready, or SimReady, specifications for OpenUSD.\nGeospatial data models for OpenUSD let users develop simulations and calculations for true-to-reality digital twins of factories, warehouses, cities and even Earth. For extreme-scale projects, it accounts for the planet’s curvature to ensure the simulations are physically accurate.\nIndustrial applications combine datasets from many tools and sources, each represented in different units. NVIDIA is developing a metrics assembly for OpenUSD that enables users to combine diverse datasets with complete accuracy.\nNVIDIA is also developing a structure for new SimReady 3D models. These will include true-to-reality material and physical properties, which are critical to accurately training autonomous robots and vehicles. For example, an autonomous robot tasked with sorting packages needs to be trained in simulation on 3D packages that move and react to physical contact just as they would in the real world.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Up to No Good: ‘No Rest for the Wicked’ Early Access Launches on GeForce NOW",
    "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-no-rest-for-the-wicked/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNWFhVVp2TkZoYVdrRTBValpJVFJDb0FSaXNBaWdCTWdtQkVZQW0ycUZFeWdF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-04-18T07:00:00.000Z",
    "time": "Apr 18",
    "articleType": "regular",
    "content": "It’s time to get a little wicked. Members can now stream No Rest for the Wicked from the cloud.\nIt leads six new games joining the GeForce NOW library of more than 1,500 games.\nThere’s always another fight to be won.\nNo Rest for the Wicked is the highly anticipated action role-playing game from Moon Studios, developer of the Ori series, and publisher Private Division. Amid a plague-ridden world, step into the boots of a Cerim, a holy warrior on a desperate mission. The Great Pestilence has ravaged the land of Sacra, and a new king reigns. As a colonialist inquisition unfolds, engage in visceral combat, battle plague-infested creatures and uncover the secrets of the continent. Make the character you want with the game’s flexible soft-class system, explore a rich storyline, and prepare for intense boss battles as you build up the town of Sacrament.\nBecome a Wild West superhero in Evil West, streaming on GeForce NOW this week and part of PC Game Pass. It’s part of six newly supported games this week:\nKill It With Fire 2 (New release on Steam, April 16)\nThe Crew Motorfest (New release on Steam, April 18)\nNo Rest for the Wicked (New release on Steam, April 18)\nEvil West (Xbox, available on PC Game Pass)\nTomb Raider I-III Remastered (Steam)\nRiot Games shared in its 14.8 patch notes that it will soon add its Vanguard security software to League of Legends as part of the publisher’s commitment to remove scripters, bots and bot-leveled accounts from the game and make it more challenging for them to continue. Since Vanguard won’t support virtual machines when it’s added to League of Legends, the game will be put under maintenance and will no longer be playable on GeForce NOW once the 14.9 update goes live globally — currently planned for May 1, 2024. Members can continue to enjoy the game on GeForce NOW until then.\nWhat are you planning to play this weekend? Let us know on X or in the comments below.",
    "favicon": ""
  },
  {
    "title": "NVIDIA AI Foundry Builds Custom Llama 3.1 Generative AI Models for the World’s Enterprises",
    "link": "https://nvidianews.nvidia.com/news/nvidia-ai-foundry-custom-llama-generative-models",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWFhbGM0YlVSQ2QwZE5hbnBHVFJDakFSaTBBaWdCTWdhZGNvb1V4UVE=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-23T07:00:00.000Z",
    "time": "Jul 23",
    "articleType": "regular",
    "content": "Enterprises and Nations Can Now Build ‘Supermodels’ With NVIDIA AI Foundry Using Their Own Data Paired With Llama 3.1 405B and NVIDIA Nemotron Models\nNVIDIA AI Foundry Offers Comprehensive Generative AI Model Service Spanning Curation, Synthetic Data Generation, Fine-Tuning, Retrieval, Guardrails and Evaluation to Deploy Custom Llama 3.1 NVIDIA NIM Microservices With New NVIDIA NeMo Retriever Microservices for Accurate Responses\nAccenture First to Use New Service to Build Custom Llama 3.1 Models for Clients; Aramco, AT&T, Uber and Other Industry Leaders Among First to Access New Llama NVIDIA NIM Microservices\nNVIDIA today announced a new NVIDIA AI Foundry service and NVIDIA NIM™ inference microservices to supercharge generative AI for the world’s enterprises with the Llama 3.1 collection of openly available models, also introduced today.\nWith NVIDIA AI Foundry, enterprises and nations can now create custom “supermodels” for their domain-specific industry use cases using Llama 3.1 and NVIDIA software, computing and expertise. Enterprises can train these supermodels with proprietary data as well as synthetic data generated from Llama 3.1 405B and the NVIDIA Nemotron™ Reward model.\nNVIDIA AI Foundry is powered by the NVIDIA DGX™ Cloud AI platform, which is co-engineered with the world’s leading public clouds, to give enterprises significant compute resources that easily scale as AI demands change.\nThe new offerings come at a time when enterprises, as well as nations developing sovereign AI strategies, want to build custom large language models with domain-specific knowledge for generative AI applications that reflect their unique business or culture.\n“Meta’s openly available Llama 3.1 models mark a pivotal moment for the adoption of generative AI within the world’s enterprises,” said Jensen Huang, founder and CEO of NVIDIA. “Llama 3.1 opens the floodgates for every enterprise and industry to build state-of-the-art generative AI applications. NVIDIA AI Foundry has integrated Llama 3.1 throughout and is ready to help enterprises build and deploy custom Llama supermodels.”\n“The new Llama 3.1 models are a super-important step for open source AI,” said Mark Zuckerberg, founder and CEO of Meta. “With NVIDIA AI Foundry, companies can easily create and customize the state-of-the-art AI services people want and deploy them with NVIDIA NIM. I’m excited to get this in people’s hands.”\nTo supercharge enterprise deployments of Llama 3.1 models for production AI, NVIDIA NIM inference microservices for Llama 3.1 models are now available for download from ai.nvidia.com. NIM microservices are the fastest way to deploy Llama 3.1 models in production and power up to 2.5x higher throughput than running inference without NIM.\nEnterprises can pair Llama 3.1 NIM microservices with new NVIDIA NeMo Retriever NIM microservices to create state-of-the-art retrieval pipelines for AI copilots, assistants and digital human avatars.\nAccenture Pioneers Custom Llama Supermodels for Enterprises With AI Foundry\nGlobal professional services firm Accenture is first to adopt NVIDIA AI Foundry to build custom Llama 3.1 models using the Accenture AI Refinery™ framework, both for its own use as well as for clients seeking to deploy generative AI applications that reflect their culture, languages and industries.\n“The world’s leading enterprises see how generative AI is transforming every industry and are eager to deploy applications powered by custom models,” said Julie Sweet, chair and CEO of Accenture. “Accenture has been working with NVIDIA NIM inference microservices for our internal AI applications, and now, using NVIDIA AI Foundry, we can help clients quickly create and deploy custom Llama 3.1 models to power transformative AI applications for their own business priorities.”\nNVIDIA AI Foundry provides an end-to-end service for quickly building custom supermodels. It combines NVIDIA software, infrastructure and expertise with open community models, technology and support from the NVIDIA AI ecosystem.\nWith NVIDIA AI Foundry, enterprises can create custom models using Llama 3.1 models and the NVIDIA NeMo platform — including the NVIDIA Nemotron-4 340B Reward model, ranked first on the Hugging Face RewardBench.\nOnce custom models are created, enterprises can create NVIDIA NIM inference microservices to run them in production using their preferred MLOps and AIOps platforms on their preferred cloud platforms and NVIDIA-Certified Systems™ from global server manufacturers.\nNVIDIA AI Enterprise experts and global system integrator partners work with AI Foundry customers to accelerate the entire process, from development to deployment.\nNVIDIA Nemotron Powers Advanced Model Customization\nEnterprises that need additional training data for creating a domain-specific model can use Llama 3.1 405B and Nemotron-4 340B together to generate synthetic data to boost model accuracy when creating custom Llama supermodels.\nCustomers that have their own training data can customize Llama 3.1 models with NVIDIA NeMo for domain-adaptive pretraining, or DAPT, to further increase model accuracy.\nNVIDIA and Meta have also teamed to provide a distillation recipe for Llama 3.1 that developers can use to build smaller custom Llama 3.1 models for generative AI applications. This enables enterprises to run Llama-powered AI applications on a broader range of accelerated infrastructure, such as AI workstations and laptops.\nIndustry-Leading Enterprises Supercharge AI With NVIDIA and Llama\nCompanies across healthcare, energy, financial services, retail, transportation and telecommunications are already working with NVIDIA NIM microservices for Llama. Among the first to access the new NIM microservices for Llama 3.1 are Aramco, AT&T and Uber.\nTrained on over 16,000 NVIDIA H100 Tensor Core GPUs and optimized for NVIDIA accelerated computing and software — in the data center, in the cloud and locally on workstations with NVIDIA RTX™ GPUs or PCs with GeForce RTX GPUs — the Llama 3.1 collection of multilingual LLMs is a collection of generative AI models in 8B-, 70B- and 405B-parameter sizes.\nNew NeMo Retriever RAG Microservices Boost Accuracy and Performance\nUsing new NVIDIA NeMo Retriever NIM inference microservices for retrieval-augmented generation (RAG), organizations can enhance response accuracy when deploying customized Llama supermodels and Llama NIM microservices in production.\nCombined with NVIDIA NIM inference microservices for Llama 3.1 405B, NeMo Retriever NIM microservices deliver the highest open and commercial text Q&A retrieval accuracy for RAG pipelines.\nEnterprise Ecosystem Ready to Power Llama 3.1 and NeMo Retriever NIM Deployments\nHundreds of NVIDIA NIM partners providing enterprise, data and infrastructure platforms can now integrate the new microservices in their AI solutions to supercharge generative AI for the NVIDIA community of more than 5 million developers and 19,000 startups.\nProduction support for Llama 3.1 NIM and NeMo Retriever NIM microservices is available through NVIDIA AI Enterprise. Members of the NVIDIA Developer Program will soon be able to access NIM microservices for free for research, development and testing on their preferred infrastructure.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "AWS and NVIDIA Extend Collaboration to Advance Generative AI Innovation",
    "link": "https://nvidianews.nvidia.com/news/aws-nvidia-generative-ai-innovation",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUllakEzUmxjM1FYcGhkVWRSVFJDaUFSaTNBaWdCTWdrUm9wVDF0T2VwemdF=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "AWS to offer NVIDIA Grace Blackwell GPU-based Amazon EC2 instances and NVIDIA DGX Cloud to accelerate performance of building and running inference on multi-trillion-parameter LLMs\nIntegration of AWS Nitro System, Elastic Fabric Adapter encryption, and AWS Key Management Service with Blackwell encryption provides customers end-to-end control of their training data and model weights to provide even stronger security for customers’ AI applications on AWS\nProject Ceiba — an AI supercomputer built exclusively on AWS with DGX Cloud — to feature 20,736 GB200 Superchips capable of 414 exaflops for NVIDIA’s own AI R&D\nAmazon SageMaker integration with NVIDIA NIM inference microservices helps customers further optimize price performance of foundation models running on GPUs\nCollaboration between AWS and NVIDIA accelerates AI innovation across healthcare and life sciences\nGTC—Amazon Web Services (AWS), an Amazon.com company (NASDAQ: AMZN), and NVIDIA (NASDAQ: NVDA) today announced that the new NVIDIA Blackwell GPU platform — unveiled by NVIDIA at GTC 2024 — is coming to AWS. AWS will offer the NVIDIA GB200 Grace Blackwell Superchip and B100 Tensor Core GPUs, extending the companies’ long standing strategic collaboration to deliver the most secure and advanced infrastructure, software, and services to help customers unlock new generative artificial intelligence (AI) capabilities.\nNVIDIA and AWS continue to bring together the best of their technologies, including NVIDIA’s newest multi-node systems featuring the next-generation NVIDIA Blackwell platform and AI software, AWS’s Nitro System and AWS Key Management Service (AWS KMS) advanced security, Elastic Fabric Adapter (EFA) petabit scale networking, and Amazon Elastic Compute Cloud (Amazon EC2) UltraCluster hyper-scale clustering. Together, they deliver the infrastructure and tools that enable customers to build and run real-time inference on multi-trillion parameter large language models (LLMs) faster, at massive scale, and at a lower cost than previous-generation NVIDIA GPUs on Amazon EC2.\n“The deep collaboration between our two organizations goes back more than 13 years, when together we launched the world’s first GPU cloud instance on AWS, and today we offer the widest range of NVIDIA GPU solutions for customers,” said Adam Selipsky, CEO at AWS. “NVIDIA’s next-generation Grace Blackwell processor marks a significant step forward in generative AI and GPU computing. When combined with AWS’s powerful Elastic Fabric Adapter Networking, Amazon EC2 UltraClusters’ hyper-scale clustering, and our unique Nitro system’s advanced virtualization and security capabilities, we make it possible for customers to build and run multi-trillion parameter large language models faster, at massive scale, and more securely than anywhere else. Together, we continue to innovate to make AWS the best place to run NVIDIA GPUs in the cloud.”\n“AI is driving breakthroughs at an unprecedented pace, leading to new applications, business models, and innovation across industries,” said Jensen Huang, founder and CEO of NVIDIA. “Our collaboration with AWS is accelerating new generative AI capabilities and providing customers with unprecedented computing power to push the boundaries of what's possible.”\nLatest innovations from AWS and NVIDIA accelerate training of cutting-edge LLMs that can reach beyond 1 trillion parameters\nAWS will offer the NVIDIA Blackwell platform, featuring GB200 NVL72, with 72 Blackwell GPUs and 36 Grace CPUs interconnected by fifth-generation NVIDIA NVLink™. When connected with Amazon’s powerful networking (EFA), and supported by advanced virtualization (AWS Nitro System) and hyper-scale clustering (Amazon EC2 UltraClusters), customers can scale to thousands of GB200 Superchips. NVIDIA Blackwell on AWS delivers a massive leap forward in speeding up inference workloads for resource-intensive, multi-trillion-parameter language models.\nBased on the success of the NVIDIA H100-powered EC2 P5 instances, which are available to customers for short durations through Amazon EC2 Capacity Blocks for ML, AWS plans to offer EC2 instances featuring the new B100 GPUs deployed in EC2 UltraClusters for accelerating generative AI training and inference at massive scale. GB200s will also be available on NVIDIA DGX™ Cloud, an AI platform co-engineered on AWS, that gives enterprise developers dedicated access to the infrastructure and software needed to build and deploy advanced generative AI models. The Blackwell-powered DGX Cloud instances on AWS will accelerate development of cutting-edge generative AI and LLMs that can reach beyond 1 trillion parameters.\nElevate AI security with AWS Nitro System, AWS KMS, encrypted EFA, and Blackwell encryption\nAs customers move quickly to implement AI in their organizations, they need to know that their data is being handled securely throughout their training workflow. The security of model weights — the parameters that a model learns during training that are critical for its ability to make predictions — is paramount to protecting customers’ intellectual property, preventing tampering with models, and maintaining model integrity.\nAWS AI infrastructure and services already have security features in place to give customers control over their data and ensure that it is not shared with third-party model providers. The combination of the AWS Nitro System and the NVIDIA GB200 takes AI security even further by preventing unauthorized individuals from accessing model weights. The GB200 allows inline encryption of the NVLink connections between GPUs and encrypts data transfers, while EFA encrypts data across servers for distributed training and inference. The GB200 will also benefit from the AWS Nitro System, which offloads I/O for functions from the host CPU/GPU to specialized AWS hardware to deliver more consistent performance, while its enhanced security protects customer code and data during processing — on both the customer side and AWS side. This capability — available only on AWS — has been independently verified by NCC Group, a leading cybersecurity firm.\nWith the GB200 on Amazon EC2, AWS will enable customers to create a trusted execution environment alongside their EC2 instance, using AWS Nitro Enclaves and AWS KMS. Nitro Enclaves allow customers to encrypt their training data and weights with KMS, using key material under their control. The enclave can be loaded from within the GB200 instance and can communicate directly with the GB200 Superchip. This enables KMS to communicate directly with the enclave and pass key material to it in a cryptographically secure way. The enclave can then pass that material to the GB200, protected from the customer instance and preventing AWS operators from ever accessing the key or decrypting the training data or model weights, giving customers unparalleled control over their data.\nProject Ceiba taps Blackwell to propel NVIDIA’s future generative AI innovation on AWS\nAnnounced at AWS re:Invent 2023, Project Ceiba is a collaboration between NVIDIA and AWS to build one of the world’s fastest AI supercomputers. Hosted exclusively on AWS, the supercomputer is available for NVIDIA’s own research and development. This first-of-its-kind supercomputer with 20,736 B200 GPUs is being built using the new NVIDIA GB200 NVL72, a system featuring fifth-generation NVLink, that scales to 20,736 B200 GPUs connected to 10,368 NVIDIA Grace CPUs. The system scales out using fourth-generation EFA networking, providing up to 800 Gbps per Superchip of low-latency, high-bandwidth networking throughput — capable of processing a massive 414 exaflops of AI — a 6x performance increase over earlier plans to build Ceiba on the Hopper architecture. NVIDIA research and development teams will use Ceiba to advance AI for LLMs, graphics (image/video/3D generation) and simulation, digital biology, robotics, self-driving cars, NVIDIA Earth-2 climate prediction, and more to help NVIDIA propel future generative AI innovation.\nAWS and NVIDIA collaboration accelerates development of generative AI applications and advance use cases in healthcare and life sciences\nAWS and NVIDIA have joined forces to offer high-performance, low-cost inference for generative AI with Amazon SageMaker integration with NVIDIA NIM™ inference microservices, available with NVIDIA AI Enterprise. Customers can use this combination to quickly deploy FMs that are pre-compiled and optimized to run on NVIDIA GPUs to SageMaker, reducing the time-to-market for generative AI applications.\nAWS and NVIDIA have teamed up to expand computer-aided drug discovery with new NVIDIA BioNeMo™ FMs for generative chemistry, protein structure prediction, and understanding how drug molecules interact with targets. These new models will soon be available on AWS HealthOmics, a purpose-built service that helps healthcare and life sciences organizations store, query, and analyze genomic, transcriptomic, and other omics data.\nAWS HealthOmics and NVIDIA Healthcare teams are also working together to launch generative AI microservices to advance drug discovery, medtech, and digital health — delivering a new catalog of GPU-accelerated cloud endpoints for biology, chemistry, imaging and healthcare data so healthcare enterprises can take advantage of the latest advances in generative AI on AWS.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "NVIDIA NIM Revolutionizes Model Deployment, Now Available to Transform World’s Millions of Developers Into Generative AI Developers",
    "link": "https://nvidianews.nvidia.com/news/nvidia-nim-model-deployment-generative-ai-developers",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXZhalZrV1cxYU5XRlFTSGN4VFJDb0FSaXNBaWdCTWdhVlVwQXBxUVk=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-06-02T07:00:00.000Z",
    "time": "Jun 2",
    "articleType": "regular",
    "content": "150+ Partners Across Every Layer of AI Ecosystem Embedding NIM Inference Microservices to Speed Enterprise AI Application Deployments From Weeks to Minutes\nCOMPUTEX—NVIDIA today announced that the world’s 28 million developers can now download NVIDIA NIM™ — inference microservices that provide models as optimized containers — to deploy on clouds, data centers or workstations, giving them the ability to easily build generative AI applications for copilots, chatbots and more, in minutes rather than weeks.\nThese new generative AI applications are becoming increasingly complex and often utilize multiple models with different capabilities for generating text, images, video, speech and more. NVIDIA NIM dramatically increases developer productivity by providing a simple, standardized way to add generative AI to their applications.\nNIM also enables enterprises to maximize their infrastructure investments. For example, running Meta Llama 3-8B in a NIM produces up to 3x more generative AI tokens on accelerated infrastructure than without NIM. This lets enterprises boost efficiency and use the same amount of compute infrastructure to generate more responses.\nNearly 200 technology partners — including Cadence, Cloudera, Cohesity, DataStax, NetApp, Scale AI and Synopsys — are integrating NIM into their platforms to speed generative AI deployments for domain-specific applications, such as copilots, code assistants and digital human avatars. Hugging Face is now offering NIM — starting with Meta Llama 3.\n“Every enterprise is looking to add generative AI to its operations, but not every enterprise has a dedicated team of AI researchers,” said Jensen Huang, founder and CEO of NVIDIA. “Integrated into platforms everywhere, accessible to developers everywhere, running everywhere — NVIDIA NIM is helping the technology industry put generative AI in reach for every organization.”\nEnterprises can deploy AI applications in production with NIM through the NVIDIA AI Enterprise software platform. Starting next month, members of the NVIDIA Developer Program can access NIM for free for research, development and testing on their preferred infrastructure.\n40+ NIM Microservices Power Gen AI Models Across Modalities\nNIM containers are pre-built to speed model deployment for GPU-accelerated inference and can include NVIDIA CUDA® software, NVIDIA Triton Inference Server™ and NVIDIA TensorRT™-LLM software.\nOver 40 NVIDIA and community models are available to experience as NIM endpoints on ai.nvidia.com, including Databricks DBRX, Google’s open model Gemma, Meta Llama 3, Microsoft Phi-3, Mistral Large, Mixtral 8x22B and Snowflake Arctic.\nDevelopers can now access NVIDIA NIM microservices for Meta Llama 3 models from the Hugging Face AI platform. This lets developers easily access and run the Llama 3 NIM in just a few clicks using Hugging Face Inference Endpoints, powered by NVIDIA GPUs on their preferred cloud.\nEnterprises can use NIM to run applications for generating text, images and video, speech and digital humans. With NVIDIA BioNeMo™ NIM microservices for digital biology, researchers can build novel protein structures to accelerate drug discovery.\nDozens of healthcare companies are deploying NIM to power generative AI inference across a range of applications, including surgical planning, digital assistants, drug discovery and clinical trial optimization.\nWith new NVIDIA ACE NIM microservices, developers can easily build and operate interactive, lifelike digital humans in applications for customer service, telehealth, education, gaming and entertainment.\nHundreds of AI Ecosystem Partners Embedding NIM\nPlatform providers including Canonical, Red Hat, Nutanix and VMware (acquired by Broadcom) are supporting NIM on open-source KServe or enterprise solutions. AI application companies Hippocratic AI, Glean, Kinetica and Redis are also deploying NIM to power generative AI inference.\nLeading AI tools and MLOps partners — including Amazon SageMaker, Microsoft Azure AI, Dataiku, DataRobot, deepset, Domino Data Lab, LangChain, Llama Index, Replicate, Run.ai, Saturn Cloud, Securiti AI and Weights & Biases — have also embedded NIM into their platforms to enable developers to build and deploy domain-specific generative AI applications with optimized inference.\nGlobal system integrators and service delivery partners Accenture, Deloitte, Infosys, Latentview, Quantiphi, SoftServe, Tata Consultancy Services (TCS) and Wipro have created NIM competencies to help the world’s enterprises quickly develop and deploy production AI strategies.\nEnterprises can run NIM-enabled applications virtually anywhere, including on NVIDIA-Certified Systems™ from global infrastructure manufacturers Cisco, Dell Technologies, Hewlett-Packard Enterprise, Lenovo and Supermicro, as well as server manufacturers ASRock Rack, ASUS, GIGABYTE, Ingrasys, Inventec, Pegatron, QCT, Wistron and Wiwynn. NIM microservices have also been integrated into Amazon Web Services, Google Cloud, Azure and Oracle Cloud Infrastructure.\nTitans of Industry Amp Up Generative AI With NIM\nIndustry leaders Foxconn, Pegatron, Amdocs, Lowe’s, ServiceNow and Siemens are among the businesses using NIM for generative AI applications in manufacturing, healthcare, financial services, retail, customer service and more:\nFoxconn — the world’s largest electronics manufacturer — is using NIM in the development of domain-specific LLMs embedded into a variety of internal systems and processes in its AI factories for smart manufacturing, smart cities and smart electric vehicles.\nPegatron — a Taiwanese electronics manufacturing company — is leveraging NIM for Project TaME, a Taiwan Mixtral of Experts model designed to advance the development of local LLMs for industries.\nAmdocs — a leading global provider of software and services to communications and media companies — is using NIM to run a customer billing LLM that significantly lowers the cost of tokens, improves accuracy by up to 30% and reduces latency by 80%, driving near real-time responses.\nLowe’s — a FORTUNE® 50 home improvement company — is using generative AI for a variety of use cases. For example, the retailer is leveraging NVIDIA NIM inference microservices to elevate experiences for associates and customers.\nServiceNow — the AI platform for business transformation — announced earlier this year that it was one of the first platform providers to access NIM to enable fast, scalable and more cost-effective LLM development and deployment for its customers. NIM microservices are integrated within the Now AI multimodal model and are available to customers that have ServiceNow’s generative AI experience, Now Assist, installed.\nSiemens — a global technology company focused on industry, infrastructure, transport and healthcare — is integrating its operational technology with NIM microservices for shop floor AI workloads. It is also building an on-premises version of its Industrial Copilot for Machine Operators using NIM.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "Nvidia overtakes Apple as No. 2 most valuable company",
    "link": "https://www.reuters.com/technology/nvidia-verge-overtaking-apple-no-2-most-valuable-company-2024-06-05/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVJPRU14WlY5cGFYWmpWM2t6VFJDM0FSaVRBaWdCTWdZSklKYVhJQWc=-w400-h224-p-df-rw",
    "source": "Reuters",
    "datetime": "2024-06-06T07:00:00.000Z",
    "time": "Jun 6",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Game Pass Comes to GeForce NOW, Along With 25 New Games",
    "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-aug-24/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUlWbnBDYnpocVlsUjFkR0o2VFJDZUFSakFBaWdCTWdhQkZKQWxtd2s=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-24T07:00:00.000Z",
    "time": "Aug 24, 2023",
    "articleType": "regular",
    "content": "As part of NVIDIA and Microsoft’s collaboration to bring more choice to gamers, new Microsoft Store integration has been added to GeForce NOW that lets gamers stream select titles from the Game Pass catalog on GeForce NOW, starting today.\nWith the Microsoft Store integration, members will see a brand-new Xbox button on supported PC games and can seamlessly launch these titles across their devices, provided they either purchased the standalone games through the Microsoft Store or have an active Xbox Game Pass Ultimate or PC Game Pass subscription.\nHot off our recent Gamescom announcement, four blockbuster titles are coming to GeForce NOW this fall: Alan Wake 2, Cyberpunk 2077: Phantom Liberty expansion, Party Animals and PAYDAY 3.\nPlus, head to the cloud and stream the 25 new titles joining the cloud this week, including DOOM 2016 from Bethesda.\nMembers have also been playing the GeForce NOW Ultimate KovaaK’s challenge, raising the bar with 240 frames per second streaming using an Ultimate membership. Check out the leaderboard to see how Ultimate members are stacking up against other GeForce NOW members — top scorers have a chance to win some ultimate prizes through Thursday, Sept. 21, including a six-month PC Game Pass.\nSelect Game Pass Titles Now Available\nGive a warm welcome to the Microsoft Store on GeForce NOW. It joins digital platforms Steam, Epic Games Store, Ubisoft Connect and others in the cloud. Experience it today with hit PC games from Xbox Game Studios, Bethesda and other top publishers recently added to GeForce NOW,  like Fatshark, Paradox and TaleWorld Entertainment.\nWith a GeForce NOW Ultimate membership, stream popular shooters Gears 5 and Deathloop with the highest graphical fidelity. Embark on a mini-adventure on the big screen by shrinking to the size of an ant in Grounded. Or enjoy the historical narrative Pentiment while on the go with a mobile device.\nTake a deep dive into history with titles from the Age of Empires series on a Chromebook and a comfy throne of your own or experience an alternative version of history in the newly added Wolfenstein II: New Colossus and Wolfenstein: Youngblood titles with the power of 4K streaming on NVIDIA SHIELD.\nFight the dark tide with the power of the cloud.\nLead armies in TaleWorld Entertainment’s action role-playing game Mount & Blade II: Bannerlord, take on hordes of enemies in Fatshark’s action shooter Warhammer 40,000: Darktide or explore infinite worlds in Hello Game’s No Man’s Sky.\nKeep an eye out for more games from the PC Game Pass library to be added to GeForce NOW. Check out this article for more details on how Game Pass will work on GeForce NOW.\nAnd this week only, on top of being able to win a six-month Ultimate membership and $100 Steam gift card for making it into the top three on the weekly leaderboard of the Ultimate KovaaK’s challenge, those who make it into the top 10 will get a six-month PC Game Pass. Keep an eye out on GeForce NOW’s Twitter and Facebook accounts for more details.\nTop publishers Epic Games Publishing, CD Projekt Red and Deep Silver are all bringing their blockbuster titles to GeForce NOW at launch in the fall.\nWake up, it’s the second game in the “Alan Wake” franchise.\nUncover the newest mystery in the upcoming survival horror game Alan Wake 2, sequel to the award-winning game Alan Wake, from Remedy Entertainment and Epic Games Publishing. Survive as the best-selling horror writer Alan Wake — who’s trapped in a dark dimension and trying to write his way out — or as FBI agent Saga Anderson in a life-or-death race to solve a small-town murder that quickly spirals into a nightmare.\nPlay through two distinct stories set in two beautiful yet terrifying worlds and see events unfold from different perspectives. The characters must take on powerful supernatural enemies and use more than just a gun to survive: light is the ultimate weapon in the fight against darkness. Members can stream the game from the cloud when it launches on Tuesday, Oct. 27.\nWelcome to the neon cloud.\nReturn as cyber-enhanced mercenary V in the upcoming spy-thriller expansion for the hit open-world action adventure Cyberpunk 2077 from CD Projekt Red. Phantom Liberty features the all-new district of Dogtown, infinitely replayable open-world activities, an exclusive skill tree and much more — including  new weapons, cyberware, vehicles and gigs for players to discover. Embark on a high-stakes mission of espionage and intrigue to save the NUS President when the expansion launches in the cloud on Tuesday, Sept. 26.\nIt pays to be a GeForce NOW member.\nJoin the Payday Gang in the upcoming third installment of the PAYDAY franchise from Starbeeze Studios, Overkill Software and Deep Silver. In PAYDAY 3, play as notorious criminals who must face off against new enemies and challenges in an action-packed, high-octane experience. Invite your friends to the four-player online co-op mode to pull off the ultimate heist when the title launches on GeForce NOW on Thursday, Sept. 21.\nThese games are all headed to the cloud this fall. Upgrade to an Ultimate membership today to skip the waiting lines over free members and get access to powerful NVIDIA technology, including RTX ON and DLSS 3.5 technology for AI-powered graphics and peak-performance gaming.\nYou won’t be able to resist the power of the cloud.\nThe next Bethesda game to heat up the cloud is DOOM 2016. Fight through hordes of demonic forces on Mars after waking up on a Union Aerospace Corporation energy-mining facility. Play as the Doom Slayer, an unnamed space marine from the DOOM franchise, and use a variety of weapons, gadgets and melee attacks in this fast-paced, first-person shooter. Plus, several online multiplayer modes are available, so members can grab some buddies to stream with.\nCatch the full list of games joining the cloud this week:\nWrestleQuest (New release on Steam, Aug. 21)\nJumplight Odyssey (New release on Steam, Aug. 21)\nBlasphemous 2 (New release on Steam, Aug. 24)\nRIDE 5 (New release on Steam, Aug. 24)\nAge of Empires: Definitive Edition (Xbox)\nAge of Empires III: Definitive Edition (Xbox)\nAge of Empires IV: Anniversary Edition (Xbox)\nMount & Blade II: Bannerlord (Xbox)\nShadowrun: Dragonfall – Director’s Cut (Xbox)\nThe Texas Chain Saw Massacre (Xbox)\nWolfenstein II: The New Colossus (Xbox)\nWhat games are you looking forward to? Let us know on Twitter or in the comments below.\n— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) August 23, 2023",
    "favicon": ""
  },
  {
    "title": "‘Everybody Will Have an AI Assistant,’ NVIDIA CEO Tells SIGGRAPH Audience",
    "link": "https://blogs.nvidia.com/blog/nvidia-ceo-siggraph/",
    "image": "https://news.google.com/api/attachments/CC8iMkNnNXhVbDh4TmtsRU1YTkhWblJhVFJDakFSaTBBaWdCTWdzSlVJZ1puS2k1U0JvVFRR=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-07-29T22:50:22.000Z",
    "time": "2 days ago",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Nvidia's stock market value topped $3.3 trillion. How it became No. 1 in the S&P 500, by the numbers",
    "link": "https://apnews.com/article/nvidia-artificial-intelligence-stock-market-nasdaq-b1e79cf391f212a4714433afd140e341",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW9aWGxuWkhScE5HcENiSGRQVFJDNkFSaVBBaWdCTWdhdFJKU1BIUWs=-w400-h224-p-df-rw",
    "source": "The Associated Press",
    "datetime": "2024-06-06T07:00:00.000Z",
    "time": "Jun 6",
    "articleType": "regular",
    "content": "Updated 11:01 PM GMT+2, June 18, 2024\nNvidia’s startling ascent in the stock market reached another milestone Tuesday as the chipmaker rose to become the most valuable company in the S&P 500. Investors now say the company is worth over $3.3 trillion.Nvidia has seen soaring demand for its semiconductors, which are used to power artificial intelligence applications. Revenue more than tripled in the latest quarter from the same period a year earlier. The company’s journey to be one of the most prominent players in AI has produced some eye-popping numbers. Here’s a look:$3.334 TrillionNvidia’s total market value as of the close Tuesday. It edged past Microsoft ($3.317 trillion). Apple is the third most-valuable company ($3.286 trillion). One year ago, the company had just crossed the $1 trillion threshold.$113 billionThe one-day increase in Nvidia’s market value on Tuesday.$135.58Nvidia’s closing stock price Tuesday. Two weeks ago the stock traded at more than $1,200, but the company completed a 10-for-1 stock split after trading closed on June 7. That gave each investor nine additional shares for every share they already owned. Companies with a high stock price often conduct stock splits to make the stock more affordable for investors.\n$119.9 billionAnalysts’ estimate for Nvidia’s revenue for the fiscal year that ends in January 2025. That would be about double its revenue for fiscal 2024 and more than four times its receipts the year before that.\n53.4%Nvidia’s estimated net margin, or the percentage of revenue that gets turned into profit. Looked at another way, about 53 cents of every $1 in revenue Nvidia took in last year went to its bottom line. By comparison, Apple’s net margin was 26.3% in its most recent quarter and Microsoft’s was 36.4%. Both those companies have significantly higher revenue than Nvidia, however.32%\nHow much of the S&P 500’s gain for the year through May came only from Nvidia.11The number of companies other than Nvidia that were once the most valuable in the S&P 500, going back to 1926, according to S&P Dow Jones Indices. Among them: AT&T, IBM and Walmart.",
    "favicon": "/favicon-32x32.png"
  }
]