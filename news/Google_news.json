[
  {
    "title": "Programs and Events",
    "link": "http://research.google/programs-and-events/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNVBVM1JDY21GVGNFODVTRzFmVFJDc0FSaW1BaWdCTWdtQk1JZ0VrNm90RWdF=-w400-h224-p-df-rw",
    "source": "Google Research",
    "datetime": "2024-04-08T23:31:12.000Z",
    "time": "Apr 9",
    "articleType": "regular",
    "content": "Working alongside the broader community\nWe’re proud to support academic and research institutions to push the boundaries of AI and computer science. Leveraging Google’s research initiatives, our programs provide funding, mentorship, and engagement opportunities - allowing the research community to innovate faster to solve the world’s biggest problems.\nWe’re proud to support academic and research institutions to push the boundaries of AI and computer science. Leveraging Google’s research initiatives, our programs provide funding, mentorship, and engagement opportunities - allowing the research community to innovate faster to solve the world’s biggest problems.\nOpportunities that support the research community\nGoogle offers fellowships, scholarships, internships and other student engagement opportunities to support the next generation of researchers.\nOne-on-one sessions with my mentor were the highlight of my experience. Through discussions with my mentor, I was able to gain solid feedback on my work — from grad school applications and industry research experiences to my current research project at Berkeley. I have a much better understanding of computer science industry research positions and future opportunities for the role.\nOur global faculty programs provide funding and support to advance academic research and enable meaningful engagements with Google researchers.\nPart of what Google does through the University Relations program [is] enable African university research and tech development. ...Now we are really walking the path together. It's the beginning of great things to come.\nWe maintain strong ties with students, faculty, labs, and academic institutions worldwide pursuing innovative research in core areas relevant to our products and services.",
    "favicon": "/static/assets/favicon.ico"
  },
  {
    "title": "Passwordless by default: Make the switch to passkeys",
    "link": "https://blog.google/technology/safety-security/passkeys-default-google-accounts/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNU9NRUkzWTJkVlMwdzFNVzV3VFJDb0FSaXJBaWdCTWdNRk13dw=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-10-10T07:00:00.000Z",
    "time": "Oct 10, 2023",
    "articleType": "regular",
    "content": "Earlier this year we rolled out support for passkeys, a simpler and more secure way to sign into your accounts online. We’ve received really positive feedback from our users, so today we’re making passkeys even more accessible by offering them as the default option across personal Google Accounts.This means the next time you sign in to your account, you’ll start seeing prompts to create and use passkeys, simplifying your future sign-ins. It also means you’ll see the “Skip password when possible” option toggled on in your Google Account settings.To use passkeys, you just use a fingerprint, face scan or pin to unlock your device, and they are 40% faster than passwords — and rely on a type of cryptography that makes them more secure. But while they’re a big step forward, we know that new technologies take time to catch on — so passwords may be around for a little while. That's why people will still be given the option to use a password to sign in and may opt-out of passkeys by turning off “Skip password when possible.”We’ve found that one of the most immediate benefits of passkeys is that they spare people the headache of remembering all those numbers and special characters in passwords. They’re also phishing resistant.\nPasskeys in more placesSince launching earlier this year, people have used passkeys on their favorite apps like YouTube, Search and Maps, and we’re encouraged by the results. We’re even more excited to see the growing adoption of passkeys across industry. Recently, Uber and eBay have enabled passkeys — giving people the option to ditch passwords when signing-in on their platforms — and WhatsApp compatibility will also be coming soon.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "New Gmail protections for a safer, less spammy inbox",
    "link": "https://blog.google/products/gmail/gmail-security-authentication-spam-protection/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVBiVnBTZWxweGVHZHFiSGxKVFJDUkFSamNBaWdCTWdZQlU0eXRMUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-10-03T07:00:00.000Z",
    "time": "Oct 3, 2023",
    "articleType": "regular",
    "content": "Starting in 2024, we’ll require bulk senders to authenticate their emails, allow for easy unsubscription and stay under a reported spam threshold.\nIt’s clear that email has become an essential part of daily communication. And whether you’re submitting a job application or staying in touch with a loved one, your emails should be safe and secure.Gmail’s AI-powered defenses stop more than 99.9% of spam, phishing and malware from reaching inboxes and block nearly 15 billion unwanted emails every day. But now, nearly 20 years after Gmail launched, the threats we face are more complex and pressing than ever.So today, we’re introducing new requirements for bulk senders — those who send more than 5,000 messages to Gmail addresses in one day — to keep your inbox even safer and more spam-free.Focus on email validationMany bulk senders don’t appropriately secure and configure their systems, allowing attackers to easily hide in their midst. To help fix that, we’ve focused on a crucial aspect of email security: the validation that a sender is who they claim to be. As basic as it sounds, it’s still sometimes impossible to verify who an email is from given the web of antiquated and inconsistent systems on the internet.Last year we started requiring that emails sent to a Gmail address must have some form of authentication. And we’ve seen the number of unauthenticated messages Gmail users receive plummet by 75%, which has helped declutter inboxes while blocking billions of malicious messages with higher precision.That’s great progress, but there’s much more we need to do — starting with new requirements for large senders.New requirements for bulk sendersBy February 2024, Gmail will start to require that bulk senders:Authenticate their email: You shouldn’t need to worry about the intricacies of email security standards, but you should be able to confidently rely on an email’s source. So we're requiring those who send significant volumes to strongly authenticate their emails following well-established best practices. Ultimately, this will close loopholes exploited by attackers that threaten everyone who uses email.Enable easy unsubscription: You shouldn’t have to jump through hoops to stop receiving unwanted messages from a particular email sender. It should take one click. So we’re requiring that large senders give Gmail recipients the ability to unsubscribe from commercial email in one click, and that they process unsubscription requests within two days. We’ve built these requirements on open standards so that once senders implement them, everyone who uses email benefits.Ensure they’re sending wanted email: Nobody likes spam, and Gmail already includes many tools that keep unwanted messages out of your inbox. To add yet another protection, moving forward, we’ll enforce a clear spam rate threshold that senders must stay under to ensure Gmail recipients aren’t bombarded with unwanted messages. This is an industry first, and as a result, you should see even less spam in your inbox.We aren’t the only ones pushing for these changes. Our industry partners also see the pressing need to institute them: \"No matter who their email provider is, all users deserve the safest, most secure experience possible,” says Marcel Becker, Sr. Dir. Product at Yahoo. “In the interconnected world of email, that takes all of us working together. Yahoo looks forward to working with Google and the rest of the email community to make these common sense, high-impact changes the new industry standard.\"These practices should be considered basic email hygiene, and many senders already meet most of these requirements. For those who need help to improve their systems, we’re sharing clear guidance before enforcement begins in February 2024.These changes are like a tune-up for the email world, and by fixing a few things under the hood, we can keep email running smoothly. But just like a tune-up, this is not a one-time exercise. Keeping email more secure, user friendly and spam-free requires constant collaboration and vigilance from the entire email community. And we'll keep working together to make sure your inbox stays safe.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "7 new Android features to elevate your everyday",
    "link": "https://blog.google/products/android/new-android-features-may-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTNhakV3WkhSV05WRm9kMmRNVFJDb0FSaXJBaWdCTWdZQmNZNk9wUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-30T07:00:00.000Z",
    "time": "May 30",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Generative AI in Search: Let Google do the searching for you",
    "link": "https://blog.google/products/search/generative-ai-google-search-may-2024/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNXNjbEZHYkcwd1JtNXhSbWhvVFJDb0FSaXNBaWdCTWdNbEJDQQ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-14T07:00:00.000Z",
    "time": "May 14",
    "articleType": "regular",
    "content": "With expanded AI Overviews, more planning and research capabilities, and AI-organized search results, our custom Gemini model can take the legwork out of searching.\nWith AI Overviews, people are visiting a greater diversity of websites for help with more complex questions. And we see that the links included in AI Overviews get more clicks than if the page had appeared as a traditional web listing for that query. As we expand this experience, we’ll continue to focus on sending valuable traffic to publishers and creators. As always, ads will continue to appear in dedicated slots throughout the page, with clear labeling to distinguish between organic and sponsored results.Adjust your AI OverviewSoon, you’ll be able to adjust your AI Overview with options to simplify the language or break it down in more detail. This can be particularly useful if you’re new to a topic, or if you’re trying to simplify something to satisfy your kid’s curiosity.\nThis update is coming soon to Search Labs, for English queries in the U.S.Ask your most complex questionsWith our custom Gemini model’s multi-step reasoning capabilities, AI Overviews will help with increasingly complex questions. Rather than breaking your question into multiple searches, you can ask your most complex questions, with all the nuances and caveats you have in mind, all in one go.For example, maybe you’re looking for a new yoga or pilates studio, and you want one that’s popular with locals, conveniently located for your commute, and also offers a discount for new members. Soon, with just one search, you’ll be able to ask something like “find the best yoga or pilates studios in Boston and show me details on their intro offers, and walking time from Beacon Hill.”\nThese multi-step reasoning capabilities are coming soon to AI Overviews in Search Labs, for English queries in the U.S.Plan aheadBeyond finding the right answer or information for a complex question, Search will also be able to plan with you.With planning capabilities directly in Search, you can get help creating plans for whatever you need, starting with meals and vacations. Search for something like “create a 3 day meal plan for a group that’s easy to prepare,” and you’ll get a starting point with a wide range of recipes from across the web.\nIf you want to change anything, you can easily ask for whatever adjustments you need, like swapping dinner to a vegetarian dish. Just like that, Search will customize your meal plan. You’ll be able to quickly export your meal plan to Docs or Gmail.Meal and trip planning are available now in Search Labs in English in the U.S. Later this year, we’ll add customization capabilities and more categories like parties, date night and workouts.Explore an AI-organized results pageWhen you’re looking for fresh ideas, it can take a lot of work to find inspiration and consider all your options. Soon, when you’re looking for ideas, Search will use generative AI to brainstorm with you and create an AI-organized results page that makes it easy to explore.You’ll see helpful results categorized under unique, AI-generated headlines, featuring a wide range of perspectives and content types.\nFor English searches in the U.S., you’ll start to see this new AI-organized search results page when you look for inspiration — starting soon with dining and recipes, followed by movies, music, books, hotels, shopping and more.Take a video, get help from AISearch is so much more than just words in a text box. Often the questions you have are about the things you see around you, including objects in motion.Thanks to advancements in video understanding, we’re able to take visual search to a whole new level, with the ability to ask questions with video.Maybe you bought a record player at a thrift shop, but it’s not working when you turn it on and the metal piece with the needle is drifting unexpectedly. Searching with video saves you the time and trouble of finding the right words to describe this issue, and you’ll get an AI Overview with steps and resources to troubleshoot.\nHere’s a look at everything we announced at Google I/O 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "5 new Chrome features to help you search on mobile",
    "link": "https://blog.google/products/chrome/chrome-mobile-features-june-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHdaMFYxUVc5b2RGQTFjRXM1VFJDUkFSamNBaWdCTWdhQklvaktHUWs=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-06-26T07:00:00.000Z",
    "time": "Jun 26",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google Keep reminders will be saved to Tasks",
    "link": "https://blog.google/products/keep/google-keep-reminders-tasks-update/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNURaVFpsU0ZoUmVuTkVlRk5mVFJDb0FSaXJBaWdCTWdZQlE0cERPZ1k=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-04-25T07:00:00.000Z",
    "time": "Apr 25",
    "articleType": "regular",
    "content": "Over the next year, Keep reminders will be automatically saved to Tasks. So you’ll be able to see, edit and complete them from Keep, Calendar, Tasks and Assistant.\nWith Google Keep, it’s quick and easy to jot down whatever’s on your mind — whether it’s a grocery list, deadline reminder or a budding idea. You can make lists, take photos, record your voice and even draw notes. You can also add time or location-based reminders to any note so you get a nudge exactly where or when you need it.Over the next year, those Keep reminders will automatically save to Google Tasks. So in addition to accessing reminders through Keep, you’ll be able to see, edit and complete them from Calendar, Tasks and Assistant.\nThis new capability will make Google Tasks the single solution for managing your to-dos across Workspace. So whether you’re saving something from Keep, Gmail, Calendar, Chat, Docs or Assistant, Google Tasks will make sure it’s up to date and accessible across the Workspace products you use.\nFor more details on when these changes will roll out, look for updates in Google Keep and the Help Center.\nGoogle Workspace5 tips for writing great prompts for Gemini in the Workspace side panel\nLearning & EducationNew AI tools for Google Workspace for Education\nLearning & EducationUpdates on how we're using AI to support students and educators\nMapsUse these 5 AI-powered tools to plan your summer travel\nAndroid EnterpriseHow we’re making Android Enterprise signup and access to Google services better\nLearning & EducationGet more out of Google’s education tools with Education Navigator",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Get more done with Gemini: Try 1.5 Pro and more intelligent features",
    "link": "https://blog.google/products/gemini/google-gemini-update-may-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVZaVTVXU21SeWJ6bEpPSEp2VFJDb0FSaXNBaWdCTWdhcFE0cU5vUWs=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-14T07:00:00.000Z",
    "time": "May 14",
    "articleType": "regular",
    "content": "Gemini, your AI assistant, is getting even smarter with a longer context window, new data analysis capabilities, and connections to more Google apps. Gemini 1.5 Pro, available to Advanced subscribers, can now handle up to 1 million tokens, making it the longest context window of any widely available consumer chatbot. You can upload files via Google Drive or your device for in-depth analysis, and Gemini will soon act as a data analyst, creating visualizations and charts from uploaded data. New ways to interact with Gemini include chatting in Google Messages and a mobile conversational experience called Live, which uses state-of-the-art speech technology for more natural conversations.\nSummaries were generated by Google AI. Generative AI is experimental.\nWe’re bringing Gemini 1.5 Pro to Gemini Advanced subscribers in over 35 languages, along with a 1 million token context window, a new conversational experience and tools that let Gemini take action on your behalf.\nNatural conversations with Gemini Live\nGemini is designed to be your personal AI assistant — one that’s conversational, intuitive and helpful. Whether you use it in the app or through the web experience, Gemini can help you tackle complex tasks, and it can take action on your behalf.Now, we’re making several updates — including a longer context window, new data analysis capabilities, connections to additional Google apps and more customizable options — so you can collaborate with the most intelligent and personalized Gemini yet.\nAnalyze documents with the world’s longest context windowToday we’re bringing Gemini 1.5 Pro, our cutting-edge model, to Gemini Advanced. Over 1 million people have signed up to try Gemini Advanced in just three months. And now Gemini 1.5 Pro brings you our latest technical advances, including a greatly expanded context window starting at 1 million tokens — the longest of any widely available consumer chatbot in the world. A context window this long means Gemini Advanced can make sense of multiple large documents, up to 1,500-pages total, or summarize 100 emails. Soon it will be able to handle an hour of video content or codebases with more than 30,000 lines.\nTo take advantage of this long context window, we’re adding the ability to upload files via Google Drive or directly from your device, right into Gemini Advanced. Now you can quickly get answers and insights about dense documents, like figuring out the details of the pet policy in your rental agreement or comparing key arguments of multiple long research papers. And soon, Gemini Advanced will act as a data analyst, uncovering insights and building custom visualizations and charts on the fly from uploaded data files like spreadsheets.Gemini keeps your files private to you, and they’re not used to train our models.\nGemini is natively multimodal, and 1.5 Pro brings big improvements to image understanding. For example, you can snap a photo of a dish at your favorite restaurant and ask for a recipe, or take a picture of a math problem and get step-by-step instructions on how to solve it — all from a single image.Gemini 1.5 Pro will be available to Gemini Advanced subscribers in more than 150 countries and over 35 languages.\nHave more natural conversations with Gemini LiveWe’re also adding new ways to interact with Gemini more naturally, whether you’re texting or talking. With Gemini in Google Messages, you can now chat with Gemini in the same app you message your friends.And in the coming months, we’re rolling out Live for Gemini Advanced subscribers, a new mobile conversational experience that uses our state-of-the-art speech technology to make speaking with Gemini more intuitive. With Gemini Live, you can talk to Gemini and choose from a variety of natural-sounding voices it can respond with. You can even speak at your own pace or interrupt mid-response with clarifying questions, just like you would in any conversation.\nLet’s say you’re getting ready for a job interview: Just go Live and ask Gemini to help you prepare. Gemini can rehearse with you, and even suggest skills to highlight when talking to your potential employer. Later this year you’ll be able to use your camera when you go Live, opening up conversations about what you see around you.\nTake the hassle out of making complex plans, like trip itinerariesSometimes you spend more time researching and organizing a trip than enjoying the trip itself. Gemini Advanced’s new planning experience will go beyond showing a list of suggested activities to create a custom itinerary just for you.Imagine you ask Gemini: “My family and I are going to Miami for Labor Day. My son loves art and my husband really wants fresh seafood. Can you pull my flight and hotel info from Gmail and help me plan the weekend?”This prompt requires Gemini to do so much more than present publicly available information like other chatbots. Gemini takes into account your flight timing, meal preferences and information about local museums, while also understanding where each stop is located and how long it will take to travel between each activity. It grabs your flight information from Gmail, taps Google Maps for restaurant and museum recommendations near your hotel, and uses Search to recommend other activities, like a walking tour of the Design District or beach time, to fill out the rest of your day. It synthesizes all that information for you and creates a personal, customized itinerary that satisfies all your requests. And if you make changes or add more details, the itinerary will update automatically.\nThis dynamic new planning experience is coming to Gemini Advanced in the coming months.\nPersonalize Gemini with GemsFor an even more personal experience, Gemini Advanced subscribers will soon be able to create Gems — customized versions of Gemini. You can create any Gem you dream up: a gym buddy, sous chef, coding partner or creative writing guide. They’re easy to set up, too. Simply describe what you want your Gem to do and how you want it to respond — like “you're my running coach, give me a daily running plan and be positive, upbeat and motivating.” Gemini will take those instructions and, with one click, enhance them to create a Gem that meets your specific needs.\nConnect with more Google appsLast year we brought Extensions directly into Gemini, allowing you to get more things done with the Google apps and services you already use. We’ve continued to bring Google apps to Gemini, like the YouTube Music Extension rolling out now. With Gemini, you can search for your favorite music even if you don't know the song title by mentioning a favorite verse or a featured artist.Soon, we’ll connect even more Google tools with Gemini, including Google Calendar, Tasks and Keep. So you’ll be able to do things like snap a picture of your child’s school syllabus and ask Gemini to create a calendar entry for each assignment, or take a photo of a new recipe and add it to your Keep as a shopping list.With these updates, many of which are coming to our business customers too, you’ll get the most personal and helpful experience with Gemini yet — all from a single prompt.Look out for these new features soon, and start chatting with Gemini today.\nHere’s a look at everything we announced at Google I/O 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Experience Google AI in even more ways on Android",
    "link": "https://blog.google/products/android/google-ai-android-update-io-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUxSVnBSVXpWR2JqTTFjek00VFJDb0FSaXJBaWdCTWdZQlVZN09zUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-14T07:00:00.000Z",
    "time": "May 14",
    "articleType": "regular",
    "content": "Google AI is revolutionizing Android devices, enabling new ways to interact and get things done.\nCircle to Search gets smarter, helping students solve physics and math problems directly from their phones and tablets.\nGemini on Android improves context understanding, allowing users to drag and drop generated images and ask questions about videos and PDFs.\nGemini Nano with Multimodality coming to Pixel, bringing multimodal capabilities for richer image descriptions and scam alerts during phone calls.\nAndroid 15 and ecosystem updates coming tomorrow.\nSummaries were generated by Google AI. Generative AI is experimental.\nGoogle is making Android phones smarter with AI.\nCircle to Search can now help students with homework.\nGemini, a new AI assistant, can understand what's on your screen and help you do things.\nAndroid phones will soon be able to alert you to suspected scams during phone calls.\nSummaries were generated by Google AI. Generative AI is experimental.\nBy building AI right into the Android operating system, we're reimagining how you can interact with your phone.\nCircle to Search and homework help\nFull multimodal coming to Gemini Nano\nMore to come on Android\nWe’re at a once-in-a-generation moment where the latest advancements in AI are reinventing what phones can do. With Google AI at the core of Android’s operating system, the billions of people who use Android can now interact with their devices in entirely new ways.Today, we’re sharing updates that let you experience Google AI on Android.\nCircle to Search can now help students with homeworkWith Circle to Search built directly into the user experience, you can search anything you see on your phone using a simple gesture — without having to stop what you’re doing or switch to a different app. Since launching at Samsung Unpacked, we’ve added new capabilities to Circle to Search, like full-screen translation, and we’ve expanded availability to more Pixel and Samsung devices.Starting today, Circle to Search can now help students with homework, giving them a deeper understanding, not just an answer — directly from their phones and tablets. When students circle a prompt they’re stuck on, they’ll get step-by-step instructions to solve a range of physics and math1 word problems without leaving their digital info sheet or syllabus. Later this year, Circle to Search will be able to help solve even more complex problems involving symbolic formulas, diagrams, graphs and more. This is all possible due to our LearnLM effort to enhance our models and products for learning.Circle to Search is already available on more than 100 million devices today. With plans to bring the experience to more devices, we’re on track to double that by the end of the year.\nGemini will get even better at understanding context to assist you in getting things doneGemini on Android is a new kind of assistant that uses generative AI to help you be more creative and productive. This experience, which is integrated into Android, is getting even better at understanding the context of what’s on your screen and what app you’re using.Soon, you’ll be able to bring up Gemini's overlay on top of the app you're in to easily use Gemini in more ways. For example, you can drag and drop generated images into Gmail, Google Messages and other places, or tap “Ask this video” to find specific information in a YouTube video. If you have Gemini Advanced, you’ll also have the option to “Ask this PDF” to quickly get answers without having to scroll through multiple pages. This update will roll out to hundreds of millions of devices over the next few months.And we’ll continue to improve Gemini to give you more dynamic suggestions related to what’s on your screen.\nFull multimodal capabilities coming to Gemini NanoAndroid is the first mobile operating system that includes a built-in, on-device foundation model. With Gemini Nano, we’re able to bring experiences to you quickly and keep your information completely private to you. Starting with Pixel later this year, we’ll be introducing our latest model, Gemini Nano with Multimodality. This means your phone will not just be able to process text input but also understand more information in context like sights, sounds and spoken language.\nClearer descriptions with TalkBackLater this year, Gemini Nano’s multimodal capabilities are coming to TalkBack, helping people who experience blindness or low vision get richer and clearer descriptions of what’s happening in an image. On average, TalkBack users come across 90 unlabeled images per day. This update will help fill in missing information — whether it’s more details about what’s in a photo that family or friends sent or the style and cut of clothes when shopping online. Since Gemini Nano is on-device, these descriptions happen quickly and even work when there's no network connection.\nReceive alerts for suspected scams during phone callsAccording to a recent report, in a 12-month period, people lost more than $1 trillion to fraud. We’re testing a new feature that uses Gemini Nano to provide real-time alerts during a call if it detects conversation patterns commonly associated with scams. For example, you would receive an alert if a “bank representative” asks you to urgently transfer funds, make a payment with a gift card or requests personal information like card PINs or passwords, which are uncommon bank requests. This protection all happens on-device, so your conversation stays private to you. We’ll share more about this opt-in feature later this year.\nMore to come on AndroidWe’re just getting started with how on-device AI can change what your phone can do, and we’ll continue building Google AI into every part of the smartphone experience with Pixel, Samsung and more. If you’re a developer, check out the Android Developers blog to learn how you can build with our latest AI models and tools, like Gemini Nano and Gemini in Android Studio.\n10 updates coming to the Android ecosys…\nFrom Theft Detection Lock to casting on Rivian to Wear OS 5 updates, here’s what’s coming to Android 15 and its device ecosystem.\nHere’s a look at everything we announced at Google I/O 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Browse safely with real-time protection on Chrome",
    "link": "https://blog.google/products/chrome/google-chrome-safe-browsing-real-time/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNXBlbEJSZERKV1FqRnlUSEJrVFJDUkFSamNBaWdCTWdtVmNwQVRMU2VpeXdF=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-03-14T07:00:00.000Z",
    "time": "Mar 14",
    "articleType": "regular",
    "content": "Cybersecurity attacks are constantly evolving, and sometimes the difference between successfully detecting a threat or not is a matter of minutes. To keep up with the increasing pace of hackers, we’re bringing real-time, privacy-preserving URL protection to Google Safe Browsing for anyone using Chrome on desktop or iOS. Plus we’re introducing new password protections on Chrome for iOS as another way to help you safely navigate the web.Real-time protection through Safe BrowsingSafe Browsing already protects more than 5 billion devices worldwide, defending against phishing, malware, unwanted software and more. In fact, Safe Browsing assesses more than 10 billion URLs and files every day, showing more than 3 million user warnings for potential threats.Previously, the Standard protection mode of Safe Browsing used a list stored on your device to check if a site or file was known to be potentially dangerous. That list is updated every 30 to 60 minutes — but we’ve found that the average malicious site actually exists for less than 10 minutes.So now, the Standard protection mode for Chrome on desktop and iOS will check sites against Google’s server-side list of known bad sites in real time. If we suspect a site poses a risk to you or your device, you’ll see a warning with more information. By checking sites in real time, we expect to block 25% more phishing attempts.\nProceed with caution when you see this warning from Safe Browsing.\nThe new capability — also rolling out to Android later this month — uses encryption and other privacy-enhancing techniques to ensure that no one, including Google, knows what website you’re visiting. While this does require some additional horsepower from the browser, we’ve worked to make sure your experience remains smooth and speedy.If you want even more protection, you can always turn on Safe Browsing’s Enhanced Protection mode, which uses AI to block attacks, provides deep file scans and offers extra protection from malicious Chrome extensions.Updates to Password CheckupPassword Checkup on iOS also recently got an update: Now, in addition to flagging compromised passwords, it will flag weak and reused passwords, too. Chrome will display an alert whenever it detects an issue with a password you’ve entered, but you can check your passwords any time by visiting Safety Check in Chrome Settings.\nCheck weak and reused passwords with Password Checkup.\nLook out for these features on Chrome and stay tuned for more ways we’re helping you stay safe online.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Changes we’re making to Google Assistant",
    "link": "https://blog.google/products/assistant/google-assistant-update-january-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDJWbkZyTUhScE1HcHpSV0ZmVFJDb0FSaXJBaWdCTWdZcGxaS3ROUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-11T08:00:00.000Z",
    "time": "Jan 11",
    "articleType": "regular",
    "content": "We’re removing some underutilized features in Google Assistant to focus on delivering the best possible user experience.\nSince we introduced Google Assistant seven years ago, it’s been incredible to see how voice technology has transformed the way we get things done at home and work. Today, Assistant helps hundreds of millions of people around the world cross off their to-do lists.Over the years, we’ve made it even easier to accomplish your tasks with Assistant — thanks, in large part, to your feedback. And in order to keep improving your experience and build the best Assistant yet, we’re making a few changes to focus on quality and reliability — ultimately making it easier to use Assistant across devices.First, as we continue to make Google Assistant more helpful, we're prioritizing the experiences you love and investing in the underlying technology to make them even better — which means that some underutilized features will no longer be supported. You can find a list of these features, including suggestions of what you can use in place of them (where alternatives are available). Beginning on January 26, when you ask for one of these features, you may get a notification that it won’t be available after a certain date.We’re also bringing you a more consistent experience within the Google app: The microphone icon will now trigger Search results in response to your queries, which is its most popular use case. You can continue to activate Assistant as you always have: Just say “Hey Google” or long press on the home or power button (on select phones) on Android, or open the Google Assistant app on iOS. But you’ll no longer be able to use the microphone icon in the Search bar to complete actions like “turn on the lights” or “send a message.” This includes the microphone in the Pixel Search bar, which will now activate Voice Search instead of Assistant. And to ensure you have access to the best, most up-to-date version of Google Assistant, you’ll be prompted to upgrade the Google app if you’re using an older version (v12 and older).We know change can be disruptive, but we’ll do everything we can to make these transitions as smooth as possible. While Google Assistant has evolved a lot over the last seven years, one thing remains true: Our improvements are driven by your feedback. And we want to hear it. Just say, “Hey Google, send feedback” and share your comments with us.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "110 new languages are coming to Google Translate",
    "link": "https://blog.google/products/translate/google-translate-new-languages-2024/",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-06-27T07:00:00.000Z",
    "time": "Jun 27",
    "articleType": "regular",
    "content": "Google Translate breaks down language barriers to help people connect and better understand the world around them. We’re always applying the latest technologies so more people can access this tool: In 2022, we added 24 new languages using Zero-Shot Machine Translation, where a machine learning model learns to translate into another language without ever seeing an example. And we announced the 1,000 Languages Initiative, a commitment to build AI models that will support the 1,000 most spoken languages around the world.Now, we’re using AI to expand the variety of languages we support. Thanks to our PaLM 2 large language model, we’re rolling out 110 new languages to Google Translate, our largest expansion ever.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Meet Pixel 8a: The Google AI phone at an unbeatable value",
    "link": "https://blog.google/products/pixel/pixel-8a-launch/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXBaREZWYTA1WFYwcDVjM1U0VFJDMEFSaVlBaWdCTWdZQkVaUUdMUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-07T07:00:00.000Z",
    "time": "May 7",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Meet the Google TV network",
    "link": "https://blog.google/products/ads-commerce/meet-the-google-tv-network/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXFUVkJ1VDNBdGJrbEhjbFpPVFJDUkFSamNBaWdCTWdhZFk0aHNMUVE=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-06-07T07:00:00.000Z",
    "time": "Jun 7",
    "articleType": "regular",
    "content": "A new advertising network from Google TV, offering targeted, in-stream video inventory across more than 125 channels built into Google TV.\nConnected TV continues to see a rise in popularity, and advertisers are already taking advantage of YouTube’s reach of over 150 million monthly viewers in the living room.1 With the recent launch of the Google TV network, we’re now giving advertisers a way to reach additional viewers on the big screen. Google TV provides targeted, in-stream video inventory across more than 20 million monthly active Google TV and other Android TV OS devices.2Google TV powers TVs and streaming devices from top brands like Sony, Hisense, TCL, and Chromecast. Google TV makes it easy to watch movies, shows and live TV from across 10,000 apps and includes popular features like Google Assistant, smart home control, built-in channels, and personalized entertainment recommendations for viewers.3\nGoogle TV network provides targeted, in-stream video inventory across more than 125 channels built into Google TV\nThe Google TV network offers in-stream video inventory across more than 125 channels built into Google TV, including live sports, full-length TV shows, and movies. In the U.S., 60% of households now watch free, ad-supported streaming services and channels.4 And viewers of Google TV’s free channels spend an average of over 75 minutes per day watching5. Today, the Google TV network offers staple CTV ad formats, including non-skippable and 6-second bumpers ads. Stay tuned for even more ad formats in the future.\nGoogle TV network is available directly through Google Ads and Google Display & Video 360\nThe Google TV network is available directly through Google Ads and Google Display & Video 360, which lets you plan, buy, and measure your Google TV campaigns alongside your existing digital video campaigns. When you set up your video campaign, just select the Google TV network alongside YouTube to maximize your reach on the biggest screen on the home.\nGoogle AdsNew reporting and genAI tools to boost creative results\nSearch8 ways to keep up with the Olympic Games Paris 2024 on Google\nAndroid10 fun facts about emoji for World Emoji Day\nAndroid10 years ago, Android expanded to 3 new platforms\nAndroid4 Google updates coming to Samsung devices\nAnalyticsFour ways Google Analytics delivers actionable insights for your business",
    "favicon": "/favicon.ico"
  },
  {
    "title": "10 updates coming to the Android ecosystem",
    "link": "https://blog.google/products/android/android-15-google-io-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHdkSGx4T0d0WlR6ZHRjbEpUVFJDb0FSaXJBaWdCTWdhQmtZek90UVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-15T07:00:00.000Z",
    "time": "May 15",
    "articleType": "regular",
    "content": "Android 15 is here with a host of new features to enhance your privacy, security, and overall user experience. Create a separate, secure space for sensitive apps with Private Space, and thwart phone thieves with Theft Detection Lock. Get real-time protection against fraud apps with Google Play Protect's on-device AI. Experience more immersive AR content in Google Maps, and enjoy a wider selection of mobile and tablet apps in your car with Google built-in and Google Cast. Discover content faster with AI-based recommendations on Google TV, and improve your fitness training with Wear OS 5 updates. Stay connected with Fast Pair, which now supports tracking your accessory's battery life and locating missing devices.\nSummaries were generated by Google AI. Generative AI is experimental.\nAndroid is getting 10 new updates:\n- Private space to keep sensitive apps secure.\n- Theft Detection Lock to keep your data safe if your phone is stolen.\n- Real-time protection against fraud apps.\n- More ways to add items to Google Wallet, like taking a photo of passes.\n- More apps in the car, including games and video streaming.\n- AI-generated descriptions on Google TV to help you find what to watch.\n- Battery life optimizations and new fitness features for Wear OS watches.\n- Fast Pair to easily connect your Android devices and accessories.\n- Find My Device to track your missing items with Bluetooth tracker tags.\nSummaries were generated by Google AI. Generative AI is experimental.\nNew experiences bring greater theft protections, battery life optimizations on watches and entertainment to TVs, cars and more.\nMessage with RCS in Japan\nFast Pair and Find My Device\nYesterday, we shared how Android is reimagining the smartphone with AI at the core. Today, we’re introducing the second beta of Android 15 and sharing more ways we’re improving the OS to help you stay safe and get the most out of your device ecosystem.\nKeep apps out of sight with private spaceComing to Android 15 this year, private space lets you create a separate space with an extra layer of authentication, keeping your sensitive apps secure and away from prying eyes. It’s like a digital safe within your phone for the apps you don’t want others to easily access or see. For example, you can hide health or banking apps in your private space to keep your personal information for your eyes only. With direct OS integration, private space offers enhanced protection for sensitive apps, isolating their data and notifications from the rest of your phone. You can also set up a separate lock for private space and hide the existence of it altogether.\nThwart phone thieves with Theft Detection LockTheft Detection Lock is coming later this year and helps you keep your personal and financial data safe if your phone is ever snatched from you. This powerful new feature uses Google AI to sense if someone snatches your phone from your hand and tries to run, bike or drive away with it. If a theft motion is detected, it will be quickly locked down to keep your information out of the wrong hands.\nGet more real-time protection against fraud appsAlso later this year, Google Play Protect will use on-device AI to spot apps that might engage in fraud or phishing. This live threat detection will analyze how apps use sensitive permissions and interact with other apps to spot suspicious behavior in real-time. If Google Play Protect finds something that looks malicious, the app is sent to Google for additional review, and we'll warn users or disable the app if we confirm it’s conducting harmful activity. This is all done without collecting any personal data.\nAndroid’s theft protection features kee…\nAndroid rolls out theft protection features to safeguard your data before, during and after a theft incident.\nMessage with RCS in JapanOver a billion monthly active users with RCS enabled in Google Messages benefit from a more modern and secure messaging experience — with high-resolution photo and video sharing, improved group chats, end-to-end encryption and more. We’re working with KDDI and other partners to bring an updated messaging experience to Japan with RCS in Google Messages.\nAdd items to Google Wallet from a photoIn addition to saving digital versions of items that contain barcodes and QR codes to your Google Wallet, soon in the U.S. you can create a digital version of most passes that just contain text. Simply take a photo of everyday passes — like event tickets, library cards, auto insurance cards, gym membership cards and more — and create a digital version in your Google Wallet for quick access.\nStay entertained with more apps in the carCatch up on episodes of your favorite shows on Max and Peacock or play a game of Angry Birds on select cars with Google built-in — which is expanding to car models like the Acura ZDX, Ford Explorer and more. You’ll also be able to enjoy a rapidly growing selection of mobile and tablet apps in the car with our new developer program.In addition, Google Cast (formerly Chromecast built-in) is coming to cars with Android Automotive OS, starting with Rivian in the coming months. You can easily cast video content from your phone or tablet to the car, opening up even more entertainment options.Find new content with AI on Google TVWith over 220 million active devices1 globally and a fast growing user base, Google TV helps you discover content faster with AI-based recommendations – so you can spend more time watching and less time searching. Now with the Gemini model, it’s even easier to pick what to watch with AI-generated descriptions on the homescreen, personalized for you based on your genre and actor preferences. AI-generated descriptions will also fill in missing or untranslated descriptions for movies and shows so you aren’t left guessing.Improve your training with Wear OS 5 updatesWith watch launches from Pixel, Samsung and more, Wear OS grew its user base by 40% in 2023 and has users in over 160 countries and regions. Now, Wear OS has expanded to more brands including OnePlus, OPPO and Xiaomi.Later this year, battery life optimizations2 are coming to watches with Wear OS 5. For example, running an outdoor marathon will consume up to 20% less power when compared to watches with Wear OS 4. And your fitness apps will be able to help improve your performance with the option to support more data types like ground contact time, stride length and vertical oscillation.\nExperience Google AI in even more ways …\nHere’s more ways you can experience Google AI on Android. Learn how on-device AI is changing what your phone can do.\nFor developers building Android apps for cars, large screens, wearables and TVs, learn about the new tools and features we announced at I/O on our Developer blog.\nHere’s a look at everything we announced at Google I/O 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Passkeys, Cross-Account Protection and new ways we’re protecting your accounts",
    "link": "https://blog.google/technology/safety-security/google-passkeys-update-april-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVNjRU5tYVdsRlFUYzJORmQwVFJDb0FSaXJBaWdCTWdhZE00aHhuUWs=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-02T07:00:00.000Z",
    "time": "May 2",
    "articleType": "regular",
    "content": "For World Password Day, we’re sharing updates to passkeys across our products and sharing more ways we’re keeping people safe online.\nPasswords are often at the core of today’s major cybersecurity issues, which is why we’ve continued to create new authentication technology over the years. In 2022, for World Password Day, we launched passkeys. Today, we’re proud to announce that they have since been used to authenticate users more than 1 billion times across over 400 million Google Accounts.We’re also excited to announce the expansion of our Cross-Account Protection program and new updates to passkeys.Expanding Cross-Account ProtectionWe’re expanding Cross-Account Protection — our program for sharing security notifications, in a privacy-preserving way, with other companies that run the non-Google apps and services you use. This helps prevent cybercriminals from gaining a foothold in one of your accounts and using it to infiltrate others. We are currently protecting 2.4 billion accounts across 3.4 million apps and sites, and are growing our collaborations across the industry to keep billions of users safer online. It’s built on the Shared Signals Framework, which we helped create and launch in 2019, and in the coming year we’re expanding our partnerships and support for this program. Stay tuned for which of your favorite apps and services begin using Cross-Account Protection.Passkeys reaches a milestone — and what’s nextIn less than a year, passkeys have been used to authenticate people more than 1 billion times across over 400 million Google Accounts. Passkeys are easy to use and phishing resistant, only relying on a fingerprint, face scan or a pin making them 50% faster than passwords. In fact, on a daily basis passkeys are already used for authentication on Google Accounts more often than legacy forms of 2SV, such as SMS one-time passwords (OTPs) and app based OTPs (such as Authenticator apps) combined.Passkeys for high risk users. We’ll soon support the use of passkeys to enroll in our strongest security offering, the Advanced Protection Program (APP). APP safeguards users who are at the highest risk of targeted attacks, including campaign workers and candidates, journalists, human rights workers, and more. APP traditionally required using hardware security keys as a second factor; but soon users can enroll in APP with any passkey in addition to their hardware security keys; or use their passkeys as a sole factor or along with a password. In a critical election year, we’ll be bringing this feature to our users who need it most, and continue to work with experts like Defending Digital Campaigns, the International Foundation for Electoral Systems, Asia Centre, Internews and Possible to help protect global high risk users.More choice in where you store your passkeys. We are pleased to see independent password manager vendors, such as 1Password and Dashlane, now leveraging the passkeys management APIs on Android and other operating systems. This important milestone, together with the ability to store passkeys on security keys, will give users more control.\nGrowing industry support for passkeys. Since we launched passkeys we’ve seen our list of partners grow, strengthening security for people across platforms. In just the last 12 months, Amazon, 1Password, Dashlane, Docusign, Kayak, Mercari, Shopify and Yahoo! JAPAN have started rolling out passkeys, joining early adopters like eBay, Uber, PayPal and Whatsapp. In fact, Dashlane is seeing a 70% increase in conversion with passkeys and Kayak users are signing in 50% faster than before.\nCreate a passkey for your Google Account today to benefit from these new protections and visit myaccount.google.com/safer to learn all the ways you’re Safer with Google.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Bard becomes Gemini: Try Ultra 1.0 and a new mobile app today",
    "link": "https://blog.google/products/gemini/bard-gemini-advanced-app/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXJkMnhoUkRscFNrb3hORE13VFJDUkFSamNBaWdCTWdZTnNwQkdQZ1k=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-08T08:00:00.000Z",
    "time": "Feb 8",
    "articleType": "regular",
    "content": "Bard is now known as Gemini, and we’re rolling out a mobile app and Gemini Advanced with Ultra 1.0.\nSince we launched Bard last year, people all over the world have used it to collaborate with AI in a completely new way — to prepare for job interviews, debug code, brainstorm new business ideas or, as we announced last week, create captivating images.Our mission with Bard has always been to give you direct access to our AI models, and Gemini represents our most capable family of models. To reflect this, Bard will now simply be known as Gemini.You can already chat with Gemini with our Pro 1.0 model in over 40 languages and more than 230 countries and territories. And now, we’re bringing you two new experiences — Gemini Advanced and a mobile app — to help you easily collaborate with the best of Google AI.\nAccess our most capable AI model with Gemini AdvancedToday we’re launching Gemini Advanced — a new experience that gives you access to Ultra 1.0, our largest and most capable state-of-the-art AI model. In blind evaluations with our third-party raters, Gemini Advanced with Ultra 1.0 is now the most preferred chatbot compared to leading alternatives.With our Ultra 1.0 model, Gemini Advanced is far more capable at highly complex tasks like coding, logical reasoning, following nuanced instructions and collaborating on creative projects. Gemini Advanced not only allows you to have longer, more detailed conversations; it also better understands the context from your previous prompts. For example:Gemini Advanced can be your personal tutor — creating step-by-step instructions, sample quizzes or back-and-forth discussions tailored to your learning style.It can help you with more advanced coding scenarios, serving as a sounding board for ideas and helping you evaluate different coding approaches.It can help digital creators go from idea to creation by generating fresh content, analyzing recent trends and brainstorming improved ways to grow their audiences.This first version of Gemini Advanced reflects our current advances in AI reasoning and will continue to improve. As we add new and exclusive features, Gemini Advanced users will have access to expanded multimodal capabilities, more interactive coding features, deeper data analysis capabilities and more. Gemini Advanced is available today in more than 150 countries and territories in English, and we'll expand it to more languages over time.\nGemini Advanced is available as part of our brand new Google One AI Premium Plan for $19.99/month, starting with a two-month trial at no cost. This plan gives you the best of Google AI and our latest advancements, along with all the benefits of the existing Google One Premium plan, such as 2TB of storage. In addition, AI Premium subscribers will soon be able to use Gemini in Gmail, Docs, Slides, Sheets and more (formerly known as Duet AI).We continue to take a bold and responsible approach to bringing this technology to the world. And, to mitigate issues like unsafe content or bias, we’ve built safety into our products in accordance with our AI Principles. Before launching Gemini Advanced, we conducted extensive trust and safety checks, including external red-teaming. We further refined the underlying model using fine-tuning and reinforcement learning, based on human feedback. You can find more detail in our updated Gemini Technical Report.Easily use Gemini on your phoneWe’ve heard that you want an easier way to access Gemini on your phone. So today we’re starting to roll out a new mobile experience for Gemini and Gemini Advanced with a new app on Android and in the Google app on iOS.With Gemini on your phone, you can type, talk or add an image for all kinds of help while you’re on the go: You can take a picture of your flat tire and ask for instructions, generate a custom image for your dinner party invitation or ask for help writing a difficult text message. It’s an important first step in building a true AI assistant — one that is conversational, multimodal and helpful.AndroidOn Android, Gemini is a new kind of assistant that uses generative AI to collaborate with you and help you get things done.If you download the Gemini app or opt in through Google Assistant, you'll be able to access it from the app or anywhere else you normally activate Google Assistant — hitting the power button or corner swiping on select phones, or saying “Hey Google.” This will enable a new overlay experience that offers easy access to Gemini as well as contextual help right on your screen — so you can, for instance, generate a caption for a picture you've just taken or ask questions about an article you're reading. Many Google Assistant voice features will be available through the Gemini app — including setting timers, making calls and controlling your smart home devices — and we’re working to support more in the future.\niOSOn iOS, we’ll roll out access to Gemini right from the Google app in the coming weeks. Just tap the Gemini toggle and chat with Gemini to supercharge your creativity, create custom images, get help writing social posts and even plan a date night right from the Google app. We can’t wait for you to try it.\nGemini is rolling out on Android and iOS phones in the U.S. in English starting today, and will be fully available in the coming weeks. Starting next week, you’ll be able to access it in more locations in English, and in Japanese and Korean, with more countries and languages coming soon.Try out the latest updates — and share your feedback to help us make your experience even better.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "What we announced at CES 2024",
    "link": "https://blog.google/products/android/ces-2024-android-updates/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDRka3BZWnpjeU0zSk5OakZEVFJDb0FSaXJBaWdCTWdZQllKQ1BwUWc=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-09T08:00:00.000Z",
    "time": "Jan 9",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google I/O 2024: An I/O for a new generation",
    "link": "https://blog.google/inside-google/message-ceo/google-io-2024-keynote-sundar-pichai/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXNSRFEyTVZaa1RuQldkR1JmVFJDU0FSalpBaWdCTWdZWkFKQ1dyQWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-14T07:00:00.000Z",
    "time": "May 14",
    "articleType": "regular",
    "content": "Editor’s note: Below is an edited transcript of Sundar Pichai’s remarks at I/O 2024, adapted to include more of what was announced on stage. See all the announcements in our collection.Google is fully in our Gemini era.Before we get into it, I want to reflect on this moment we’re in. We’ve been investing in AI for more than a decade — and innovating at every layer of the stack: research, product, infrastructure, and we’re going to talk about it all today.Still, we are in the early days of the AI platform shift. We see so much opportunity ahead, for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let’s get started.\nThe Gemini eraA year ago on the I/O stage we first shared our plans for Gemini: a frontier model built to be natively multimodal from the beginning, that could reason across text, images, video, code, and more. It marks a big step in turning any input into any output — an “I/O” for a new generation.Since then, we introduced the first Gemini models, our most capable yet. They demonstrated state-of-the-art performance on every multimodal benchmark. Two months later, we introduced Gemini 1.5 Pro, delivering a big breakthrough in long context. It can run 1 million tokens in production, consistently, more than any other large-scale foundation model yet.We want everyone to benefit from what Gemini can do. So we’ve worked quickly to share these advances with all of you. Today more than 1.5 million developers use Gemini models across our tools. You’re using it to debug code, get new insights, and build the next generation of AI applications.We’ve also been bringing Gemini’s breakthrough capabilities across our products, in powerful ways. We’ll show examples today across Search, Photos, Workspace, Android and more.\nProduct progressToday, all of our 2-billion user products use Gemini.And we’ve introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app, now available on Android and iOS. And through Gemini Advanced which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum.\nExpanding AI Overviews in SearchOne of the most exciting transformations with Gemini has been in Google Search.In the past year, we’ve answered billions of queries as part of our Search Generative Experience. People are using it to Search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer.\nWe’ve been testing this experience outside of Labs. And we’re encouraged to see not only an increase in Search usage, but also an increase in user satisfaction.I’m excited to announce that we’ll begin launching this fully-revamped experience, AI Overviews, to everyone in the U.S. this week. And we’ll bring it to more countries soon.There’s so much innovation happening in Search. Thanks to Gemini we can create much more powerful search experiences, including within our products.\nIntroducing Ask PhotosOne example is Google Photos, which we launched almost nine years ago. Since then, people have used it to organize their most important memories. Today that amounts to more than 6 billion photos and videos uploaded every single day.And people love using Photos to search across their life. With Gemini we’re making that a whole lot easier.Say you’re paying at the parking station, but you can't recall your license plate number. Before, you could search Photos for keywords and then scroll through years’ worth of photos, looking for license plates. Now, you can simply ask Photos. It knows the cars that appear often, it triangulates which one is yours, and tells you the license plate number.And Ask Photos can help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucia’s early milestones. Now, you can ask Photos: “When did Lucia learn to swim?”And you can follow up with something even more complex: “Show me how Lucia’s swimming has progressed.”Here, Gemini goes beyond a simple search, recognizing different contexts — from doing laps in the pool, to snorkeling in the ocean, to the text and dates on her swimming certificates. And Photos packages it all up together in a summary, so you can really take it all in, and relive amazing memories all over again. We’re rolling out Ask Photos this summer, with more capabilities to come.\nUnlocking more knowledge with multimodality and long contextUnlocking knowledge across formats is why we built Gemini to be multimodal from the ground up. It’s one model, with all the modalities built in. So not only does it understand each type of input — and finds connections between them.Multimodality radically expands the questions we can ask, and the answers we’ll get back. Long context takes this a step further, enabling us to bring in even more information: hundreds of pages of text, hours of audio or an hour of video, entire code repos…or, if you want, roughly 96 Cheesecake Factory menus.For that many menus, you’d need a one million token context window, now possible with Gemini 1.5 Pro. Developers have been using it in super interesting ways.\nWe’ve been rolling out Gemini 1.5 Pro with long context in preview over the last few months. We’ve made a series of quality improvements across translation, coding and reasoning. You’ll see these updates reflected in the model starting today.Now I’m excited to announce that we’re bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition, today Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced. This can be used across 35 languages.\nExpanding to 2 million tokens in private previewOne million tokens is opening up entirely new possibilities. It’s exciting, but I think we can push ourselves even further.So today, we’re expanding the context window to 2 million tokens, and making it available for developers in private preview.It's amazing to look back and see just how much progress we've made in a few months. And this represents the next step on our journey towards the ultimate goal of infinite context.\nBringing Gemini 1.5 Pro to WorkspaceSo far, we’ve talked about two technical advances: multimodality and long context. Each is powerful on its own. But together, they unlock deeper capabilities, and more intelligence.This comes to life with Google Workspace.People are always searching their emails in Gmail. We’re working to make it much more powerful with Gemini. So for example, as a parent, you want to stay informed about everything that’s going on with your child’s school. Gemini can help you keep up.Now we can ask Gemini to summarize all recent emails from the school. In the background, it’s identifying relevant emails, and even analyzing attachments, like PDFs. You get a summary of the key points and action items. Maybe you were traveling this week and couldn’t make the PTA meeting. The recording of the meeting is an hour long. If it’s from Google Meet, you can ask Gemini to give you the highlights. There’s a parents group looking for volunteers, and you’re free that day. So of course, Gemini can draft a reply.There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. Aparna shares more.\nAudio outputs in NotebookLMWe just looked at an example with text outputs. But with a multimodal model, we can do so much more.We’re making progress here, with more to come. Audio Overviews in NotebookLM shows the progress. It uses Gemini 1.5 Pro to take your source materials and generate a personalized and interactive audio conversation.This is the opportunity with multimodality. Soon you’ll be able to mix and match inputs and outputs. This is what we mean when we say it’s an I/O for a new generation. But what if we could go even further?\nWhat it means for our missionThe power of Gemini — with multimodality, long context and agents — brings us closer to our ultimate goal: making AI helpful for everyone.We see this as how we’ll make the most progress against our mission: Organizing the world’s information across every input, making it accessible via any output, and combining the world’s information, with the information in YOUR world, in a way that’s truly useful for you.\nBreaking new groundTo realize the full potential of AI, we’ll need to break new ground. The Google DeepMind team has been hard at work on this.We’ve seen so much excitement around 1.5 Pro and its long context window. But we also heard from developers that they wanted something faster and more cost effective. So tomorrow, we’re introducing Gemini 1.5 Flash, a lighter-weight model built for scale. It’s optimized for tasks where low latency and cost matter most. 1.5 Flash will be available in AI Studio and Vertex AI on Tuesday.Looking further ahead, we’ve always wanted to build a universal agent that will be useful in everyday life. Project Astra, shows multimodal understanding and real-time conversational capabilities.\nWe’ve also made progress on video and image generation with Veo and Imagen 3, and introduced Gemma 2.0, our next generation of open models for responsible AI innovation. Read more from Demis Hassabis.\nInfrastructure for the AI era: Introducing TrilliumTraining state-of-the-art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year, it increases tenfold.Google was built for this. For 25 years, we’ve invested in world-class technical infrastructure. From the cutting-edge hardware that powers Search, to our custom tensor processing units that power our AI advances.Gemini was trained and served entirely on our fourth and fifth generation TPUs. And other leading AI companies, including Anthropic, have trained their models on TPUs as well.Today, we’re excited to announce our 6th generation of TPUs, called Trillium. Trillium is our most performant and most efficient TPU to date, delivering a 4.7x improvement in compute performance per chip over the previous generation, TPU v5e.We’ll make Trillium available to our Cloud customers in late 2024.Alongside our TPUs, we’re proud to offer CPUs and GPUs to support any workload. That includes the new Axion processors we announced last month, our first custom Arm-based CPU that delivers industry-leading performance and energy efficiency.We’re also proud to be one of the first Cloud providers to offer Nvidia’s cutting-edge Blackwell GPUs, available in early 2025. We’re fortunate to have a longstanding partnership with NVIDIA, and are excited to bring Blackwell’s breakthrough capabilities to our customers.Chips are a foundational part of our integrated end-to-end system. From performance-optimized hardware and open software to flexible consumption models. This all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture.Businesses and developers are using it to tackle more complex challenges, with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers.We’ve been doing this for nearly a decade, long before it became state-of-the-art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing — that’s close to 70 times the capacity of any other fleet.Underlying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber: over 10 times (!) the reach of the next leading cloud provider.We will keep making the investments necessary to advance AI innovation and deliver state-of-the-art capabilities.\nThe most exciting chapter of Search yetOne of our greatest areas of investment and innovation is in our founding product, Search. 25 years ago we created Search to help people make sense of the waves of information moving online.With each platform shift, we’ve delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers — using better context, location awareness, and real-time information. With advances in natural language understanding and computer vision, we enabled new ways to search, with a voice, or a hum to find your new favorite song; or with an image of that flower you saw on your walk. And now you can even Circle to Search those cool new shoes you might want to buy. Go for it, you can always return them!Of course, Search in the Gemini Era will take this to a whole new level, combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, and our decades of experience connecting you to the richness of the web. The result is a product that does the work for you.Google Search is generative AI at the scale of human curiosity. And it’s our most exciting chapter of Search yet. Read more about the Gemini era of Search from Liz Reid.\nMore intelligent Gemini experiencesGemini is more than a chatbot; it’s designed to be your personal, helpful assistant that can help you tackle complex tasks and take actions on your behalf.Interacting with Gemini should feel conversational and intuitive. So we’re announcing a new Gemini experience that brings us closer to that vision called Live that allows you to have an in-depth conversation with Gemini using your voice. We’ll also be bringing 2M tokens to Gemini Advanced later this year, making it possible to upload and analyze super dense files like video and long code. Sissie Hsiao shares more.\nGemini on AndroidWith billions of Android users worldwide, we're excited to integrate Gemini more deeply into the user experience. As your new AI assistant, Gemini is here to help you anytime, anywhere. And we've incorporated Gemini models into Android, including our latest on-device model: Gemini Nano with Multimodality, which processes text, images, audio, and speech to unlock new experiences while keeping information private on your device. Sameer Samat shares the Android news here.\nOur responsible approach to AIWe continue to approach the AI opportunity boldly, with a sense of excitement. We’re also making sure we do it responsibly. We’re developing a cutting-edge technique we call AI-assisted red teaming, that draws on Google DeepMind's gaming breakthroughs like AlphaGo, to improve our models. Plus, we’ve expanded SynthID, our watermarking tool that makes AI-generated content easier to identify, to two new modalities: text and video. James Manyika shares more.\nCreating the future togetherAll of this shows the important progress as we take a bold and responsible approach to making AI helpful for everyone.We’ve been AI-first in our approach for a long time. Our decades of research leadership have pioneered many of the modern breakthroughs that power AI progress, for us and for the industry. On top of that we have:World-leading infrastructure built for the AI eraCutting-edge innovation in Search, now powered by GeminiProducts that help at extraordinary scale — including 15 products with half a billion usersAnd platforms that enable everyone — partners, customers, creators, and all of you — to invent the future.This progress is only possible because of our incredible developer community. You are making it real, through the experiences and applications you build every day. So, to everyone here in Shoreline and the millions more watching around the world, here’s to the possibilities ahead and creating them together.\nHere’s a look at everything we announced at Google I/O 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Android’s theft protection features keep your device and data safe",
    "link": "https://blog.google/products/android/android-theft-protection/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTFXR3BzV0hKbWJ6ZHpRVVZFVFJDbkFSaXRBaWdCTWdZWlU1RExyUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-15T07:00:00.000Z",
    "time": "May 15",
    "articleType": "regular",
    "content": "Sorry, your browser doesn't support embedded videos, but don't worry, you can\nand watch it with your favorite video player!\nSmartphones help us with everyday tasks like online banking, storing sensitive information, taking photos of our friends and families and quickly paying for stuff. While our phones make our lives easier, they also contain a lot of valuable information which makes these devices a target for people who might want to get their hands on our data.To help keep your device and your data safe before, during and after a theft attempt, we’re introducing a new suite of advanced theft protection features. These features will be rolling out through Google Play services updates later this year to the billions of devices running Android 10+, with some features available in Android 15.\n1. Improved device and data protection to deter theft before it happensWe're working to strengthen your device's security against theft with new and improved protection features that will make thieves think twice about trying.Factory reset upgrade prevents a reset by a thief. For some criminals, the goal is to quickly reset your stolen device and resell it. We’re making it more difficult to do that with an upgrade to Android’s factory reset protection. With this upgrade, if a thief forces a reset of the stolen device, they’re not able to set it up again without knowing your device or Google account credentials. This renders a stolen device unsellable, reducing incentives for phone theft.Private space hides your sensitive apps. Some thieves just want the device, but many aim to extract valuable data and transfer funds from your phone that can be worth much more than your hardware. Private space is a new feature that lets you create a separate area in your phone that you can hide and lock with a separate PIN, giving you additional security for apps that might contain sensitive data, like health or financial information.More steps for changing sensitive device settings to protect your data. Disabling Find My Device or extending screen timeout now requires your PIN, password or biometric authentication, adding an extra layer of security preventing criminals who got a hold of your device from keeping it unlocked or untrackable online.Increased authentication to protect you in case your PIN is known by a thief. When enabled, our new enhanced authentication will require biometrics for accessing and changing critical Google account and device settings, like changing your PIN, disabling theft protection or accessing Passkeys, from an untrusted location.Factory reset protection updates and private space will be released as part of Android 15. Enhanced authentication protections will be released to select devices later this year.\nAndroid will protect access to sensitive settings by requiring users to enter their PIN or biometrics.\n2. Automatic protection the moment your phone is stolenHaving a device stolen is unexpected and stressful and it can be hard to react quickly at the moment it happens. That’s why we created features that can automatically recognize suspicious signals and proactively protect your data on the device.Automatic AI-powered screen lock for when your phone is snatched. Theft Detection Lock is a powerful new feature that uses Google AI to sense if someone snatches your phone from your hand and tries to run, bike or drive away. If a common motion associated with theft is detected, your phone screen quickly locks – which helps keep thieves from easily accessing your data.Added protection when a thief has your device. If a thief tries to disconnect your phone for prolonged periods of time, Offline Device Lock automatically locks your screen to help protect your data even when your device is off the grid. Android can also recognize other signs that your device may be in the wrong hands. For example, it will lock your device screen when excessive failed authentication attempts are made.Theft Detection Lock and Offline Device Lock will be available to Android 10+ devices through a Google Play services update later this year.\nAndroid uses AI to lock the device if the phone detects motion that could indicate theft.\n3. Lock your device and act quickly after your phone is stolenFind My Device already lets you remotely lock or wipe a lost or stolen phone and you can now mark it as lost for easier tracking. But many users are shocked and stressed after a phone goes missing and can’t recall their Google account password to access Find My Device.Remote Lock feature throws you a lifeline if your phone is already gone. You'll be able to lock the screen of your phone with just your phone number and a quick security challenge using any device. This buys you time to recover your account details and access additional helpful options in Find My Device, including sending a full factory reset command to completely wipe the device.Remote Lock will be available to Android 10+ devices through a Google Play services update later this year. Find My Device is available on Android 5+ devices.\nRemote Lock lets you remotely lock your device screen quickly\n10 updates coming to the Android ecosys…\nFrom Theft Detection Lock to casting on Rivian to Wear OS 5 updates, here’s what’s coming to Android 15 and its device ecosystem.\nHere’s a look at everything we announced at Google I/O 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Introducing Gemini: our largest and most capable AI model",
    "link": "https://blog.google/technology/ai/google-gemini-ai/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNWZhV3B2VDIxeFJITmZlazlpVFJDb0FSaXNBaWdCTWdtUlVwSVJ0YWFLaHdJ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-12-06T08:00:00.000Z",
    "time": "Dec 6, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "New ways we’re tackling spammy, low-quality content on Search",
    "link": "https://blog.google/products/search/google-search-update-march-2024/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNHlkVmxqYkVGSlNFeEtXSEEwVFJDUkFSamNBaWdCTWdrRkFJU3VoZXZBTVFF=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-03-05T08:00:00.000Z",
    "time": "Mar 5",
    "articleType": "regular",
    "content": "Every day, people turn to Search to find the best of what the web has to offer. We’ve long had policies and automated systems to fight against spammers, and we work to address emerging tactics that look to game our results with low-quality content. We regularly update those policies and systems to effectively tackle these trends so we can continue delivering useful content and connecting people with high-quality websites.Today we’re announcing key changes we’re making to improve the quality of Search and the helpfulness of your results:Improved quality ranking: We’re making algorithmic enhancements to our core ranking systems to ensure we surface the most helpful information on the web and reduce unoriginal content in search results.New and improved spam policies: We’re updating our spam policies to keep the lowest-quality content out of Search, like expired websites repurposed as spam repositories by new owners and obituary spam.Reducing low-quality, unoriginal resultsIn 2022, we began tuning our ranking systems to reduce unhelpful, unoriginal content on Search and keep it at very low levels. We're bringing what we learned from that work into the March 2024 core update.This update involves refining some of our core ranking systems to help us better understand if webpages are unhelpful, have a poor user experience or feel like they were created for search engines instead of people. This could include sites created primarily to match very specific search queries.We believe these updates will reduce the amount of low-quality content on Search and send more traffic to helpful and high-quality sites. Based on our evaluations, we expect that the combination of this update and our previous efforts will collectively reduce low-quality, unoriginal content in search results by 40%.Update April 26, 2024: As of April 19, we’ve completed the rollout of these changes. You’ll now see 45% less low-quality, unoriginal content in search results versus the 40% improvement we expected across this work.Keeping more spam out of your resultsFor decades, we’ve relied on advanced spam-fighting systems and spam policies to prevent the lowest-quality content from appearing in search results — and that work continues.We’re making several updates to our spam policies to better address new and evolving abusive practices that lead to unoriginal, low-quality content showing up on Search. We’ll take action on more types of these manipulative behaviors starting today. While our ranking systems keep many types of low-quality content from ranking highly on Search, these updates allow us to take more targeted action under our spam policies.Scaled content abuseWe’ve long had a policy against using automation to generate low-quality or unoriginal content at scale with the goal of manipulating search rankings. This policy was originally designed to address instances of content being generated at scale where it was clear that automation was involved.Today, scaled content creation methods are more sophisticated, and whether content is created purely through automation isn't always as clear. To better address these techniques, we’re strengthening our policy to focus on this abusive behavior — producing content at scale to boost search ranking — whether automation, humans or a combination are involved. This will allow us to take action on more types of content with little to no value created at scale, like pages that pretend to have answers to popular searches but fail to deliver helpful content.Site reputation abuseSometimes, websites that have their own great content may also host low-quality content provided by third parties with the goal of capitalizing on the hosting site's strong reputation. For example, a third party might publish payday loan reviews on a trusted educational website to gain ranking benefits from the site. Such content ranking highly on Search can confuse or mislead visitors who may have vastly different expectations for the content on a given website.We’ll now consider very low-value, third-party content produced primarily for ranking purposes and without close oversight of a website owner to be spam. We're publishing this policy two months in advance of enforcement on May 5, to give site owners time to make any needed changes.Expired domain abuseOccasionally, expired domains are purchased and repurposed with the primary intention of boosting search ranking of low-quality or unoriginal content. This can mislead users into thinking the new content is part of the older site, which may not be the case. Expired domains that are purchased and repurposed with the intention of boosting the search ranking of low-quality content are now considered spam.Search helps people with billions of questions every day, but there will always be areas where we can improve. We’ll continue to work hard at keeping low-quality content on Search to low levels, and showing more information created to help people.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "The next chapter of our Gemini era",
    "link": "https://blog.google/technology/ai/google-gemini-update-sundar-pichai-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHpiWFJQYzNOeFNGbEpORGN3VFJDb0FSaXJBaWdCTWdhTmtwSUpOUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-08T08:00:00.000Z",
    "time": "Feb 8",
    "articleType": "regular",
    "content": "Sorry, your browser doesn't support embedded videos, but don't worry, you can\nand watch it with your favorite video player!\nFor years, we’ve been investing deeply in AI as the single best way to improve Search and all of our products. We’re excited by the progress, for example with our Search Generative Experience, or SGE, which you can try in Search Labs. AI is also now central to two businesses that have grown rapidly in recent years: our Cloud and Workspace services and our popular subscription service Google One, which is just about to cross 100 million subscribers.A new state of the artIn December, we took a significant step on our journey to make AI more helpful for everyone with the start of the Gemini era, setting a new state of the art across a wide range of text, image, audio, and video benchmarks. However, Gemini is evolving to be more than just the models. It supports an entire ecosystem — from the products that billions of people use every day, to the APIs and platforms helping developers and businesses innovate.The largest model Ultra 1.0 is the first to outperform human experts on MMLU (massive multitask language understanding), which uses a combination of 57 subjects — including math, physics, history, law, medicine and ethics — to test knowledge and problem-solving abilities.Today we’re taking our next step and bringing Ultra to our products and the world.Introducing Gemini AdvancedBard has been the best way for people to directly experience our most capable models. To reflect the advanced tech at its core, Bard will now simply be called Gemini. It’s available in 40 languages on the web, and is coming to a new Gemini app on Android and on the Google app on iOS.The version with Ultra will be called Gemini Advanced, a new experience far more capable at reasoning, following instructions, coding, and creative collaboration. For example, it can be a personal tutor, tailored to your learning style. Or it can be a creative partner, helping you plan a content strategy or build a business plan. You can read more in this post.You can start using Gemini Advanced by subscribing to the new Google One AI Premium plan, which offers the best of Google’s AI features in a single place. This premium plan builds off the popular Google One service offering expanded storage and exclusive product features.\nBringing Gemini’s capabilities to more productsGemini models are also coming to products that people and businesses use every day, including Workspace and Google Cloud:Workspace: Already, more than 1 million people are using features like Help me write to enhance their productivity and creativity through Duet AI. Duet AI will become Gemini for Workspace, and soon consumers with the Google One AI Premium plan can use Gemini in Gmail, Docs, Sheets, Slides and Meet.Google Cloud: For Cloud customers, Duet AI will also become Gemini in the coming weeks. Gemini will help companies boost productivity, developers code faster, and organizations to protect themselves from cyber attacks, along with countless other benefits.More to comeDevelopers have been fundamental to every major technology shift and will play an equally important role in the Gemini ecosystem. Hundreds of thousands of developers and businesses have already been building with Gemini models. While today is about Gemini Advanced and its new capabilities, next week we'll share more details on what's coming for developers and Cloud customers.These latest updates reflect how we’re approaching innovation boldly, and advancing and deploying this technology responsibly. And we’re already well underway training the next iteration of our Gemini models — so stay tuned for more!",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Generative AI in Search expands to more than 120 new countries and territories",
    "link": "https://blog.google/products/search/google-search-generative-ai-international-expansion/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUtTR3hJY25Bd1RXMVZXbTF1VFJDUkFSamNBaWdCTWdhSk1ZeldIQWs=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-08T08:00:00.000Z",
    "time": "Nov 8, 2023",
    "articleType": "regular",
    "content": "Search Generative Experience (SGE) gets its largest international expansion yet, adding support for four new languages and bringing you helpful new upgrades, too.\nThis year, we’ve been testing how generative AI in Search can help people find what they’re looking for in new, faster ways. With Search Generative Experience (SGE), you can get AI-powered overviews that bring together the most helpful and relevant information available for your search.Already, generative AI in Search is allowing us to serve a wider range of information needs, including those that benefit from multiple perspectives. With SGE, we’re showing more links, and links to a wider range of sources on the results page, creating new opportunities for content to be discovered.As we’ve continually improved the experience, we’ve also expanded internationally beyond the United States with recent launches in India and Japan. So far, the vast majority of feedback in all three countries has been positive, and people are finding generative AI particularly useful for complex questions they wouldn’t typically think to search.Now, in our largest global expansion yet, we’re bringing generative AI in Search to more than 120 new countries and territories, along with support for four new languages. And we’re introducing a few upgrades, launching first in the U.S., that will make SGE more interactive as you search – including easier follow-ups, AI-powered translation help, and more definitions for topics like coding.Launching in more than 120 new countries and territories, plus four new languagesStarting today, we’re bringing Search Labs and SGE in English to more than 120 new countries and territories around the world, including Mexico, Brazil, South Korea, Indonesia, Nigeria, Kenya and South Africa.As part of this expansion, we’re also enabling four new languages for everyone using SGE: Spanish, Portuguese, Korean and Indonesian. So if, for example, you’re a Spanish speaker in the U.S., you can now use generative AI in Search with your preferred language.\nSearch Labs is a new way for you to test early-stage experiments on Search, available on the Google app (Android and iOS) and on Chrome desktop. Once you’re enrolled in Search Labs, just enable the SGE experiment to get started. In new countries, access via Chrome desktop is available today, and access through the Google app will be enabled over the coming week.Testing an easier way to ask follow-up questionsWe’ve learned from testing generative AI in Search that people appreciate the ability to ask follow-up questions, because it’s a more natural way to seek information or dig deeper on a topic. Now, we’re experimenting with a new way for you to ask follow-up questions directly from the search results page. As you continue to explore a topic, you can easily see your prior questions and search results, including Search ads in dedicated ad slots throughout the page.\nMaybe you searched for “how to run with my beagle” and now you want to ask “how about hiking.” Just tap to enter your question and let your curiosity lead the way. This update will start to roll out over the coming weeks, starting first in English in the United States, and we will continue to iterate on this experience.Adding more context to your translationsSometimes, translating from one language to another involves a bit of guesswork. For instance, in many languages, a single word can have multiple meanings that standard translation tools won’t necessarily detect. Let’s say you want to translate “is there a tie?” from English to Spanish. It seems straightforward, but it’s possible that the word tie could refer to either a piece of clothing or a contest with no winner.Fortunately, generative AI in Search can help you avoid that kind of ambiguity. Soon, if you ask Search to translate a phrase where certain words could have more than one possible meaning, you’ll see those terms underlined. Tap any of those words and you can indicate the specific meaning that reflects what you want to say. This option may also appear when you need to specify the gender for a particular word.\nThis AI-powered translation capability is coming soon to the United States for English-to-Spanish translations, and we plan to cover more countries and languages in the near future.Showing definitions for coding and health informationIn August, we added a new, interactive way to see definitions for educational topics — like science, economics, or history — on AI-powered overviews. Now, we’re extending this functionality to more areas, including coding and health information. On relevant searches, you’ll see certain words highlighted, so you can hover over them to preview their definition or view related images. This update is rolling out over the next month, in English in the United States, with more countries and languages expected to follow soon.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Bringing Project Starline out of the lab",
    "link": "https://blog.google/technology/research/google-project-starline-hp-partnership/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNU1ZbXh4VG5kTk5rOUNjRGx5VFJDUkFSamNBaWdCTWdZTllwajFwQWc=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-13T07:00:00.000Z",
    "time": "May 13",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Bard’s latest updates: Access Gemini Pro globally and generate images",
    "link": "https://blog.google/products/gemini/google-bard-gemini-pro-image-generation/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHdRWFJFVDJkelFtYzFPR0Z5VFJDb0FSaXJBaWdCTWdhWmc1Q0tMUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-01T08:00:00.000Z",
    "time": "Feb 1",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Gemini breaks new ground with a faster model, longer context, AI agents and more",
    "link": "https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/R",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHhRakZxWkMxWWNHeFlha2N0VFJDb0FSaXJBaWdCTWdhSmtJVEd2QVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-14T07:00:00.000Z",
    "time": "May 14",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Buying Spying: How the commercial surveillance industry works and what can be done about it",
    "link": "https://blog.google/threat-analysis-group/commercial-surveillance-vendors-google-tag-report/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUtYMFpyT0VKbk56SklRME5UVFJDb0FSaXJBaWdCTWdZQk00eXVuUWs=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-06T08:00:00.000Z",
    "time": "Feb 6",
    "articleType": "regular",
    "content": "This report documents the rise of commercial surveillance vendors and the industry that threatens free speech, the free press and the open internet.\nUpdated April 18, 2024: A PDF of \"Buying Spying: Insights into Commercial Surveillance Vendors\" with updated graphics was uploaded. There are no substantive changes to the text of the report.Spyware is typically used to monitor and collect data from high-risk users like journalists, human rights defenders, dissidents and opposition party politicians. These capabilities have grown the demand for spyware technology, making way for a lucrative industry used to sell governments and nefarious actors the ability to exploit vulnerabilities in consumer devices. Though the use of spyware typically only affects a small number of human targets at a time, its wider impact ripples across society by contributing to growing threats to free speech, the free press and the integrity of elections worldwide.To shine a light on the spyware industry, today, Google’s Threat Analysis Group (TAG) is releasing Buying Spying, an in-depth report with our insights into Commercial Surveillance Vendors (CSVs). TAG actively tracks around 40 CSVs of varying levels of sophistication and public exposure. The report outlines our understanding of who is involved in developing, selling, and deploying spyware, how CSVs operate, the types of products they develop and sell, and our analysis of recent activity.Key findingsWhile prominent CSVs garner public attention and headlines, there are dozens of others that are less noticed, but play an important role in developing spyware.The proliferation of spyware by CSVs causes real world harm. We partnered with Google's Jigsaw unit to highlight the stories of three high-risk users who attested to the fear felt when these tools were used against them, the chilling effect on their professional relationships, and their determination to continue their important work.If governments ever claimed to have a monopoly on the most advanced cyber capabilities, that era is over. The private sector is now responsible for a significant portion of the most sophisticated tools we detect.CSVs pose a threat to Google users, and Google is committed to disrupting that threat and keeping our users safe. CSVs are behind half of known 0-day exploits targeting Google products as well as Android ecosystem devices.The business of 0-days and spyware supply chainPrivate sector firms have been involved in discovering and selling exploits for many years, but there is a rise in turnkey espionage solutions. CSVs offer pay-to-play tools that bundle an exploit chain designed to get past security measures, along with the spyware and the necessary infrastructure, in order to collect the desired data from the targeted user. Four primary groups have found it profitable to work together — thereby further enabling this industry:Vulnerability researchers and exploit developers: While some vulnerability researchers choose to monetize their work by improving the security of products (e.g., contributing to bug bounty programs, or working as defenders), others use their knowledge to develop and sell exploits to brokers, or directly to CSVs.Exploit brokers and suppliers: Individuals or companies located all over the world, specialized in selling exploits to customers which are often, but not always, governments.Commercial Surveillance Vendors (CSVs) or Private Sector Offensive Actors (PSOAs): Businesses focused on developing and selling spyware as a product, including the initial delivery mechanisms, the exploits, the command and control (C2) infrastructure, and the tools for organizing collected data.Government customers: Governments who purchase spyware from CSVs and select specific targets, craft campaigns that deliver the spyware, then monitor the spyware implant to collect and receive data from their target’s device.International efforts to combat spywareCommunity efforts to raise awareness have built momentum towards an international policy response. Today, we joined representatives from industry, governments and civil society at the conference, The Pall Mall Process: Tackling the Proliferation and Irresponsible Use of Commercial Cyber Intrusion Capabilities. The event was co-hosted by the governments of France and the UK and designed to build consensus and progress towards limiting the harms from this industry. These efforts build on earlier governmental actions, including steps taken last year by the US Government to limit government use of spyware, and a first-of-its-kind international commitment to similar efforts. We hope to see these initial steps followed by more concrete actions from a broader community of nations to reform the industry and shine more light on abuses.Disrupting the spyware ecosystem to protect usersCSVs have proliferated hacking and spyware capabilities that weaken the safety of the internet for all. This is why we discover and patch vulnerabilities used by CSVs, share intelligence strategies and fixes with industry peers and publicly release information about the operations we disrupt. Since November 2010, we have also used our vulnerability rewards program (VRP) to recognize the contributions of security researchers who invest their time and skills in helping secure the digital ecosystem. Additionally, Google offers a range of tools to help protect high-risk users from online threats. Though these steps help protect users and the internet at large, meaningfully curtailing this market will require collective action and a concerted international effort.We hope our detailed analysis on CSVs and recommended solutions will support the recent momentum toward global action.Special thanks to TAG's Aurora Blum for her contribution to this report.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google Cloud and NVIDIA Expand Partnership to Scale AI Development",
    "link": "https://nvidianews.nvidia.com/news/google-cloud-ai-development",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXZkWG90UkRWT2MwSTRZa3BUVFJDb0FSaXNBaWdCTWdhTm9aYUhPZ2M=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2024-03-18T07:00:00.000Z",
    "time": "Mar 18",
    "articleType": "regular",
    "content": "New AI infrastructure offerings and integrations enable more open and accessible AI\nGTC—Google Cloud and NVIDIA today announced a deepened partnership to enable the machine learning (ML) community with technology that accelerates their efforts to easily build, scale and manage generative AI applications.\nTo continue bringing AI breakthroughs to its products and developers, Google announced its adoption of the new NVIDIA Grace Blackwell AI computing platform, as well as the NVIDIA DGX Cloud service on Google Cloud. Additionally, the NVIDIA H100-powered DGX™ Cloud platform is now generally available on Google Cloud.\nBuilding on their recent collaboration to optimize the Gemma family of open models, Google also will adopt NVIDIA NIM inference microservices to provide developers with an open, flexible platform to train and deploy using their preferred tools and frameworks. The companies also announced support for JAX on NVIDIA GPUs and Vertex AI instances powered by NVIDIA H100 and L4 Tensor Core GPUs.\n“The strength of our long-lasting partnership with NVIDIA begins at the hardware level and extends across our portfolio – from state-of-the-art GPU accelerators, to the software ecosystem, to our managed Vertex AI platform,” said Google Cloud CEO Thomas Kurian. “Together with NVIDIA, our team is committed to providing a highly accessible, open and comprehensive AI platform for ML developers.”\n“Enterprises are looking for solutions that empower them to take full advantage of generative AI in weeks and months instead of years,” said Jensen Huang, founder and CEO of NVIDIA. “With expanded infrastructure offerings and new integrations with NVIDIA’s full-stack AI, Google Cloud continues to provide customers with an open, flexible platform to easily scale generative AI applications.”\nThe new integrations between NVIDIA and Google Cloud build on the companies’ longstanding commitment to providing the AI community with leading capabilities at every layer of the AI stack. Key components of the partnership expansion include:\nAdoption of NVIDIA Grace Blackwell: The new Grace Blackwell platform enables organizations to build and run real-time inference on trillion-parameter large language models. Google is adopting the platform for various internal deployments and will be one of the first cloud providers to offer Blackwell-powered instances.\nGrace Blackwell-powered DGX Cloud coming to Google Cloud: Google will bring NVIDIA GB200 NVL72 systems, which combine 72 Blackwell GPUs and 36 Grace CPUs interconnected by fifth-generation NVLink®, to its highly scalable and performant cloud infrastructure. Designed for energy-efficient training and inference in an era of trillion-parameter LLMs, NVIDIA GB200 NVL72 systems will be available via DGX Cloud, an AI platform offering a serverless experience for enterprise developers building and serving LLMs. DGX Cloud is now generally available on Google Cloud A3 VM instances powered by NVIDIA H100 Tensor Core GPUs.\nSupport for JAX on GPUs: Google Cloud and NVIDIA collaborated to bring the advantages of JAX to NVIDIA GPUs, widening access to large-scale LLM training among the broader ML community. JAX is a framework for high-performance machine learning that is compiler-oriented and Python-native, making it one of the easiest to use and most performant frameworks for LLM training. AI practitioners can now use JAX with NVIDIA H100 GPUs on Google Cloud through MaxText and Accelerated Processing Kit (XPK).\nNVIDIA NIM on Google Kubernetes Engine (GKE): NVIDIA NIM inference microservices, a part of the NVIDIA AI Enterprise software platform, will be integrated into GKE. Built on inference engines including TensorRT-LLM™, NIM helps speed up generative AI deployment in enterprises, supports a wide range of leading AI models and ensures seamless, scalable AI inferencing.\nSupport for NVIDIA NeMo: Google Cloud has made it easier to deploy the NVIDIA NeMo™ framework across its platform via Google Kubernetes Engine (GKE) and Google Cloud HPC Toolkit. This enables developers to automate and scale the training and serving of generative AI models, and it allows them to rapidly deploy turnkey environments through customizable blueprints that jump-start the development process. NVIDIA NeMo, part of NVIDIA AI Enterprise, is also available in the Google Marketplace, providing customers with another way to easily access NeMo and other frameworks to accelerate AI development.\nVertex AI and Dataflow expand support for NVIDIA GPUs: To advance data science and analytics, Vertex AI now supports Google Cloud A3 VMs powered by NVIDIA H100 GPUs and G2 VMs powered by NVIDIA L4 Tensor Core GPUs. This provides MLOps teams with scalable infrastructure and tooling to confidently manage and deploy AI applications. Dataflow has also expanded support for accelerated data processing on NVIDIA GPUs.\nGoogle Cloud has long offered GPU VM instances powered by NVIDIA’s cutting-edge hardware coupled with leading Google innovations. NVIDIA GPUs are a core component of the Google Cloud AI Hypercomputer – a supercomputing architecture that unifies performance-optimized hardware, open software and flexible consumption models. The holistic partnership enables AI researchers, scientists and developers to train, fine-tune and serve the largest and most sophisticated AI models – now with even more of their favorite tools and frameworks jointly optimized and available on Google Cloud.\n“Runway’s text-to-video platform is powered by AI Hypercomputer. At the base, A3 VMs, powered by NVIDIA H100 GPUs gave our training a significant performance boost over A2 VMs, enabling large-scale training and inference for our Gen-2 model. Using GKE to orchestrate our training jobs enables us to scale to thousands of H100 GPUs in a single fabric to meet our customers’ growing demand.”\nAnastasis Germanidis, CTO and Co-Founder of Runway\n“By moving to Google Cloud and leveraging AI Hypercomputer architecture with NVIDIA T4 GPUs, G2 VMs powered by NVIDIA L4 GPUs and Triton Inference Server, we saw a significant boost in our model inference performance while lowering our hosting costs 15% using novel techniques enabled by the flexibility that Google Cloud offers.”\nAshwin Kannan, Sr Staff Machine Learning Engineer, Palo Alto Networks\n“Writer’s platform all comes together through this extremely productive partnership with Google and NVIDIA. We’re able to use NVIDIA GPUs optimally for training and inference. We leverage NVIDIA NeMo to build our industrial-strength models, which generate 990,000 words a second with over a trillion API calls per month. We’re delivering the highest quality models that exceed those from companies with larger teams and bigger budgets – and all of that is possible with the Google and NVIDIA partnership. The benefits of their AI expertise are passed down to our enterprise customers, who can build meaningful AI workflows in days not months or years.”\nDanny Leung, Director of Alliances, Writer",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "100 things we announced at I/O 2024",
    "link": "https://blog.google/technology/ai/google-io-2024-100-announcements/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWFhRzFJU1VvM0xUQkVlWFV0VFJDb0FSaXJBaWdCTWdheEFZTHhGQVU=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-15T07:00:00.000Z",
    "time": "May 15",
    "articleType": "regular",
    "content": "At Google I/O 2024, the company unveiled a plethora of innovations, including advancements in AI models, generative media tools, search enhancements, Workspace integrations, Android features, and developer tools. Highlights include the introduction of Gemini 1.5 Flash and Pro models, the unveiling of Imagen 3 for high-quality image generation, and the introduction of Veo for video generation. Additionally, Google announced updates to Search with AI Overviews, planning capabilities, and multi-step reasoning. Gemini models are now integrated into Gmail, Docs, and other Workspace tools, while Android advancements include multimodal capabilities for Gemini Nano and enhanced privacy features. Developers can benefit from the Gemini API Developer Competition, new open-source models, and improved tools for Android development. Google also emphasized responsible AI practices with red teaming and the expansion of SynthID.\nSummaries were generated by Google AI. Generative AI is experimental.\nAt I/O 2024, Google announced a bunch of new stuff:\nImproved AI models like Gemini 1.5 Pro and Trillium TPU for faster and more efficient AI tasks.\nNew image and video generation models like Imagen 3 and Veo for creating realistic images and videos.\nUpdates to the Gemini app with new features like Gemini Live, Gems, and more integrations with Google tools.\nSearch improvements with AI Overviews, multi-step reasoning, and planning capabilities.\nGemini models coming to Workspace and Photos for help with emails, documents, and photos.\nAndroid advancements like Gemini Nano on Pixel, improved accessibility, and new privacy features.\nDevelopments for developers with new APIs, tools, and support for Kotlin multiplatform.\nProgress in responsible AI with red teaming, watermarking, and partnerships for ethical AI use.\nSummaries were generated by Google AI. Generative AI is experimental.\nPro improvements, now in preview,\nGmail, Docs, Drive, Sheets, enhanced,\nSummaries were generated by Google AI. Generative AI is experimental.\nPhew — it’s been a busy couple of days.\nA lot happened at I/O 2024! Whether you were most into the latest Gemini app updates, felt especially excited about what’s coming for developers or can’t wait to try the latest generative AI tools, there was something for just about everyone. Don’t believe us? Below, we rounded up 100 things we announced over the last two days.\nAI moments and model momentumWe introduced Gemini 1.5 Flash: a lighter-weight model that’s designed to be fast and efficient to serve at scale. 1.5 Flash is the fastest Gemini model served in the API.We’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window on Google AI Studio and Vertex AI.1.5 Pro is also available with a 2 million token context window to developers via waitlist in Google AI Studio and Vertex AI.\nContext lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability.\n25. We added in more editorial controls to ImageFX — a top feature request from the community — so you can add, remove or change elements by simply brushing over your image.26. ImageFX will also use Imagen 3 to unlock more photorealism with richer details and fewer visual artifacts and more accurate text rendering.27. MusicFX has a new feature called “DJ Mode” that helps you mix beats by combining genres and instruments, using the power of generative AI to bring music stories to life.28. As of this week, ImageFX and MusicFX are now available in over 100 countries through Labs.New ways to get more done with the Gemini app29. We’re bringing Gemini 1.5 Pro, our cutting edge model, to Gemini Advanced subscribers — which means Gemini Advanced now has a 1 million token context window and can do things like make sense of 1,500-page PDFs.30. This also means Gemini Advanced now has the largest context window of any commercially available chatbot in the world.31. We added the ability to upload files via Google Drive or directly from your device right into Gemini Advanced.32. Soon, Gemini Advanced will help you analyze your data to quickly uncover insights and build charts from uploaded data files like spreadsheets.33. Great news for travelers: Gemini Advanced has a new planning feature that goes beyond a list of suggested activities and will actually create a custom itinerary just for you.\n34. Then there’s Gemini Live for Gemini Advanced subscribers, a new, mobile-first conversational experience that uses state-of-the-art speech technology to help you have more natural, intuitive spoken conversations with Gemini.35. Gemini Live lets you choose from 10 natural-sounding voices it can respond to you with; plus, you can speak at your own pace or interrupt mid-response with clarifying questions.36. Gemini in Google Messages now lets you chat with Gemini in the same app where you message your friends.37. Gemini Advanced subscribers will soon be able to create Gems, customized versions of Gemini designed for whatever you dream up. Simply describe what you want your Gem to do and how you want it to respond and Gemini will take those instructions and create a Gem for your specific needs.38. And look out for more Google tools being connected to Gemini, including Google Calendar, Tasks, Keep and Clock.Updates that make Search do the work for you39. We’re using a new Gemini model customized for Google Search to bring together Gemini’s advanced capabilities — including multi-step reasoning, planning and multimodality — with our best-in-class Search systems.40. AI Overviews in Search are rolling out to everyone in the U.S. beginning this week with more countries coming soon.\n41. And multi-step reasoning capabilities are coming soon to AI Overviews in Search Labs for English queries in the U.S. So rather than breaking your question into multiple searches, you can ask complex questions like “find the best yoga or pilates studios in Boston and show details on their intro offers and walking time from Beacon Hill.”42. Soon, you’ll be able to adjust your AI Overview with options to simplify the language or break it down in more detail, when you’re new to a topic or trying to get to the heart of a subject.43. Search is also getting new planning capabilities. For example, meal and trip planning with customization will launch later this year in Search Labs, followed soon by more categories like parties and fitness.44. Thanks to advancements in video understanding, you now have the ability to ask questions with a video. Search can take a complex visual question and figure it out for you, then explain next steps and offer resources with an AI Overview.45. And soon, generative AI in Search will also create an AI-organized results page when you’re searching for fresh ideas. These AI-organized search result pages will be available when you’re searching for categories like dining, recipes, movies, music, books, hotels, shopping and more.Help from Gemini models in Workspace and Photos46. Gemini 1.5 Pro is now available in the side panel in Gmail, Docs, Drive, Slides and Sheets via Workspace Labs — and it’s rolling out to our Gemini for Workspace customers and Google One AI Premium subscribers next month.47. You’ll be able to use Gmail’s side panel to summarize emails to get the most important details and action items.48. In addition to summaries, Gmail’s mobile app will soon use Gemini for two other new features: Contextual Smart Reply and Gmail Q&A.49. In the coming weeks, Help me write in Gmail and Docs will support Spanish and Portuguese.50. Later this year in Labs, you can even ask Gemini to automatically organize email attachments in Drive, generate a sheet with the data and then analyze it with Data Q&A.51. A new experimental feature in Google Photos called Ask Photos makes it even easier to look for specific memories or recall information included in your gallery. The feature uses Gemini models, and it’s rolling out over the coming months.\n52. You can also use Ask Photos to create a highlight gallery from a recent trip, and it will even write personalized captions for you to share on social media.Android advancements53. Starting with Pixel later this year, Gemini Nano — Android’s built-in, on-device foundation model — will have multimodal capabilities. Beyond just processing text input, your Pixel phone will also be able to understand more information in context like sights, sounds and spoken language.54. Talkback, an accessibility feature for Android devices that helps blind and low-vision people use touch and spoken feedback to better interact with their devices, is being improved thanks to Gemini Nano with Multimodality.55. A new, opt-in scam protection feature that will use Gemini Nano’s on-device AI to help detect scam phone calls in a privacy preserving way. Look out for more details later this year.56. We announced that Circle to Search is currently available on more than 100 million Android devices, and we’re on track to double that by the end of the year.57. Soon, you’ll be able to use Gemini on Android to create and drag and drop generated images into Gmail, Google Messages and more, or ask about the YouTube video you’re viewing.58. If you have Gemini Advanced, you’ll also have the option to “Ask this PDF” to get an answer quickly without having to scroll through multiple pages.59. Students can now use Circle to Search for homework help directly from select Android phones and tablets. This feature is powered by LearnLM — our new family of models based on Gemini, fine-tuned for learning.60. Later this year, Circle to Search will be able to solve even more complex problems involving symbolic formulas, diagrams, graphs and more.\n61. Oh, and we introduced the second beta of Android 15.62. Theft Detection Lock uses powerful Google AI to sense if your device has been snatched and quickly lock down your information on your phone.63. Private space is coming to Android 15, which lets you choose apps to keep secure inside a separate space that requires an extra layer of authentication to open.64. And if a separate lock screen isn’t enough for your private spaces, you can hide the existence of it altogether.65. Later this year, Google Play Protect will use on-device AI to help spot apps that attempt to hide their actions to engage in fraud or phishing.66. We’re bringing an updated messaging experience to Japan with RCS in Google Messages.67. Soon in the U.S., you’ll be able to create a digital version of passes that just contain text. Simply take a photo of a pass (like an insurance card or event ticket) and easily add it to your Google Wallet for quick access.68. We showed off how augmented reality content will be available directly in Google Maps, laying the foundation for an extended reality (XR) platform we’re building in collaboration with Samsung and Qualcomm for the Android ecosystem.69. You can now catch up on episodes of your favorite shows on Max and Peacock or start a game of Angry Birds on select cars with Google built-in.70. We are also bringing Google Cast to cars with Android Automotive OS, starting with Rivian in the coming months, so you can easily cast video content from your phone to the car.71. Later this year, battery life optimizations are coming to watches with Wear OS 5. For example, running an outdoor marathon will consume up to 20% less power when compared to watches with Wear OS 4.72. Wear OS 5 will also give fitness apps the option to support more data types like ground contact time, stride length and vertical oscillation.73. It’s now easier to pick what to watch on Google TV and other Android TV OS devices with personalized AI-generated descriptions, thanks to our Gemini model.74. These AI-generated descriptions will also fill in missing or untranslated descriptions for movies and shows.75. Here’s a fun stat: Since launch, people have made over 1 billion Fast Pair connections.76. Later this month, you’ll be able to use Fast Pair to connect and find items like your keys, wallet or luggage in the Find My Device app with Bluetooth tracker tags from Chipolo and PebbleBee (with more partners to come).Developments for developers77. You can join the Gemini API Developer Competition and be a part of discovering the most helpful and groundbreaking AI apps. The prize: an electrically retrofitted custom 1981 DeLorean.78. We introduced PaliGemma, our first vision-language open model optimized for visual Q&A and image captioning.79. We previewed the next version of Gemma, Gemma 2. It’s built on a whole new architecture and will include a larger 27B parameter instance which outperforms models twice its size and runs on a single TPU host.\n80. Gemini models are now available to help developers be more productive in Android Studio, IDX, Firebase, Colab, VSCode, Cloud and Intellj.81. Gemini 1.5 Pro is coming to Android Studio later this year. Equipped with a large context window, this model leads to higher-quality responses and unlocks use cases like multimodal input.82. Google AI Studio is now available in more than 200 countries including the U.K. and E.U.83. Parallel function calling and video frame extraction are now supported by the Gemini API.84. And with the new context caching feature in the Gemini API, coming next month, you’ll be able to streamline workflows for large prompts by caching frequently used context files at lower costs.85. Android now provides first-class support for Kotlin multiplatform to help developers share their apps' business logic across platforms.86. Resizable Emulator, Compose UI check Mode and Android Device Streaming powered by Firebase are new products that can all help developers build for all form factors.87. Starting with Chrome 126, Gemini Nano will be built into the Chrome Desktop client.88. View Transitions API for multi-page apps, a much-requested feature, is now available so developers can easily build smooth, fluid app-like navigation regardless of site architecture.89. Project IDX, our new integrated developer experience for full-stack, multiplatform apps, is now open for everyone to try.90. Firebase released Firebase Genkit in beta, which will make it even easier for developers to build generative AI experiences into their apps.91. Firebase also released Firebase Data Connect, a new way for developers to use SQL with Firebase (via Google Cloud SQL). This will not only bring SQL workflows to Firebase, but also reduce the amount of app code developers need to write.92. We took developers under the hood in a deep-dive conversation about the technology and research powering our AI with James Manyika, Jeff Dean and Koray Kavukcuoglu.Responsible AI progress93. We’re enhancing red teaming — a proven practice where we proactively test our own systems for weakness and try to break them — through a new technique we’re calling “AI-Assisted Red Teaming.”94. We’re also expanding SynthID to two new modalities: text and video.95. SynthID text watermarking will also be open-sourced in the coming months through our updated Responsible Generative AI toolkit.96. We announced LearnLM, a new family of models based on Gemini and fine-tuned for learning. LearnLM is already powering a range of features across our products, including Gemini, Search, YouTube and Google Classroom.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google Cloud and NVIDIA Expand Partnership to Advance AI Computing, Software and Services",
    "link": "https://nvidianews.nvidia.com/news/google-cloud-and-nvidia-expand-partnership-to-advance-ai-computing-software-and-services",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWxiRlZoTkZCSVpGOVNkSFJqVFJDeUFSaWJBaWdCTWdZSlVaTG9wQWc=-w400-h224-p-df-rw",
    "source": "NVIDIA Blog",
    "datetime": "2023-08-29T07:00:00.000Z",
    "time": "Aug 29, 2023",
    "articleType": "regular",
    "content": "NVIDIA Generative AI Technology Used by Google DeepMind and Google Research Teams Now Optimized and Available to Google Cloud Customers Worldwide\nGoogle Cloud Next — Google Cloud and NVIDIA today announced new AI infrastructure and software for customers to build and deploy massive models for generative AI and speed data science workloads.\nIn a fireside chat at Google Cloud Next, Google Cloud CEO Thomas Kurian and NVIDIA founder and CEO Jensen Huang discussed how the partnership is bringing end-to-end machine learning services to some of the largest AI customers in the world — including by making it easy to run AI supercomputers with Google Cloud offerings built on NVIDIA technologies. The new hardware and software integrations utilize the same NVIDIA technologies employed over the past two years by Google DeepMind and Google research teams.\n“We’re at an inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace,” Huang said. “Our expanded collaboration with Google Cloud will help developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs.”\n“Google Cloud has a long history of innovating in AI to foster and speed innovation for our customers,” Kurian said. “Many of Google’s products are built and served on NVIDIA GPUs, and many of our customers are seeking out NVIDIA accelerated computing to power efficient development of LLMs to advance generative AI.”\nNVIDIA Integrations to Speed AI and Data Science Development\nGoogle’s framework for building massive large language models (LLMs), PaxML, is now optimized for NVIDIA accelerated computing.\nOriginally built to span multiple Google TPU accelerator slices, PaxML now enables developers to use NVIDIA® H100 and A100 Tensor Core GPUs for advanced and fully configurable experimentation and scale. A GPU-optimized PaxML container is available immediately in the NVIDIA NGC™ software catalog. In addition, PaxML runs on JAX, which has been optimized for GPUs leveraging the OpenXLA compiler.\nGoogle DeepMind and other Google researchers are among the first to use PaxML with NVIDIA GPUs for exploratory research.\nThe NVIDIA-optimized container for PaxML will be available immediately on the NVIDIA NGC container registry to researchers, startups and enterprises worldwide that are building the next generation of AI-powered applications.\nAdditionally, the companies announced Google’s integration of serverless Spark with NVIDIA GPUs through Google’s Dataproc service. This will help data scientists speed Apache Spark workloads to prepare data for AI development.\nThese new integrations are the latest in NVIDIA and Google’s extensive history of collaboration. They cross hardware and software announcements, including:\nGoogle Cloud on A3 virtual machines powered by NVIDIA H100 — Google Cloud announced today its purpose-built Google Cloud A3 VMs powered by NVIDIA H100 GPUs will be generally available next month, making NVIDIA’s AI platform more accessible for a broad set of workloads. Compared to the previous generation, A3 VMs offer 3x faster training and significantly improved networking bandwidth.\nNVIDIA H100 GPUs to power Google Cloud’s Vertex AI platform — H100 GPUs are expected to be generally available on VertexAI in the coming weeks, enabling customers to quickly develop generative AI LLMs.\nGoogle Cloud to gain access to NVIDIA DGX™ GH200 — Google Cloud will be one of the first companies in the world to have access to the NVIDIA DGX GH200 AI supercomputer — powered by the NVIDIA Grace Hopper™ Superchip — to explore its capabilities for generative AI workloads.\nNVIDIA DGX Cloud Coming to Google Cloud — NVIDIA DGX Cloud AI supercomputing and software will be available to customers directly from their web browser to provide speed and scale for advanced training workloads.\nNVIDIA AI Enterprise on Google Cloud Marketplace — Users can access NVIDIA AI Enterprise, a secure, cloud native software platform that simplifies developing and deploying enterprise-ready applications including generative AI, speech AI, computer vision, and more.\nGoogle Cloud first to offer NVIDIA L4 GPUs — Earlier this year, Google Cloud became the first cloud provider to offer NVIDIA L4 Tensor Core GPUs with the launch of the G2 VM. NVIDIA customers switching to L4 GPUs from CPUs for AI video workloads can realize up to 120x higher performance with 99% better efficiency. L4 GPUs are used widely for image and text generation, as well as VDI and AI-accelerated audio/video transcoding.\nGoogle Cloud accelerates every organization's ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google's cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.\nSince its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/.\nCertain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, features and availability of our products and technologies, including NVIDIA GPUs, NVIDIA accelerated computing, NVIDIA H100 and A100 Tensor Core GPUs, the NVIDIA DGX GH200 AI supercomputer, NVIDIA DGX Cloud, NVIDIA AI Enterprise and NVIDIA L4 Tensor Core GPUs; NVIDIA’s partnership with Google Cloud, including the benefits, impact, features and availability of Google Cloud offerings built on NVIDIA technologies; the inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace; and NVIDIA’s expanded collaboration with Google Cloud helping developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\nMany of the products and features described herein remain in various stages and will be offered on a when-and-if-available basis. The statements within are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described for our products is subject to change and remains at the sole discretion of NVIDIA. NVIDIA will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX, NGC and NVIDIA Grace Hopper are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.\nGoogle Cloud accelerates every organization's ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google's cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.\nSince its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/.\nCertain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, features and availability of our products and technologies, including NVIDIA GPUs, NVIDIA accelerated computing, NVIDIA H100 and A100 Tensor Core GPUs, the NVIDIA DGX GH200 AI supercomputer, NVIDIA DGX Cloud, NVIDIA AI Enterprise and NVIDIA L4 Tensor Core GPUs; NVIDIA’s partnership with Google Cloud, including the benefits, impact, features and availability of Google Cloud offerings built on NVIDIA technologies; the inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace; and NVIDIA’s expanded collaboration with Google Cloud helping developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\nMany of the products and features described herein remain in various stages and will be offered on a when-and-if-available basis. The statements within are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described for our products is subject to change and remains at the sole discretion of NVIDIA. NVIDIA will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX, NGC and NVIDIA Grace Hopper are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.",
    "favicon": "/media/sites/219/images/favicon.ico"
  },
  {
    "title": "A new, scrapbook-like Memories view in Google Photos",
    "link": "https://blog.google/products/photos/google-photos-memories-view/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWZiV1pyYjJsS1VVVXpUWEUxVFJDb0FSaXJBaWdCTWdZZE01QnRtUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-08-15T07:00:00.000Z",
    "time": "Aug 15, 2023",
    "articleType": "regular",
    "content": "Introducing a new home for your Memories that lets you easily relive, customize and share your most memorable moments.\nOur phones are filled with a growing number of photos and videos capturing everything from everyday moments to life milestones. But they’re also cluttered with duplicate images, screenshots, and blurry videos, making it hard to focus on the most meaningful moments. That’s why four years ago, we started resurfacing important photos and videos in the Memories carousel at the top of Google Photos — a feature that more than half a billion people now use each month.Today, we’re introducing the new Memories view, a home for your memories that is automatically curated and organized with the help of AI. This scrapbook-like timeline lets you easily relive, customize and share your most memorable trips, celebrations and daily moments with your loved ones. You’ll find the Memories view — which starts rolling out today in the U.S. and will be available globally in the coming months — in the updated navigation menu at the bottom of the Photos app so it’s always easy to get to.\nYour memories, your wayThere are now more ways to look back at your best photos and videos in Google Photos, and AI does the bulk of the work for you. Ultimately your memories are yours, and, as always, you’re in control. You can choose to save your favorite memories to your Memories view or create your own from scratch. The Memories view also lets you easily add or remove specific photos and videos that show up and hide memories altogether.Give your memories a name — with help from AIYou can rename your memories to make them recognizable and reflect the meaning and feeling of the moment. Or if you want some inspiration, let Photos provide a set of customized title suggestions created with generative AI. Just look for the “Help me title” button on select memories. If you don't like what you see, edit the suggested titles or ask for more options. You can even steer the suggestions toward important details to include by clicking the “Add hint” button. We know this feature won’t always get things right, so your feedback will help improve the suggestions over time. This is an experimental feature from Google Labs and will initially be available to select accounts in the U.S.\nShare your memories in even more waysAnd because your most meaningful moments are often shared, you’ll now be able to co-author your memories with others. Similar to how shared albums work today, you can invite friends or family to collaborate on a memory, contributing photos and videos to help fill in the gaps. If a memory is shared with you, you can choose to save that memory to your Memories view so you can return to it later.\nComing soon, we’ll also add the ability to share memories as videos so you can send them to your friends and family using your favorite messaging or social media app.The Memories view starts rolling out today in the U.S. Open up Google Photos to start exploring this new home for your memories and making it your own.\nPhotosHow to use Google Photos AI editing tools, now available to everyone\nPhotosAsk Photos: A new way to search your photos with Gemini\nPixel8 things I loved in my first week with the Pixel 8a\nPixel4 tips on getting the most out of Pixel 8 Pro’s Video Boost\nPhotosAI editing tools are coming to all Google Photos users\nGoogle Workspace5 AI tools to help organize your digital life",
    "favicon": "/favicon.ico"
  },
  {
    "title": "How we’ve created a helpful and responsible Bard experience for teens",
    "link": "https://blog.google/products/gemini/google-bard-expansion-teens/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDNiMDFOUjFsa2VHeFdXSGgyVFJDb0FSaXJBaWdCTWdhWmdwSk50UVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-15T08:00:00.000Z",
    "time": "Nov 15, 2023",
    "articleType": "regular",
    "content": "Teens in most countries around the world will be able to use Bard to easily find inspiration and learn new skills.\nTomorrow, we’ll open up access to Bard to teenagers in most countries around the world. Teens in those countries who meet the minimum age requirement to manage their own Google Account will be able to access Bard in English, with more languages to come over time.A helpful and informational tool for teensTeens can use Bard to find inspiration, discover new hobbies and solve everyday problems. For example, they could ask Bard for writing tips for a class president speech, suggestions for what universities to apply to, or ways to learn a new sport like pickleball.\nBard will be able to help with data visualization, too. With this new capability, Bard can generate charts from tables or data included in a prompt — like if a teen asks Bard to show in a bar chart how many hours they volunteered across a few months.Both features will be live tomorrow and available in English to start.Our responsible approachWe're continuing to be responsible as we open up Bard to more people. Before launching to teens, we consulted with child safety and development experts to help shape our content policies and an experience that prioritizes safety. And organizations like the Family Online Safety Institute (FOSI) advised us on how to keep the needs of teens and families in mind.\"FOSI's research found that most teens and parents expect that GenAI skills will be an important part of their future,” says Stephen Balkam, Founder & CEO of the Family Online Safety Institute. “Google's thoughtful approach to expanding Bard access to teens represents an important step in offering teens the opportunity to explore this technology with the appropriate safeguards in place.”Teens also shared feedback with us directly that they have questions about how to use generative AI and what its limitations might be. So we’ve developed a tailored onboarding experience in Bard for teens that includes resources like our AI Literacy Guide and a video with tips on how to use generative AI responsibly. The onboarding will also share an overview of how Bard Activity is used, and give teens the option to turn it on or off.\nDuring their Bard onboarding, teens will see this video about how to use generative AI responsibly.\nMeanwhile, we’ve trained Bard to recognize areas that are inappropriate to younger users and implemented safety features and guardrails to help prevent unsafe content, such as illegal or age-gated substances, from appearing in its responses to teens.We also recognize that many people, including teens, are not always aware of hallucinations in LLMs. So the first time a teen asks a fact-based question, we’ll automatically run our double-check response feature, which helps evaluate whether there’s content across the web to substantiate Bard’s response. Soon, this feature will automatically run when any new Bard user asks their first factual question. And for teens, we'll actively recommend using double-check to help them develop information literacy and critical thinking skills.Over time, we’ll add even more ways for teens to learn and explore responsibly — and for everyone using Bard to have a creative collaborator at their fingertips.\nGoogle Workspace5 tips for writing great prompts for Gemini in the Workspace side panel\nGeminiGemini’s big upgrade: Faster responses with 1.5 Flash, expanded access and more\nSearch8 ways to keep up with the Olympic Games Paris 2024 on Google\nCompany announcements4 ways Google will show up in NBCUniversal’s Olympic Games Paris 2024 coverage\nAndroid4 Google updates coming to Samsung devices\nLearning & EducationUpdates on how we're using AI to support students and educators",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Bard can now connect to your Google apps and services",
    "link": "https://blog.google/products/gemini/google-bard-new-features-update-sept-2023/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVNSMjlwVURKSGVISmtkWEpRVFJDb0FSaXJBaWdCTWdhaGRKRHR0UVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-09-19T07:00:00.000Z",
    "time": "Sep 19, 2023",
    "articleType": "regular",
    "content": "Use Bard alongside Google apps and services, easily double-check its responses and access features in more places.\nOne of the biggest benefits of Bard, an experiment to collaborate with generative AI, is that it can tailor its responses to exactly what you need. For instance, you could ask Bard to start a trip planning Doc for you and your friends, draft up your online marketplace listing, or help explain a science topic to your kids. And now, Bard is getting even better at customizing its responses so you can easily bring your ideas to life.Today we’re rolling out Bard’s most capable model yet. Bard now integrates with Google apps and services for more helpful responses. We’ve also improved the “Google it” feature to double-check Bard’s answers and expanded features to more places.Connect to Google apps and servicesToday we’re launching Bard Extensions in English, a completely new way to interact and collaborate with Bard. With Extensions, Bard can find and show you relevant information from the Google tools you use every day — like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels — even when the information you need is across multiple apps and services.For example, if you’re planning a trip to the Grand Canyon (a project that takes up many tabs), you can now ask Bard to grab the dates that work for everyone from Gmail, look up real-time flight and hotel information, see Google Maps directions to the airport, and even watch YouTube videos of things to do there — all within one conversation.\nWe’re also making it easier to build on others’ conversations with Bard. Starting today, when someone shares a Bard chat with you through a public link, you can continue the conversation and ask Bard additional questions about that topic, or use it as a starting point for your own ideas.\nAccess features in more placesAs we continue to build Bard responsibly, we’re now expanding access to existing English language features — such as the ability to upload images with Lens, get Search images in responses, and modify Bard’s responses — to more than 40 languages.Try our most capable model yetAll of these new features are possible because of updates we’ve made to our PaLM 2 model, our most capable yet. Based on your feedback, we’ve applied state-of-the-art reinforcement learning techniques to train the model to be more intuitive and imaginative. So, whether you want to collaborate on something creative, start in one language and continue in one of 40+ others, or ask for in-depth coding assistance, Bard can now respond with even greater quality and accuracy.\nWith these latest updates, Bard is more equipped to help you get all your unique ideas off the ground. Try the latest features at bard.google.com.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Updates to Location History and new controls coming soon to Maps",
    "link": "https://blog.google/products/maps/updates-to-location-history-and-new-controls-coming-soon-to-maps/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDJWVVphVm1wTGEyVm5SRFZ6VFJESkFoaVpBU2dCTWdZUlFvcnp3QVU=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-12-12T08:00:00.000Z",
    "time": "Dec 12, 2023",
    "articleType": "regular",
    "content": "People turn to Google Maps to make their lives easier — from seeing how crowded a bus will be or when a restaurant is busy to remembering a beach they visited years ago on vacation. These helpful features are possible because of location data. And with Google, managing location data is simple thanks to tools like auto-delete and Incognito mode. Today, we’re introducing new updates to give you even more control over this important, personal information.Coming soon: Your Timeline saved on your deviceThe Timeline feature in Maps helps you remember places you’ve been and is powered by a setting called Location History. If you’re among the subset of users who have chosen to turn Location History on (it’s off by default), soon your Timeline will be saved right on your device — giving you even more control over your data. Just like before, you can delete all or part of your information at any time or disable the setting entirely.If you’re getting a new phone or are worried about losing your existing one, you can always choose to back up your data to the cloud so it doesn’t get lost. We’ll automatically encrypt your backed-up data so no one can read it, including Google.Additionally, when you first turn on Location History, the auto-delete control will be set to three months by default, which means that any data older than that will be automatically deleted. Previously this option was set to 18 months. If you want to save memories to your Timeline for a longer period, don’t worry — you can always choose to extend the period or turn off auto-delete controls altogether.These changes will gradually roll out through the next year on Android and iOS, and you’ll receive a notification when this update comes to your account.\nSoon your Timeline will be saved right on your device — giving you even more control over your data.\nGetting a new phone (or worried you’ll lose yours)? You can choose to back up your data to the cloud — we’ll automatically encrypt it end to end.\nDelete activity related to specific places, right from MapsSay you’re planning a surprise birthday party, and you get directions to a nearby bakery to pick up the cake. Soon, you’ll be able to see all your recent activity on Maps related to the bakery in one central place, and easily delete your searches, directions, visits, and shares with just a few taps. The ability to delete place-related activity from Maps starts rolling out on Android and iOS in the coming weeks.\nSoon, you’ll be able to see and delete all your recent activity on Maps related to a place.\nAccess key location controls right from the blue dot in MapsThe blue dot, which shows where you are on Google Maps, now brings key location controls right to your fingertips. Just tap it, and at a glance, you'll see whether your Location History or Timeline settings are turned on and whether you’ve given Maps access to your device’s location. New blue dot controls start rolling out in the coming weeks on Android and iOS.\nJust tap the blue dot and you'll see whether your Location History or Timeline settings are turned on and whether you’ve given Maps access to your device’s location.\nYour location information is personal. We’re committed to keeping it safe, private and in your control. Remember: Google Maps never sells your data to anyone, including advertisers. So you can spend less time worrying about your data, and more time exploring new places, getting where you need to go or hanging out with friends — all with the help of Maps.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "New ways to find just what you need on Search",
    "link": "https://blog.google/products/search/google-search-november-2023-update/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNXFaa2xHWXkxQmJFdG9OVVIxVFJDUkFSamNBaWdCTWdrSkVZZ0R4Nk5ycEFJ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-15T08:00:00.000Z",
    "time": "Nov 15, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "What’s new with Android for cars at CES",
    "link": "https://blog.google/products/android/android-auto-new-features-ces24/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVRTRzVtTFZSTk1XVTVkbVZpVFJDb0FSaXNBaWdCTWdZQk1JZ1N4UVE=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-09T08:00:00.000Z",
    "time": "Jan 9",
    "articleType": "regular",
    "content": "Cars do so much more than get you from point A to point B these days, and Android is dedicated to making the full driving experience as safe, connected and enjoyable as possible. This year at CES, we're excited to announce new updates and partnerships that bring drivers what they want most to Android Auto and cars with Google built-in.\nNavigate with confidenceElectric vehicles compatible with Android Auto can share real-time battery information with Google Maps – coming first to the Ford Mustang Mach-E and F-150 Lightning in the coming months, with others to follow. This integration will make driving an electric vehicle easier, as Google Maps will provide your estimated battery level upon arrival at your destination, suggest charging stops along the way and even estimate how long charging will take based on your vehicle.Planning your trip is also getting easier for cars with Google built-in. Starting to roll out today, you can send the trip you’ve planned on Google Maps on your Android or iOS phone directly to your car with Google built-in, helping you seamlessly go from planning to navigating.\nStay connected, informed and entertained with more appsWhether you’re looking to check the weather forecast at your destination, or parked waiting for your kids at school pickup, there’s new apps to help you stay informed and entertained in cars with Google built-in.Today, Chrome browser is starting to roll out to select Polestar and Volvo cars in beta, and will be available in more cars later this year, so you can do a little shopping or access your saved bookmarks to keep you busy while parked.And if you need to keep the kids entertained (or hey, maybe keep yourself entertained!), you can watch shows from PBS KIDS and Crunchyroll, which are both now available in select cars with Google built-in.Finally, by popular demand, The Weather Channel app is now available for cars with Google built-in, giving you peace of mind on the road. Now you can stay up-to-date on changing weather conditions with hourly forecasts, follow-me, alerting and “Trip View” radar right from your dashboard.\nWhat’s next?We’re continuing to partner with more brands and developers to expand the helpful apps and services for your car. Android Auto is compatible with nearly every major car brand and is in over 200 million cars on the road, bringing the best of your phone experience to the car. And the list of major brands that offer cars with Google built-in continues to grow, with Nissan, Ford and Lincoln models rolling out this year and Porsche expected to launch in the future.Digital car key availability also continues to expand, rolling out to select Volvo cars soon and to even more phone and car brands in the future. Using a digital car key, you have the ability to unlock, lock and start your car with supported Android phones and securely share keys with friends and family on iOS or Android phones.If you happen to be at CES, be sure to check out the Android booth at CES to see demos of the experiences on the Ford Mustang Mach-E and the Polestar 3 — and if not, you can see these updates in action on the Android Auto and the cars with Google built-in websites or a vehicle showroom near you.\nWhat's new at CES 2024\nDo more with Google on your Android devices with new ways to help your devices work better together.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "New ways to search in 2024",
    "link": "https://blog.google/products/search/google-search-ai-january-2024-update/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNHRka0ZUVDBRMGJtUlNjV2s1VFJDUkFSamNBaWdCTWdtVklZSnlHV3BGMWdB=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-17T08:00:00.000Z",
    "time": "Jan 17",
    "articleType": "regular",
    "content": "For more than two decades, we’ve continuously redefined what a search engine can do — always guided by our mission to organize the world’s information and make it universally accessible and useful. This has gone hand in hand with our ongoing advancements in AI, which help us better understand information in its many forms — whether it’s text, audio, images or videos.As part of this evolution, we’ve made it easier to express what you’re looking for in ways that are more natural and intuitive. For instance, you can search with your voice, or you can search with your camera using Lens. And recently, we’ve been testing how generative AI’s ability to understand natural language makes it possible to ask questions on Search in a more natural way.Ultimately, we envision a future where you can search any way, anywhere you want. Now, as we enter 2024, we’re introducing two major updates that bring this vision closer to reality: Circle to Search and an AI-powered multisearch experience. Let’s take a look at what’s ahead.Circle (or highlight or scribble) to SearchToday we unveiled Circle to Search, a new way to search anything on your Android phone screen without switching apps. With a simple gesture, you can select images, text or videos in whatever way comes naturally to you — like circling, highlighting, scribbling or tapping — and find the information you need right where you are.\nWhen something grabs your interest (like these adorable dog goggles), it can be disruptive to stop what you’re doing and use another app or browser to start searching for information. But now, whether you’re texting friends, browsing social media or watching a video, you can search what’s on your screen right when your curiosity strikes. And as we’ve shared, Search and Shopping ads will continue to appear in dedicated ad slots throughout the results page.Circle to Search is launching globally on select premium Android smartphones on January 31, starting with the Pixel 8, the Pixel 8 Pro and the new Samsung Galaxy S24 series.Point your camera, ask a question, get help from AIIn 2022, we pioneered multisearch in Lens as a new way to search multimodally, with both images and text. Since it launched, multisearch has been best for refining visual queries — like searching for a photo of red sneakers with Lens and adding the word “blue” to find them in your preferred color. But now, thanks to our recent breakthroughs in generative AI, multisearch makes exploring the world easier than ever before.Starting today, when you point your camera (or upload a photo or screenshot) and ask a question using the Google app, the new multisearch experience will show results with AI-powered insights that go beyond just visual matches. This gives you the ability to ask more complex or nuanced questions about what you see, and quickly find and understand key information.For example, imagine you’re at a yard sale and you come across an unfamiliar board game. There’s no box or instructions, so immediately some questions spring to mind: What is this game and how is it played? This is where the new multisearch experience can help.Just take a picture of the game, add your question (“How do you play this?”), and you’ll get an AI-powered overview that brings together the most relevant information from across the web. This way, you can quickly find out what the game is called and how to win. And with the AI-powered overview, it’s easy to dig deeper with supporting links and get all the details.\nAI-powered overviews on multisearch results are launching this week in English in the U.S. for everyone — no enrollment in Search Labs required. To get started, just look for the Lens camera icon in the Google app for Android or iOS. If you’re outside the U.S. and opted into Search Generative Experience (SGE), you can preview this new experience in the Google app. You’ll also be able to access AI-powered overviews on multisearch results within Circle to Search.Continuing to boldly experiment with generative AI in SearchThis week’s launch of AI-powered insights for multisearch is the result of testing we began last year to see how gen AI can make Search radically more helpful, with SGE in Search Labs. We’ve gotten lots of useful feedback from people who’ve chosen to join this experiment, and we’ll continue to offer SGE in Labs as a testbed for bold new ideas.But our goal is to make AI helpful for everyone, not just early adopters. So moving forward, as we continue to experiment and uncover which applications of gen AI are most helpful, we’ll introduce them into Search more broadly, like we’re doing now with multisearch results.Today’s updates will make Search even more natural and intuitive, but we've only just scratched the surface of what's possible. To try out the latest capabilities we’re testing, enroll in Search Labs (where available) and opt into the SGE experiment. We hope you’ll join Labs as we continue reimagining the future of Search.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Get more personalized shopping options with these Google tools",
    "link": "https://blog.google/products/shopping/google-personalized-shopping-tips/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNDFSbTkxV1RacGJGOU5lV1pWVFJDUkFSamNBaWdCTWdtaFlvS3R1U05ON0FF=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-03-27T07:00:00.000Z",
    "time": "Mar 27",
    "articleType": "regular",
    "content": "People shop on Google more than a billion times a day.1 And thanks to the Shopping Graph, they see billions of products in their results — more than 45 billion,2 actually — that are constantly being refreshed. In fact, every hour, more than 2 billion listings are updated with the latest information, including pricing, in-stock availability and shipping details.3 So whether you’re looking for a specialty notepad from Japan, a monogrammed handbag from Paris or just a hammer from your local hardware store, the Shopping Graph can help you find it in just a few clicks.With that many options, we’re focused on making it easy to find exactly what you like. Here are a few ways you can have a more personalized shopping experience on Google.1. Rate styles to discover more of what you likeWe’ve started rolling out a new feature to help you get more personalized results in some of our most popular shopping categories. Starting with signed-in U.S. shoppers using mobile browsers and the Google app, when you search for certain apparel, shoes or accessories items — like “straw tote bags” or “men’s polo shirts” — you’ll see a section labeled “style recommendations.” There, you can rate options with a thumbs up or thumbs down, or a simple swipe right or left, and instantly see personalized results.If you haven’t quite found what you want (or you just want to keep going), we’ll give you the option to rate more items and instantly see another set of results. We’ll remember your preferences for next time, too. So when you’re looking for, say, men’s polo shirts again, you’ll see personalized style recommendations based on what you liked in the past and products you interacted with.If you made a mistake with your rating or don’t want to see personalized shopping results, you can easily manage your preferences: Just tap the three dots next to the \"Get style recommendations\" section and look for personalization options in the “About this result” panel.\n2. See more from your favorite brandsYou can also specify what brands you like. U.S. shoppers searching for apparel, shoes or accessories on mobile browsers, desktop or in the Google app can select brands they’d like to see more of while shopping with Google. Once you’ve selected them, you’ll see options from those brands right away. Tap the three dots next to the \"Popular from your favorite brands\" section and look for personalization options in the “About this result” panel to manage your preferences.\n3. Bring your vision to life with image generation20% of apparel shopping queries on Google have five words or more,4 which may mean that people have a very specific idea in mind when they’re shopping for clothes. Descriptions people use can also vary. For instance, someone might call something boxy, while another might call it oversized.We developed AI image generation for shopping so you can shop for apparel styles similar to whatever you had in mind. Say you want a new spring jacket, but haven’t found the right match. After searching for your query — like “colorful quilted spring jacket” — tap “Generate images” to see photorealistic images that match your vision. Once you’ve found one you like, just click it and scroll to see shoppable options. Anyone in the U.S. who has opted into Search Generative Experience (SGE) within Search Labs can access this experiment on the Google app or mobile browsers.\n4. Virtually try it onEven when you’ve found a style you like, you might want that extra assurance it’s going to look just right on you. That’s where our virtual try-on (VTO) tool comes into play. In the U.S. on desktop, mobile or the Google app, just look for the “try-on” icon in shopping results for men’s or women’s tops. You’ll see what that top looks like on a diverse set of real models ranging in size from XXS-4XL — including how it would drape, fold or form wrinkles and shadows on the model. VTO is also helpful for brands: After engaging with VTO imagery, shoppers are more likely to click out to retailers’ sites to potentially buy.5\nNo two shoppers are alike, which is why we’re designing the shopping experience on Google so it’s tailored to you. Look out for more personalized experiences to help you find and shop for what you love.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "How AI is helping airlines mitigate the climate impact of contrails",
    "link": "https://blog.google/technology/ai/ai-airlines-contrails-climate-change/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDROVzVuWjBVNGIyVllWV3RTVFJDb0FSaXNBaWdCTWdZbEVvU3Z6UVE=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-08-08T07:00:00.000Z",
    "time": "Aug 8, 2023",
    "articleType": "regular",
    "content": "We worked with the airline industry to use AI and satellite imagery to reduce the warming effects of contrails\nContrails — the thin, white lines you sometimes see behind airplanes — have a surprisingly large impact on our climate. The 2022 IPCC report noted that clouds created by contrails account for roughly 35% of aviation's global warming impact, over half the impact of the world’s jet fuel.1 Google Research teamed up with American Airlines and Breakthrough Energy to bring together huge amounts of data — like satellite imagery, weather and flight path data — and used AI to develop contrail forecast maps to test if pilots can choose routes that avoid creating contrails.\nVisual explanation of nighttime and daytime contrail radiative effects. Nighttime contrails are often more warming than daytime contrails because they exclusively trap heat.\nContrails form when airplanes fly through layers of humidity and they can persist as cirrus clouds for minutes or hours depending on the atmospheric conditions. While these extra clouds can reflect sunlight back into space during the day, they also trap large amounts of heat that would otherwise leave the Earth’s atmosphere. This creates a net warming effect. Avoiding flying through areas that create contrails can reduce warming. The challenge is knowing which flight routes will create contrails.Reducing the warming impact of contrailsA group of pilots at American flew 70 test flights over six months while using Google’s AI-based predictions, cross-referenced with Breakthrough Energy’s open-source contrail models, to avoid altitudes that are likely to create contrails. After these test flights, we analyzed satellite imagery and found that the pilots were able to reduce contrails by 54%. This is the first proof point that commercial flights can verifiably avoid contrails and thereby reduce their climate impact.\nAmerican Airlines Managing Director of Flight Operations, Captain John P. Dudley (right), and First Officer, Tammy Caudill (left), from the flight deck of the first contrail avoidance flight, who used our predictions in PACE’s FPO application to avoid contrails.\nThe other significant finding of our test with American is the flights that attempted to avoid creating contrails burned 2% additional fuel. Recent studies show that a small percentage of flights need to be adjusted to avoid the majority of contrail warming. Therefore, the total fuel impact could be as low as 0.3% across an airline’s flights.2 This suggests that contrails could be avoided at scale for around $5-25/ton CO2e (carbon dioxide equivalent) using our existing predictions, making it a cost-effective warming-reduction measure, and further improvements are expected.\nContrails detected over the United States using AI and GOES-16 satellite imagery.\nWhat’s next?Contrail avoidance has the potential to be a cost-effective, scalable solution to reduce the climate impact of flying. We will continue research and development to automate avoidance, target the highest impact contrails and improve satellite-based verification. We’re committed to working across the aviation industry to use AI to make contrail avoidance a reality over the coming years.\nGoogle AdsNew reporting and genAI tools to boost creative results\nSustainabilityWildfire boundary maps expand to new countries in Europe and Africa\nSustainabilityHow Google uses AI to reduce stop-and-go traffic on your route — and fight fuel emissions\nAI3 things parents and students told us about how generative AI can support learning\nSafety & SecurityIntroducing the Coalition for Secure AI (CoSAI) and founding member organizations\nCompany announcements4 ways Google will show up in NBCUniversal’s Olympic Games Paris 2024 coverage",
    "favicon": "/favicon.ico"
  },
  {
    "title": "New AI tools to help merchants market brands and products",
    "link": "https://blog.google/products/shopping/google-generative-ai-marketing-features-may-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTJUREkzUW1kU1dWTkhaRXRvVFJDZEFSakJBaWdCTWdhMUFZWXhqUXM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-21T07:00:00.000Z",
    "time": "May 21",
    "articleType": "regular",
    "content": "Today at GML we introduced new tools for retailers including a new profile for e-commerce brands, generative AI features for more effective product imagery and new immersive ad formats.\nWe’ve all had those magic shopping moments - finally discovering just the right product we were looking for – it’s beautiful, and we love the brand. But sometimes finding that product and brand connection feels harder than it should be.Today at Google Marketing Live, I shared some of our most exciting new tech for retailers that help you cultivate more of those magic moments. At a time when shoppers are flooded with options, we’re introducing new ways to deepen the connections between shoppers and merchants through rich and compelling content.Showing brand info at a glanceMore than 40 percent of shopping queries on Search mention a brand or retailer.1 This suggests that a key step in a shopper’s path to purchase involves learning more about your brand. Today, we unveiled a new visual brand profile right on Search that gives richer results for those common shopping queries. On the new brand profile we will highlight information you’ve provided through Google Merchant Center - as well as Google’s Shopping Graph - to showcase your ethos and offering.Our visual brand profiles are inspired by the Business Profiles we offer local businesses on Search and Maps. Your profile will spotlight engaging product and brand imagery, videos and customer reviews. Plus, we’ll show your current deals, promotions and shipping policies front and center – in one convenient place for shoppers. Ads will continue to appear on the page along with this new profile. We'll roll this out in phases in the coming months.\nHelping you create better, on-brand contentLast year, we introduced Product Studio, a merchant’s one-stop-shop for AI-powered content creation. Eighty percent of merchants who use Product Studio tell us that they have been more efficient - or expect to be more efficient - from using it.2 We’ve also seen that one in three product images generated with Product Studio are published or downloaded.3 This clearly shows its efficacy given it generates four images with each prompt.New Product Studio features continue to put the power of Google AI in the hands of merchants. For generated images to be useful, they need to be cohesive with your existing campaigns and content. Now, you’ll be able to generate new product images that match your unique brand style. Just upload an image that represents your aesthetic, add a prompt describing the scene and within moments Product Studio will generate campaign-ready content. It’s all in your brand style, cutting down on costs and saving you time. Now a rugged-chic e-commerce brand like Taylor Stitch can upload a standard product image of a new suede weekender bag alongside an inspo image of the California coast from a recent photo shoot. Product Studio will generate a brand new, realistic image of the bag with a backdrop in the style of the shoot.\nProduct Studio will also give you the ability to generate videos from just one photo. So, with just the click of a button, you can animate components of still product images to create short videos or playful product GIFs for social media. Product Studio is now available in Australia, Canada, U.K. and U.S. in Merchant Center Next and the Google & YouTube app on Shopify and coming to India and Japan in the next few weeks.\nBringing Shopping ads to life with generative AIToday we’re introducing three new ad formats powered by Google’s generative AI to help your ads work harder for you.Initially launching for retailers as a closed beta later this year, you can connect your brand’s short-form product videos - or videos from creators - to your ads. Now, with just a click, shoppers on Search can engage with these short videos showing how your clothes look on others, view helpful styling suggestions, and easily explore related or complementary products from your brand. We’ll also show AI summaries below the video highlights so shoppers can see key details about a product before they make a decision to buy.\nSince we brought our pioneering Virtual Try-On technology to Search last year, we’ve seen people click out to retailer sites more when viewing products with Virtual Try-On enabled, and brands have seen that their Virtual Try-On imagery get 60% more high-quality views than other images.4 To help you harness the power of VTO, we're launching Virtual Try-On in apparel ads for men’s and women’s tops. This lets shoppers see how your products look on a variety of body types and build the confidence to buy.We’ve also used 3D technology to automatically create 3D spins from a set of images, making asset creation easier for merchants who sell shoes. We’re excited to bring this innovation to ads. Now, consumers will have an engaging way to explore your shoes, like this pair from adidas, right from a Shopping ad. All a merchant needs to do is provide a handful of high-quality images of their shoes at different angles and we’ll use our advanced AI techniques to create a 360-degree view of their footwear.\nWe’re excited to help retailers go beyond simply listing their products to creating experiences that can help establish genuine, lasting connections with shoppers. Be sure to check out the other ads innovations powered by AI that we shared at Google Marketing Live 2024.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Chrome is getting 3 new generative AI features",
    "link": "https://blog.google/products/chrome/google-chrome-generative-ai-features-january-2024/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNXNRbFpQZFhvMmJWUnhUM0p4VFJDUkFSamNBaWdCTWdNQlFRZw=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-23T08:00:00.000Z",
    "time": "Jan 23",
    "articleType": "regular",
    "content": "Turn on these experimental AI features to organize your tabs, create custom themes and get help with writing on the web.\nOver the last few years, we’ve brought the latest machine learning and AI technologies into Chrome to make searching the web easier, safer and more accessible. We started with improving practical, everyday tasks, like helping you add real-time captions to videos, better detect malicious sites, manage permission prompts and generate the key points of a webpage.Starting with today’s release of Chrome (M121), we're introducing experimental generative AI features to make it even easier and more efficient to browse — all while keeping your experience personalized to you.You’ll be able to try out these new features in Chrome on Macs and Windows PCs over the next few days, starting in the U.S. Just sign into Chrome, select “Settings” from the three-dot menu and navigate to the “Experimental AI” page. Because these features are early public experiments, they’ll be disabled for enterprise and educational accounts for now.\nSmartly organize your tabsTab groups are a helpful way to manage lots of tabs, but curating them can be a pretty manual process. With a new tab organizer feature, Chrome will automatically suggest and create tab groups based on your open tabs. This can be particularly helpful if you’re working on several tasks in Chrome at the same time, like planning a trip, researching a topic and shopping. To use this feature, right-click on a tab and select “Organize Similar Tabs” or click the drop-down arrow to the left of your tabs. Chrome will even suggest names and emoji for these new groups so you can easily find them again when you need them next.\nCreate your own themes with AILast year, we introduced generative AI wallpapers with Android 14 and Pixel 8 devices. Now we’re bringing that same text-to-image diffusion model to Chrome so you can personalize your browser even more. You’ll be able to quickly generate custom themes based on a subject, mood, visual style and color that you choose — no need to become an AI prompt expert! To get started, visit the “Customize Chrome” side panel, click “Change theme” and then “Create with AI.” For example, maybe you’re enamored with the “aurora borealis” and want to see it in an “animated” style with a “serene” mood. Just select those options to see what Chrome comes up with. For more inspiration, check out this collection of the Chrome team’s favorite theme creations.\nQuick reminder: In addition to AI-generated themes, you can also customize Chrome with photos you’ve uploaded or themes from our collections in the Chrome Web Store, including ones commissioned from a growing community of artists.\nGet help drafting things on the webWriting on the web can be daunting, especially if you want to articulate your thoughts on public spaces or forums. So in next month's Chrome release, we’ll launch another experimental AI-powered feature to help you write with more confidence on the web — whether you want to leave a well-written review for a restaurant, craft a friendly RSVP for a party or make a formal inquiry about an apartment rental. To get started, right-click a text box or field on any site you visit in Chrome and select “Help me write.” Type in a few words and our AI will kickstart the writing process for you.\nLook out for more ways we’re bringing AI and ML into Chrome this year, including integrating our new AI model Gemini, to help you browse even easier and faster.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "A new modern look for the Android brand",
    "link": "https://blog.google/products/android/modern-look/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTRXVXBYUVhwNFdGZElTREp3VFJDb0FSaXJBaWdCTWdZTlVvcUdRUVU=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-09-05T07:00:00.000Z",
    "time": "Sep 5, 2023",
    "articleType": "regular",
    "content": "Sorry, your browser doesn't support embedded videos, but don't worry, you can\nand watch it with your favorite video player!\nAs the world’s largest operating system, Android’s open platform serves a diverse community of developers, device makers and users. And as we’ve grown — with over 3 billion Android devices worldwide — so has our vision. We believe our brand system and how we show up visually to the world as Android should reflect Android’s core ethos of being open, iterative and inclusive. That’s why we’re sharing an update to our visual identity that better represents our Android community — and it’s also a lot of fun, too.Behind our new logo makeoverOver the past decade, the Android brand has undergone several updates to modernize its look and feel and evolve with the needs of our community. In 2019, for instance, we changed our logo to be more accessible and easier to read. We also updated the naming convention for Android releases from fanciful (e.g., Android Lollipop) to simple (e.g., Android 14), making subsequent releases clearer and easier to understand globally.Each time we overhaul our branding, we evaluate not only changing needs, but also future goals. We know people today want more choice and autonomy, and we want our brand to be reflective of Android: something that gives people the freedom to create on their terms. As an open platform, it’s important that both our technology and brand are an invitation for people to create, connect and do more with Google on Android devices.Our new visuals draw inspiration from Material design to complement the Google brand palette, as well as be adaptable. The refreshed and dynamic robot shows up where Android connects with people, community and cultural moments. It can reflect individual passions, personality and context.With this update, you’ll notice some subtle changes that help connect Android to Google. In addition to moving away from our longstanding lowercase stylization of “android,” we’re elevating the Android logo by capitalizing the “A,” adding more weight to its appearance when placed next to Google’s logo. While we’ve added more curves and personality unique to Android, the new Android stylization more closely mirrors Google’s logo and creates balance between the two. We hope these small but significant updates to the Android typeface will better communicate the relationship between Android devices and the Google apps and services people already know.\nAndroid robots as unique as our communityAs part of our last update, the Android robot became a prominent fixture of our logo, reflecting the playfulness people have come to expect from Android. Today, we’re giving the most recognizable non-human member of our Android community an entirely new 3D look.The bugdroid — the face and most identifiable element of the Android robot — now appears with more dimension, and a lot more character. As a visual signifier of our brand, we wanted the bugdroid to appear as dynamic as Android itself. We've also updated the robot’s full-body appearance to ensure it can easily transition between digital and real-life environments, making it a versatile and reliable companion across channels, platforms and contexts.\nYou’ll start seeing the new aspects of the brand identity, like the updated logo and 3D bugdroid, appear on Android devices and in more places starting this year.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Gemini’s big upgrade: Faster responses with 1.5 Flash, expanded access and more",
    "link": "https://blog.google/products/gemini/google-gemini-new-features-july-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTNjVk4zTWpsd1VFSmFMWE55VFJDb0FSaXJBaWdCTWdZTmc1Um50Z2M=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-07-25T16:04:03.000Z",
    "time": "Jul 25",
    "articleType": "regular",
    "content": "Every day, we learn about how people are using Gemini to be more productive, creative and curious. And with today’s update, Gemini is getting better at helping you accomplish those tasks in ways that work best for you.You can now access 1.5 Flash in the unpaid version of Gemini for faster and more helpful responses. Plus, we’re introducing a new feature to further address hallucinations, and expanding our Gemini for Teens experience and mobile app to more places.Faster, smarter responses with 1.5 FlashWe’ve heard that one of the main reasons people enjoy using Gemini is because it saves time. Whether you’re using Gemini to write a compelling email or debug tricky code, it’s important to get fast, high-quality responses.Today we’re upgrading our free-tier experience to Gemini 1.5 Flash. With Gemini 1.5 Flash, you’ll notice across-the-board improvements in quality and latency, with especially noticeable improvements in reasoning and image understanding. And just like we greatly expanded the context window in Gemini Advanced, we’re quadrupling Gemini’s to 32K tokens. That means you can have longer back-and-forth conversations and ask Gemini more complex questions — all free of charge.To get the most out of the larger context window, we’ll soon add the ability to upload files via Google Drive or directly from your device, which has been available in Gemini Advanced. That means you’ll be able to do things like upload your economics study guide and ask Gemini to create practice questions. Gemini will also soon be able to analyze data files for you, allowing you to uncover insights and visualize them through charts and graphics.\nGemini features in more placesEarlier this year, we introduced the ability to chat with Gemini directly in Google Messages on select Android devices. Starting today, we’re gradually rolling out Gemini in Google Messages to the European Economic Area (EEA), UK and Switzerland, with the ability to chat in newly added languages like French, Polish and Spanish. Click the \"Start chat\" button in Messages and select Gemini to start brainstorming ideas, planning trips and more — all without leaving the Google Messages app.We’re also rolling out the Gemini mobile app to more countries, so more people around the world can get help from Gemini on the go.Expanded access to Gemini for teensIn the coming week, we’ll expand Gemini access to teenagers globally, in over 40 languages. Teens who meet the minimum age requirement to manage their own Google Account will be able to access Gemini to do things like better understand school subjects, prepare for university or get help with creative projects.We want to create opportunities for teens to benefit from all generative AI has to offer and prepare them for a future where AI will play an even more central role — all while prioritizing safety and meeting their developmental needs. To help them navigate Gemini confidently and safely, we’ve put additional policies and safeguards in place, introduced a teen-specific onboarding process, and included an AI literacy guide to help teens use AI responsibly. We’ve also partnered with child safety and development experts, including MediaSmarts (CA), Miudos Seguros na Net (PT), and Fad Juventud (ES), who continue to provide expertise on meeting the unique needs of teens and families.How we responsibly design GeminiGemini’s development has always been guided by a commitment to responsibility and user safety. As it continues to evolve, we’re sharing more about how we design Gemini and intend for it to respond. You can now read about our approach to Gemini, plus more details about our policy guidelines to better understand how we’re navigating complex and sensitive topics, including responses to topics related to public interest issues, and political, religious or moral beliefs. These guidelines, grounded in our AI Principles, reflect our ongoing commitment to developing this technology responsibly and transparently.Look out for more Gemini news coming at Made by Google. In the meantime, start chatting with Gemini today to try out these updates.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "AI editing tools are coming to all Google Photos users",
    "link": "https://blog.google/products/photos/google-photos-editing-features-availability/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWxaRzV4UkRGb1VXNURRbGR3VFJDb0FSaXJBaWdCTWdZQmtKQ21OQWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-04-10T07:00:00.000Z",
    "time": "Apr 10",
    "articleType": "regular",
    "content": "Magic Editor, Photo Unblur, Magic Eraser and more enhanced editing features are coming to all Google Photos users — no subscription required.\nWith the right editing tools, your photos can really shine. Google Photos has several features to help you enhance your pictures without pro-level editing skills, from removing a distraction in the background to unblurring a fuzzy shot. And now we’re bringing those capabilities to even more people.Starting on May 15, many of our AI-powered editing tools — like Magic Eraser, Photo Unblur and Portrait light — will be available to anyone using Google Photos1, no subscription required. You'll also be able to access these features on more devices, including Pixel tablets.Here’s a closer look at some of these editing tools, and how they can spruce up your photos:\nThe before and after effects of using Photo Unblur to sharpen a photo of three people\nPhoto Unblur: Clear up those blurry shots to relive your favorite moments with greater clarity.\nMake complex edits with Magic EditorLast year, we launched Magic Editor on Pixel 8 and Pixel 8 Pro. Using generative AI, this editor makes it easy to do complex photo edits with simple and intuitive actions, like repositioning your subject or turning the sky from gray to blue. Now we’re expanding access to Magic Editor to all Pixel devices.Additionally, all Google Photos users on Android and iOS will get 10 Magic Editor saves per month2. To go beyond this limit, you'll need a Pixel device or a Premium Google One plan (2TB and above).\nMagic Editor: Reimagine your photos with the help of generative AI.\nThese tools will gradually roll out starting May 15 and over the following weeks to all devices that meet minimum requirements. Look out for them in Google Photos to help make your shots picture-perfect.\nPhotosHow to use Google Photos AI editing tools, now available to everyone\nPhotosAsk Photos: A new way to search your photos with Gemini\nPixel8 things I loved in my first week with the Pixel 8a\nPixel4 tips on getting the most out of Pixel 8 Pro’s Video Boost\nGoogle Workspace5 AI tools to help organize your digital life\nSearch6 AI tools to help you give better gifts",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Chromebooks will get 10 years of automatic updates",
    "link": "https://blog.google/outreach-initiatives/education/automatic-update-extension-chromebook/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHRMVFJRZEd4ak5sUTRTSFJ2VFJDUkFSamNBaWdCTWdhTklvcXNKUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-09-14T07:00:00.000Z",
    "time": "Sep 14, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "How to find holiday shopping deals on Google",
    "link": "https://blog.google/products/shopping/google-shopping-best-holiday-deals-2023/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNDRSMkp1U2tkbVZrWjJWbTVaVFJDb0FSaXJBaWdCTWdrVlU0cElMdVpNNndF=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-07T08:00:00.000Z",
    "time": "Nov 7, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "New performance and safety features are coming to Chrome",
    "link": "https://blog.google/products/chrome/google-chrome-december-2023-update/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWZWVUp6Y1dScE1FcE9iSE5vVFJDUkFSamNBaWdCTWdhWmtvcXRPUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-12-21T08:00:00.000Z",
    "time": "Dec 21, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Gemma: Google introduces new state-of-the-art open models",
    "link": "https://blog.google/technology/developers/gemma-open-models/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDNabVJOUWxOSFdFOVZUelJLVFJDb0FSaXNBaWdCTWdhbGhJcXN4UVE=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-21T08:00:00.000Z",
    "time": "Feb 21",
    "articleType": "regular",
    "content": "Gemma is built for responsible AI development from the same research and technology used to create Gemini models.\nAt Google, we believe in making AI helpful for everyone. We have a long history of contributing innovations to the open community, such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold, and AlphaCode. Today, we’re excited to introduce a new generation of open models from Google to assist developers and researchers in building AI responsibly.Gemma open modelsGemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Developed by Google DeepMind and other teams across Google, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning “precious stone.” Accompanying our model weights, we’re also releasing tools to support developer innovation, foster collaboration, and guide responsible use of Gemma models.Gemma is available worldwide, starting today. Here are the key details to know:We’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-trained and instruction-tuned variants.A new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI applications with Gemma.We’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks: JAX, PyTorch, and TensorFlow through native Keras 3.0.Ready-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging Face, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.Pre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud with easy deployment on Vertex AI and Google Kubernetes Engine (GKE).Optimization across multiple AI hardware platforms ensures industry-leading performance, including NVIDIA GPUs and Google Cloud TPUs.Terms of use permit responsible commercial usage and distribution for all organizations, regardless of size.State-of-the-art performance at sizeGemma models share technical and infrastructure components with Gemini, our largest and most capable AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for their sizes compared to other open models. And Gemma models are capable of running directly on a developer laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key benchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical report for details on performance, dataset composition, and modeling methodologies.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google commits to invest $2 billion in OpenAI competitor Anthropic",
    "link": "https://www.cnbc.com/2023/10/27/google-commits-to-invest-2-billion-in-openai-competitor-anthropic.html",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUZlbFZhWVRCRGVFcFlTblZ1VFJDb0FSaXNBaWdCTWdtZEFvS01WYUk4SkFN=-w400-h224-p-df-rw",
    "source": "CNBC",
    "datetime": "2023-10-27T07:00:00.000Z",
    "time": "Oct 27, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "3 new Chrome features to get more helpful suggestions",
    "link": "https://blog.google/products/chrome/google-chrome-update-february-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUJiVEpUT0hwTlNGRTNjbWhKVFJDUkFSamNBaWdCTWdhSllZcUxNUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-29T08:00:00.000Z",
    "time": "Feb 29",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Advancing personal health and wellness insights with AI",
    "link": "http://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUJNMGhzYTIxSVR6Tk5lR0UwVFJDcUFSaXBBaWdCTWdrQkFJaGdtcVFleEFJ=-w400-h224-p-df-rw",
    "source": "Google Research",
    "datetime": "2024-06-11T07:00:00.000Z",
    "time": "Jun 11",
    "articleType": "regular",
    "content": "Mobile and wearable devices can provide continuous, granular, and longitudinal data on an individual’s physiological state and behaviors. Examples include step counts, raw sensor measurements such as heart rate variability, sleep duration, and more. Individuals can use these data for personal health monitoring as well as to motivate healthy behavior. This represents an exciting area in which generative AI models can be used to provide additional personalized insights and recommendations to an individual to help them reach their health goals. To do so, however, models must be able to reason about personal health data comprising complex time series and sporadic information (like workout logs), contextualize these data using relevant personal health domain knowledge, and produce personalized interpretations and recommendations grounded in an individual’s health context.Consider a common health query, “How can I get better sleep?” Though a seemingly straightforward question, arriving at a response that is customized to the individual involves performing a series of complex analytical steps, such as: checking data availability, calculating average sleep duration, identifying sleep pattern anomalies over a period of time, contextualizing these findings within the individual's broader health, integrating knowledge of population norms of sleep, and offering tailored sleep improvement recommendations. Recently, we showed how building on Gemini models’ advanced capabilities in multimodality and long-context reasoning could enable state-of-the-art performance on a diverse set of medical tasks. However, such tasks rarely make use of complex data sourced from mobile and wearable devices relevant for personal health monitoring.Building on the next-generation capabilities of Gemini models, we present research that highlights two complementary approaches to providing accurate personal health and wellness information with LLMs. The first paper, “Towards a Personal Health Large Language Model”, demonstrates that LLMs fine-tuned on expert analysis and self-reported outcomes are able to successfully contextualize physiological data for personal health tasks. The second paper, “Transforming Wearable Data into Personal Health Insights Using Large Language Model Agents”, emphasizes the value of code generation and agent-based workflows to accurately analyze behavioral health data through natural language queries. We believe that bringing these ideas together, to enable interactive computation and grounded reasoning over personal health data, will be critical components for developing truly personalized health assistants. With these two papers, we curate new benchmark datasets across a range of personal health tasks, which help evaluate the effectiveness of these models.\nTowards a personal health large language modelThe Personal Health Large Language Model (PH-LLM) is a fine-tuned version of Gemini, designed to generate insights and recommendations to improve personal health behaviors related to sleep and fitness patterns. By using a multimodal encoder, PH-LLM is optimized for both textual understanding and reasoning as well as interpretation of raw time-series sensor data such as heart rate variability and respiratory rate from wearables.To systematically evaluate PH-LLM, we create and curate a set of three benchmark datasets that test:The model’s ability to produce detailed insights and recommendations for individuals based on their measured sleep patterns, physical activity, and physiological responses.Expert-level domain knowledge.Prediction of self-reported assessments of sleep quality.\nTo evaluate PH-LLM, we curate three benchmark datasets that span long-form coaching recommendation tasks, assessments of expert domain knowledge, and prediction of self-reported sleep outcomes.\nFor the insights and recommendations tasks, we created 857 case studies from consenting US-based users across two personal health verticals: sleep and fitness. These case studies, designed in collaboration with domain experts, represent real-world coaching scenarios and highlight the model’s capabilities in understanding, reasoning, and coaching by interpreting time-series physiological data using textual representations. Through comprehensive evaluation of model responses, we observe that performance of both Gemini Ultra 1.0 and PH-LLM is not statistically different from expert performance in fitness. While recommendations written by experts are rated higher for sleep, the performance is close, and further fine-tuning PH-LLM significantly improves its ability to use relevant domain knowledge and personalize information when generating insights and predicting potential causal factors.\nBased on evaluations by human experts, fine-tuning of PH-LLM improves its ability to generate accurate insights and potential causative factors in sleep. Here we present the mean expert rating (higher is better) across evaluation rubrics for each case study subsection. Performance in fitness is not statistically different from human experts. “∗” indicates a statistically significant difference between two response types after multiple hypothesis testing correction.\nTo further assess expert domain knowledge, we evaluated PH-LLM performance on multiple choice question datasets in the style of sleep medicine and fitness certification examinations via manual test takers using online portals. PH-LLM achieved 79% on sleep (N=629 questions) and 88% on fitness (N=99 questions), both of which exceed average scores from a sample of human experts (76% and 71%, respectively) as well as benchmarks for receiving continuing education credit to maintain professional licenses in those domains.\nAMA = American Medical Association, PRA = Physician's Recognition Award, ABIM = American Board of Internal Medicine, MOC = Maintenance of Certification, CME = Continuing Medical Education, NSCA = National Strength and Conditioning Association, CSCS = Certified Strength and Conditioning Specialists.\nFinally, to enable PH-LLM to predict self-reported assessments of sleep quality, we trained the model on responses to validated survey questions on sleep disruption and impairment using textual and multimodal encoding representations of wearable sensor data. Shown below, we demonstrate that multimodal encoding is both necessary and sufficient to achieve performance on par with discriminative models trained solely to predict these outcomes.\nPredictive AUROC performance of PH-LLM model variants on self-reported sleep outcomes. Encoding multimodal sensor data with an adapter outperforms equivalent text representations for both zero- and few-shot prompting for 12 out of 16 outcomes (denoted with “*”) with statistical significance.\nTogether, these results demonstrate the benefit of tuning PH-LLM to contextualize physiological data for personal health applications.\nTransforming wearable data into personal health insightsLLMs can be augmented with software tools to extend their capabilities, examples of which include code generation and information retrieval. The ability of LLM-based agents to iteratively reason and interact with tools offers a promising way to extend their reasoning abilities to complex, temporal wearable data. In our second paper, we introduce a framework for a personal health insights agent based on Gemini Ultra 1.0. The agent leverages the power of Gemini models along with an agent framework, code generation capabilities, and information retrieval tools to iteratively analyze raw wearable data and provide personalized interpretations and recommendations to health queries. This combination enables the agent to:Analyze data from wearable devices: the agent employs a Python interpreter to analyze multi-dimensional time-series data from wearables, performing complex calculations and identifying trends.Integrate additional health knowledge: the agent accesses a knowledge base via a search engine, incorporating up-to-date medical and health information into its responses.Provide personalized insights: the agent conducts iterative multi-step reasoning through individual data, medical knowledge, and specific user queries, generating tailored insights and recommendations.\nAn example that shows how the agent reasons a personal health query step by step. This is for illustrative purposes only.\nTo evaluate the agent’s capabilities, we curated two datasets: one to test the agent's numerical accuracy in health queries, and the other to assess the quality of its reasoning and code in open-ended health queries through human annotations.On the first dataset, Objective Health Insights Queries, the agent achieved 84% accuracy on a dataset of 4,000 objective personal health insights queries, demonstrating its ability to handle numerical reasoning and data analysis.\nOur agent scores better than the Code Generation and standard LLM Numerical Reasoning baselines on objective personal health insights queries. Accuracy is based on an exact match to within two digits of precision.\nOn the second dataset, Open-Ended Health Insights Queries, we assess the agent’s performance on 172 representative open-ended personal health queries across over 600 hours of human evaluation of more than 6000 model responses. Overall, the agent significantly improved performance over a non-agent code generation baseline in nine of the 14 axes of evaluation, including key aspects like domain knowledge, logic, and reasoning quality.\nOur human and expert evaluations show that our agent outperforms the code generation baseline, indicating the importance of iterative reasoning and tool usage. “∗” indicates a statistically significant difference between average ratings.\nWhile the agent focuses on sleep and fitness data, its framework can be extended to analyze a broader range of health information, including medical records, nutrition data, and even user-provided journal entries. As LLMs continue to advance, agents have the potential to become increasingly sophisticated and may offer even deeper insights and more effective guidance for personal health management.\nConclusionOur primary goals are to research features and capabilities that may help people live longer, healthier lives. Sleep and fitness are crucial components of population health and are predictors of premature mortality worldwide. The capabilities enabled by our research in case studies, personal health domain knowledge, and open-ended queries in sleep and fitness represent a meaningful step toward AI models that support personalized insights and recommendations that enable individuals to draw accurate and actionable conclusions from their own health data. We look forward to careful testing and understanding which capabilities are most helpful for users.\nAcknowledgementsThe research described here is joint work across Google Research, Google Health, Google DeepMind and partnering teams.",
    "favicon": "/static/assets/favicon.ico"
  },
  {
    "title": "Google’s True Moonshot",
    "link": "https://stratechery.com/2023/googles-true-moonshot/",
    "source": "Stratechery by Ben Thompson",
    "datetime": "2023-12-18T08:00:00.000Z",
    "time": "Dec 18, 2023",
    "articleType": "regular",
    "content": "This Article is available as a video essay on YouTube\nWhen I first went independent with Stratechery, I had a plan to make money on the side with speaking, consulting, etc.; what made me pull the plug on the latter was my last company speaking gig, with Google in November 2015 (I have always disclosed this on my About page). It didn’t seem tenable for me to have any sort of conflict of interest with companies I was covering, and the benefit of learning more about the companies I covered — the justification I told myself for taking the engagement — was outweighed by the inherent limitations that came from non-public data. And so, since late 2015, my business model has been fully aligned to my nature: fully independent, with access to the same information as everyone else.1\nI bring this up for three reasons, that I shall get to through the course of this Article. The first one has to do with titles: it was at that talk that a Google employee asked me what I thought of invoking the then-unannounced Google Assistant by saying “OK Google”. “OK Google” was definitely a different approach from Apple and Amazon’s “Siri” and “Alexa”, respectively, and I liked it: instead of pretending that the assistant was the dumbest human you have ever talked to, why not portray it as the smartest robot, leaning on the brand name that Google had built over time?\n“OK Google” was, in practice, not as compelling as I hoped. It was better than Siri or Alexa, but it had all of the same limitations that were inherent to the natural language processing approach: you had to get the incantations right to get the best results, and the capabilities and responses were ultimately more deterministic than you might have hoped. That, though, wasn’t necessarily a problem for the brand: Google search is, at its core, still about providing the right incantations to get the set of results you are hoping for; Google Assistant, like Search, excelled in more mundane but critical attributes like speed and accuracy, if not personality and creativity.\nWhat was different from search is that an Assistant needed to provide one answer, not a list of possible answers. This, though, was very much in keeping with Google’s fundamental nature; I once wrote in a Stratechery Article:\nAn assistant has to be far more proactive than, for example, a search results page; it’s not enough to present possible answers: rather, an assistant needs to give the right answer.\nThis is a welcome shift for Google the technology; from the beginning the search engine has included an “I’m Feeling Lucky” button, so confident was Google founder Larry Page that the search engine could deliver you the exact result you wanted, and while yesterday’s Google Assistant demos were canned, the results, particularly when it came to contextual awareness, were far more impressive than the other assistants on the market. More broadly, few dispute that Google is a clear leader when it comes to the artificial intelligence and machine learning that underlie their assistant.\nThat paragraph was from Google and the Limits of Strategy, where I first laid out some of the fundamental issues that have, over the last year, come into much sharper focus. On one hand, Google had the data, infrastructure, and customer touch points to win the “Assistant” competition; that remains the case today when it comes to generative AI, which promises the sort of experience I always hoped for from “OK Google.” On the other hand, “I’m feeling lucky” may have been core to Google’s nature, but it was counter to their business model; I continued in that Article:\nA business, though, is about more than technology, and Google has two significant shortcomings when it comes to assistants in particular. First, as I explained after this year’s Google I/O, the company has a go-to-market gap: assistants are only useful if they are available, which in the case of hundreds of millions of iOS users means downloading and using a separate app (or building the sort of experience that, like Facebook, users will willingly spend extensive amounts of time in).\nSecondly, though, Google has a business-model problem: the “I’m Feeling Lucky Button” guaranteed that the search in question would not make Google any money. After all, if a user doesn’t have to choose from search results, said user also doesn’t have the opportunity to click an ad, thus choosing the winner of the competition Google created between its advertisers for user attention. Google Assistant has the exact same problem: where do the ads go?\nIt is now eight years on from that talk, and seven years on from the launch of Google Assistant, but all of the old questions are as pertinent as ever.\nMy first point brings me to the second reason I’m reminded of that Google talk: my presentation was entitled “The Opportunity — and the Enemy.” The opportunity was mobile, the best market the tech industry had ever seen; the enemy was Google itself, which even then was still under-investing in its iOS apps.\nIn the presentation I highlighted the fact that Google’s apps still didn’t support Force Touch, which Apple had introduced to iOS over a year earlier; to me this reflected the strategic mistake the company made in prioritizing Google Maps on Android, which culminated in Apple making its own mapping service. My point was one I had been making on Stratechery from the beginning: Google was a services company, which meant their optimal strategy was to serve all devices; by favoring Android they were letting the tail wag the dog.\nEight years on, and it’s clear I wasn’t the only one who saw the Maps fiasco as a disaster to be learned from: one of the most interesting revelations from the ongoing DOJ antitrust case against Google was reported by Bloomberg:\nTwo years after Apple Inc. dropped Google Maps as its default service on iPhones in favor of its own app, Google had regained only 40% of the mobile traffic it used to have on its mapping service, a Google executive testified in the antitrust trial against the Alphabet Inc. company. Michael Roszak, Google’s vice president for finance, said Tuesday that the company used the Apple Maps switch as “a data point” when modeling what might happen if the iPhone maker replaced Google’s search engine as the default on Apple’s Safari browser.\nIt’s a powerful data point, and I think the key to understanding what you might call the Google Aggregator Paradox: if Google wins by being better, then why does it fight so hard for defaults, both for search and, in the case of Android, the Play Store? The answer, I think, is that it is best to not even take the chance of alternative defaults being good enough. This is made easier given the structure of these deals, which are revenue shares, not payments; this does show up on Google’s income statement as Traffic Acquisition Costs (TAC), but from a cash flow perspective it is foregone zero marginal cost revenue. There is no pain of payment, just somewhat lower profitability on zero marginal cost searches.\nThe bigger cost is increasingly legal: the decision in the DOJ case won’t come down until next year, and Google may very well win; it’s hard to argue that the company ought not be able to bid on Apple’s default search placement if its competitors can (if anything the case demonstrates Apple’s power).\nThat’s not Google’s only legal challenge, though: last week the company lost another antitrust case, this time to Epic. I explained why the company lost — while Apple won — in last Tuesday’s Update:\nThat last point may seem odd in light of Apple’s victory, but again, Apple was offering an integrated product that it fully controlled and customers were fully aware of, and is thus, under U.S. antitrust law, free to set the price of entry however it chooses. Google, on the other hand, “entered into one or more agreements that unreasonably restrained trade” — that quote is from the jury instructions, and is taken directly from the Sherman Act — by which the jurors mean basically all of them: the Google Play Developer Distribution Agreement, investment agreements under the Games Velocity Program (i.e. Project Hug), and Android’s mobile application distribution agreement and revenue share agreements with OEMs, were all ruled illegal.\nThis goes back to the point I made above: Google’s fundamental legal challenge with Android is that it sought to have its cake and eat it too: it wanted all of the shine of open source and all of the reach and network effects of being a horizontal operating system provider and all of the control and profits of Apple, but the only way to do that was to pretty clearly (in my opinion) violate antitrust law.\nGoogle’s Android strategy was, without question, brilliant, particularly when you realize that the ultimate goal was to protect search. By making it “open source”, Google got all of the phone carriers desperate for an iOS alternative on board, ensuring that hated rival Microsoft was not the alternative to Apple as it had been on PCs; a modular approach, though, is inherently more fragmented — and Google didn’t just want an alternative to Apple, they wanted to beat them, particularly in the early days of the smartphone wars — so the company spun a web of contracts and incentives to ensure that Android was only really usable with Google’s services. For this the company was rightly found guilty of antitrust violations in the EU, and now, for similar reasons, in the U.S.\nThe challenge for Google is that the smartphone market has a lot more friction than search: the company needs to coordinate both OEMs and developers; when it came to search the company could simply take advantage of the openness of the web. This resulted in tension between Google’s nature — being the one-stop shop for information — and the business model of being a horizontal app platform and operating system provider. It’s not dissimilar to the tension the company faces with its Assistant, and in the future with Generative AI: the company wants to simply give you the answer, but how to do that while still making money?\nThe third reason I remember that weekend in 2015 is it was the same month that Google open-sourced TensorFlow, its machine-learning framework. I thought it was a great move, and wrote in TensorFlow and Monetizing Intellectual Property:\nI’m hardly qualified to judge the technical worth of TensorFlow, but I feel pretty safe in assuming that it is excellent and likely far beyond what any other company could produce. Machine learning, though, is about a whole lot more than a software system: specifically, it’s about a whole lot of data, and an infrastructure that can process that data. And, unsurprisingly, those are two areas where Google has a dominant position.\nIndeed, as good as TensorFlow might be, I bet it’s the weakest of these three pieces Google needs to truly apply machine learning to all its various business, both those of today and those of the future. Why not, then, leverage the collective knowledge of machine learning experts all over the world to make TensorFlow better? Why not make a move to ensure the machine learning experts of the future grow up with TensorFlow as the default? And why not ensure that the industry’s default machine learning system utilizes standards set in place by Google itself, with a design already suited for Google’s infrastructure?\nAfter all, contra Gates’ 2005 claim, it turns out the value of pure intellectual property is not derived from government-enforced exclusivity, but rather from the complementary pieces that surround that intellectual property which are far more difficult to replicate. Google is betting that its lead in both data and infrastructure are significant and growing, and that’s a far better bet in my mind than an all-too-often futile attempt to derive value from an asset that by its very nature can be replicated endlessly.\nIn fact, it turned out that TensorFlow was not so excellent — that link I used to support my position in the above excerpt now 404s — and it has been surpassed by Meta’s PyTorch in particular; at Google Cloud Next the company announced a partnership with Nvidia to build out OpenXLA as a compiler of sorts to ensure that output from TensorFlow, Jax, and PyTorch can run on any hardware. This matters for Google because those infrastructure advantages very much exist; the more important “Tensor” product for Google is its Tensor Processing Unit series of chips, the existence of which make Google uniquely able to scale beyond whatever allocation it can get of Nvidia GPUs.\nThe importance of TPUs was demonstrated with the announcement of Gemini, Google’s latest AI model; the company claims the “Ultra” variant, which it hasn’t yet released, is better than GPT-4. What is notable is that Gemini was trained and will run inference on TPUs. While there are some questions about the ultimate scalability of TPUs, for now Google is the best positioned to both train and, more importantly, serve generative AI in a cost efficient way.\nThen there is data: a recent report in The Information claims that Gemini relies heavily on data from YouTube, and that is not the only proprietary data Google has access to: free Gmail and Google Docs are another massive resource, although it is unclear to what extent Google is using that data, or if it is, for what. At a minimum there is little question that Google has the most accessible repository of Internet data going back a quarter of a century to when Larry Page and Sergey Brin first started crawling the open web from their dorm room.\nAnd so we are back where we started: Google has incredible amounts of data and the best infrastructure, but once again, an unsteady relationship with the broader development community.\nThe part of the Gemini announcement that drew the most attention did not have anything to do with infrastructure or data: what everyone ended up talking about was the company’s Gemini demo, and the fact it wasn’t representative of Gemini’s actual capabilities. Here’s the demo:\nParmy Olson for Bloomberg Opinion was the first to highlight the problem:\nIn reality, the demo also wasn’t carried out in real time or in voice. When asked about the video by Bloomberg Opinion, a Google spokesperson said it was made by “using still image frames from the footage, and prompting via text,” and they pointed to a site showing how others could interact with Gemini with photos of their hands, or of drawings or other objects. In other words, the voice in the demo was reading out human-made prompts they’d made to Gemini, and showing them still images. That’s quite different from what Google seemed to be suggesting: that a person could have a smooth voice conversation with Gemini as it watched and responded in real time to the world around it.\nThis was obviously a misstep, and a bizarre one at that: as I noted in an Update Google, given its long-term advantages in this space, would have been much better served in being transparent, particularly since it suddenly finds itself with a trustworthiness advantage relative to Microsoft and OpenAI. The goal for the company should be demonstrating competitiveness and competence; a fake demo did the opposite.\nAnd yet, I can understand how the demo came to be; it is getting close to the holy grail of Assistants: an entity with which you can conduct a free-flowing conversation, without the friction of needing to invoke the right incantations or type and read big blocks of text. If Gemini Ultra really is better than GPT-4, or even roughly competitive, than I believe this capability is close. After all, I got a taste of it with GPT-4 and its voice capabilities; from AI, Hardware, and Virtual Reality:\nThe first AI announcement of the week was literally AI that can talk: OpenAI announced that you can now converse with ChatGPT, and I found the experience profound.\nYou have obviously been able to chat with ChatGPT via text for many months now; what I only truly appreciated after talking with ChatGPT, though, was just how much work it was to type out questions and read answers. There was, in other words, a human constraint in our conversations that made it feel like I was using a tool; small wonder that the vast majority of my interaction with ChatGPT has been to do some sort of research, or try to remember something on the edge of my memory, too fuzzy to type a clear search term into Google.\nSimply talking, though, removed that barrier: I quickly found myself having philosophical discussions including, for example, the nature of virtual reality. It was the discussion itself that provided a clue: virtual reality feels real, but something can only feel real if human constraints are no longer apparent. In the case of conversation, there is no effort required to talk to another human in person, or on the phone; to talk to them via chat is certainly convenient, but there is a much more tangible separation. So it is with ChatGPT.\nThe problem is that this experience requires a pretty significant suspension of disbelief, because there is too much friction. You have to open the OpenAI app, then you have to set it to voice mode, then you have to wait for it to connect, then every question and answer contains a bit too much lag, and the answers start sounding like blocks of text instead of a conversation. Notice, though, that Google is much better placed than OpenAI to solve all of these challenges:\nGoogle sells its own phones which could be configured to have a conversation UI by default (or with Google’s Pixel Buds). This removes the friction of opening an app and setting a mode. Google also has a fleet of home devices already designed for voice interaction.\nGoogle has massive amounts of infrastructure all over the globe, with the lowest latency and fastest response. This undergirds search today, but it could undergird a new generative AI assistant tomorrow.\nGoogle has access to gobs of data specifically tied to human vocal communication, thanks to YouTube in particular.\nIn short, the Gemini demo may have been faked, but Google is by far the company best positioned to make it real.\nThere was one other interesting tidbit in The Information article (emphasis mine):\nOver the next few months, Google will have to show it can integrate the AI models it groups under the Gemini banner into its products, without cannibalizing existing businesses such as search. It has already put a less advanced version of Gemini into Bard, the chatbot it created to compete with ChatGPT, which has so far seen limited uptake. In the future, it plans to use Gemini across nearly its entire line of products, from its search engine to its productivity applications and an AI assistant called Pixie that will be exclusive to its Pixel devices, two people familiar with the matter said. Products could also include wearable devices, such as glasses that could make use of the AI’s ability to recognize the objects a wearer is seeing, according to a person with knowledge of internal discussions. The device could then advise them, say, on how to use a tool, solve a math problem or play a musical instrument.\nThe details of Pixie, such as they were, came at the very end:\nThe rollout of Pixie, an AI assistant exclusively for Pixel devices, could boost Google’s hardware business at a time when tech companies are racing to integrate their hardware with new AI capabilities. Pixie will use the information on a customer’s phone — including data from Google products like Maps and Gmail — to evolve into a far more personalized version of the Google Assistant, according to one of the people with knowledge of the project. The feature could launch as soon as next year with the Pixel 9 and the 9 Pro, this person said.\nThat Google is readying a super-charged version of the Google Assistant is hardly a surprise; what is notable is the reporting that it will be exclusive to Pixel devices. This is counter to Gemini itself: the Gemini Nano model, which is designed to run on smartphones, will be available to all Android devices with neural processing units like Google’s Tensor G3. That is very much in-line with the post-Maps Google: services are the most valuable when they are available everywhere, and Pixel has a tiny amount of marketshare.\nThat, by extension, makes me think that the “Pixie exclusive to Pixel” report is mistaken, particularly since I’ve been taken in by this sort of thing before. That Google Assistant piece I quote above — Google and the Limits of Strategy — interpreted the launch of Google Assistant on Pixel devices as evidence that Google was trying to differentiate its own hardware:\nToday’s world, though, is not one of (somewhat) standards-based browsers that treat every web page the same, creating the conditions for Google’s superior technology to become the door to the Internet; it is one of closed ecosystems centered around hardware or social networks, and having failed at the latter, Google is having a go at the former. To put it more generously, Google has adopted Alan Kay’s maxim that “People who are really serious about software should make their own hardware.” To that end the company introduced multiple hardware devices, including a new phone, the previously-announced Google Home device, new Chromecasts, and a new VR headset. Needless to say, all make it far easier to use Google services than any 3rd-party OEM does, much less Apple’s iPhone.\nWhat is even more interesting is that Google has also introduced a new business model: the Pixel phone starts at $649, the same as an iPhone, and while it will take time for Google to achieve the level of scale and expertise to match Apple’s profit margins, the fact there is unquestionably a big margin built-in is a profound new direction for the company.\nThe most fascinating point of all, though, is how Google intends to sell the Pixel: the Google Assistant is, at least for now, exclusive to the first true Google phone, delivering a differentiated experience that, at least theoretically, justifies that margin. It is a strategy that certainly sounds familiar, raising the question of whether this is a replay of the turn-by-turn navigation disaster. Is Google forgetting that they are a horizontal company, one whose business model is designed to maximize reach, not limit it?\nMy argument was that Google was in fact being logical, for the business model reasons I articulated both in that Article and at the beginning of this year in AI and the Big Five: simply giving the user the right answer threatened the company’s core business model, which meant it made sense to start diversifying into new ones. And then, just a few months later, Google Assistant was available to other Android device makers. It was probably the right decision, for the same reason that the company should have never diminished its iOS maps product in favor of Android.\nAnd yet, all of the reasoning I laid out for making the Google Assistant a differentiator still hold: AI is a threat to Search for all of the same reasons I laid out in 2016, and Google is uniquely positioned to create the best Assistant. The big potential difference with Pixie is that it might actually be good, and a far better differentiator than the Google Assistant. The reason, remember, is not just about Gemini versus GPT-4: it’s because Google actually sells hardware, and has the infrastructure and data to back it up.\nGoogle’s collection of moonshots — from Waymo to Google Fiber to Nest to Project Wing to Verily to Project Loon (and the list goes on) — have mostly been science projects that have, for the most part, served to divert profits from Google Search away from shareholders. Waymo is probably the most interesting, but even if it succeeds, it is ultimately a car service rather far afield from Google’s mission statement “to organize the world’s information and make it universally accessible and useful.”\nWhat, though, if the mission statement were the moonshot all along? What if “I’m Feeling Lucky” were not a whimsical button on a spartan home page, but the default way of interacting with all of the world’s information? What if an AI Assistant were so good, and so natural, that anyone with seamless access to it simply used it all the time, without thought?\nThat, needless to say, is probably the only thing that truly scares Apple. Yes, Android has its advantages to iOS, but they aren’t particularly meaningful to most people, and even for those that care — like me — they are not large enough to give up on iOS’s overall superior user experience. The only thing that drives meaningful shifts in platform marketshare are paradigm shifts, and while I doubt the v1 version of Pixie would be good enough to drive switching from iPhone users, there is at least a path to where it does exactly that.\nOf course Pixel would need to win in the Android space first, and that would mean massively more investment by Google in go-to-market activities in particular, from opening stores to subsidizing carriers to ramping up production capacity. It would not be cheap, which is why it’s no surprise that Google hasn’t truly invested to make Pixel a meaningful player in the smartphone space.\nThe potential payoff, though, is astronomical: a world with Pixie everywhere means a world where Google makes real money from selling hardware, in addition to services for enterprises and schools, and cloud services that leverage Google’s infrastructure to provide the same capabilities to businesses. Moreover, it’s a world where Google is truly integrated: the company already makes the chips, in both its phones and its data centers, it makes the models, and it does it all with the largest collection of data in the world.\nThis path does away with the messiness of complicated relationships with OEMs and developers and the like, which I think suits the company: Google, at its core, has always been much more like Apple than Microsoft. It wants to control everything, it just needs to do it legally; that the best manifestation of AI is almost certainly dependent on a fully integrated (and thus fully seamless) experience means that the company can both control everything and, if it pulls this gambit off, serve everyone.\nThe problem is that the risks are massive: Google would not only be risking search revenue, it would also estrange its OEM partners, all while spending astronomical amounts of money. The attempt to be the one AI Assistant that everyone uses — and pays for — is the polar opposite of the conservative approach the company has taken to the Google Aggregator Paradox. Paying for defaults and buying off competitors is the strategy of a company seeking to protect what it has; spending on a bold assault on the most dominant company in tech is to risk it all.\nAnd yet, to simply continue on the current path, folding AI into its current products and selling it via Google Cloud, is a risk of another sort. Google is not going anywhere anytime soon, and Search has a powerful moat in terms of usefulness, defaults, and most critically, user habits; Google Cloud, no matter the scenario, remains an attractive way to monetize Google AI and leverage its infrastructure, and perhaps that will be seen as enough. Where will such a path lead in ten or twenty years, though?\nUltimately, this is a question for leadership, and I thought Daniel Gross’s observation on this point in the recent Stratechery Interview with him and Nat Friedman was insightful:\nSo to me, yeah, does Google figure out how to master AI in the infrastructure side? Feels pretty obvious, they’ll figure it out, it’s not that hard. The deeper question is, on the much higher margin presumably, consumer angle, do they just cede too much ground to startups, Perplexity or ChatGPT or others? I don’t know what the answer is there and forecasting that answer is a little bit hard because it probably literally depends on three or four people at Google and whether they want to take the risk and do it.\nWe definitively know that if the founders weren’t in the story — we could not definitively, but forecast with pretty good odds — that it would just run its course and it would gradually lose market share over time and we’d all sail into a world of agents. However, we saw Sergey Brin as an individual contributor on the Gemini paper and we have friends that work on Gemini and they say that’s not a joke, he is involved day-to-day. He has a tremendous amount of influence, power, and control over Google so if he’s staring at that, together with his co-founder, I do think they could overnight kill a lot of startups, really damage ChatGPT, and just build a great product, but that requires a moment of [founder initiative].\nIt’s possible, it’s just hard to forecast if they will do it or not. In my head, that is the main question that matters in terms of whether Google adds or loses a zero. I think they’ll build the capability, there’s no doubt about it.\nI agree. Google could build the AI to win it all. It’s not guaranteed they would succeed, but the opportunity is there if they want to go for it. That is the path that would be in the nature of the Google that conquered the web twenty years ago, the Google that saw advertising as the easiest way to monetize what was an unbridled pursuit of self-contained technological capability.\nThe question is if that nature been superceded by one focused on limiting losses and extracting profits; yes, there is still tremendous technological invention, but as Horace Dediu explained on Asymco, that is different than innovation, which means actually making products that move markets. Can Google still do that? Do they want to? Whither Google?\nI do still speak at conferences, but last spoke for pay in January 2017",
    "favicon": "/favicon/favicon-32x32.png"
  },
  {
    "title": "How we built and tested body temperature on Pixel 8 Pro",
    "link": "https://blog.google/products/pixel/google-thermometer-app-body-temperature/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXZSMFZhU1daMmFVeEdVblk1VFJDUkFSamNBaWdCTWdhQlFZeG51Z1U=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-25T08:00:00.000Z",
    "time": "Jan 25",
    "articleType": "regular",
    "content": "Jim Taylor, a research scientist with Google Health, knows a thing or two about taking temperatures. That's because in addition to his role at Google, he's also a pediatrician. “The first thing I ask a parent when they call and say ‘My child has a fever’ is ‘What’s their temperature?’” he says.For six years, Jim has been part of the team at Google Health working to bring health tools to Pixel phones. One of the first of those tools is the new body temperature feature in the Thermometer app for Pixel 8 Pro. Part of the January Pixel feature drop, it allows you to quickly scan a person’s forehead with your phone and measure the body temperature. In clinical trials, our software algorithm was able to calculate body temperature in the range of 96.9°F - 104°F (36.1°C - 40°C) to within ±0.3°C when compared with an FDA-cleared temporal artery thermometer. In layman's terms, this means the Pixel body temperature feature is about as accurate as other temporal artery thermometers.Ravi Narasimhan, a research and development technical lead at Google, developed a miniaturized device that included an infrared sensor for body temperature measurement, which eventually evolved to become a feature for the Pixel phone. “You always have your phone with you, so it’s more convenient to measure body temperature without an additional device,” says Ravi.Last month, we received the FDA’s De Novo classification1 for our body temperature app, the first for smartphones in the U.S. And as of this week, it’s now available on Pixel 8 Pro. Here's a look at how the team built and tested it.How we built the first smartphone body temperature appBodies emit infrared radiation — or, simply put, heat. The Pixel 8 Pro team added an infrared sensor to the phone next to the rear camera, which at launch was used to power the object temperature feature and now powers the body temperature feature as well. Measuring body temperature on the Pixel 8 Pro is simple: Just point the rear camera at the forehead and sweep across. The Pixel 8 Pro body temperature app accurately measures your temperature by scanning the temporal artery, unlike less accurate forehead thermometers that are pointed at the center of the forehead. The data from the infrared sensor is passed to an algorithm to calculate the temperature that will be displayed on your device, powered by the Tensor G3 chip.The Pixel 8 Pro’s infrared sensor’s wide field of view (more than 130 degrees) causes it to sense heat beyond the forehead when the phone is too far away from the forehead. “It’s basically a big cone that the sensor takes in,” Ravi says. “Arteries are relatively small, so the closer you are, the more accurate reading you will get.”\nThe team wanted users to hold the phone close to their foreheads without touching. “We want this to be contactless to prevent the spread of germs, and we found in testing that telling someone to bring their phones within a half inch of their forehead just by estimating was very difficult,” Toni Urban, a Pixel product manager who focuses on health experiences, explains. “When you use most apps, you’re looking at the screen — but that’s not the case here if you’re using it on your own forehead.” The solution was to use another Pixel sensor. “We decided to use the LDAF (laser detection autofocus) sensor, which typically powers the autofocus system, to detect if the phone is close enough to a person’s forehead before initiating a measurement,” Toni says. Using the LDAF sensor, the Pixel 8 Pro knows if it’s in the best position to take a reading. Additionally, the team implemented haptics and audio instructions to guide users to sweep the phone across the forehead and past the temporal artery.The FDA De Novo grant processThe Pixel 8 Pro's temperature sensor has broad uses, but the team felt accurate body temperature readings were most important because of their health implications. Since thermometers are regulated medical devices, the team not only had to run the feature through rigorous testing, but also had to undergo an exhaustive review as part of a De Novo classification request with the FDA before we could launch the feature in the United States. As part of that process, “we had to do a clinical validation study, meaning we tested the product on a large number of participants,” Jim says.Developing a tool that’s safe and easy to use was a big part of getting the FDA’s grant. “We built it so that it can run on the phone without any internet connection,” Toni says. “It’s something that’s available even if you are traveling or in an area without cell phone reception.” And the team also had to implement instructions that were easy to understand. “The idea was to make sure that every person can reliably use this tool,” Toni says. This approach was successful.The team is proud of the product’s accuracy, but they are most excited about its convenience and ease of use. “Now this gives people an easy way to get their or their child’s temperature,” Jim says. “To me, that’s the best part about this.”",
    "favicon": "/favicon.ico"
  },
  {
    "title": "New Maps updates: Immersive View for routes and other AI features",
    "link": "https://blog.google/products/maps/google-maps-october-2023-update/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTBRWE42TTFjMmVIUjJXbVZyVFJDUkFSamNBaWdCTWdZUk1Zd0lvZ2c=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-10-26T07:00:00.000Z",
    "time": "Oct 26, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Circle (or highlight or scribble) to Search",
    "link": "https://blog.google/products/search/google-circle-to-search-android/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUthMVE1YkZNeFRHWkJMVXhzVFJDb0FSaXNBaWdCTWdhTllvNXRMUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-17T08:00:00.000Z",
    "time": "Jan 17",
    "articleType": "regular",
    "content": "Circle to Search can help you quickly identify items in a photo or video. Relevant ads will continue to appear in dedicated slots throughout the page.\nOr perhaps you’re on YouTube Shorts and you notice a video covering an unfamiliar topic: thrift flipping. With Circle to Search, you can quickly scribble over the text “thrift flip” to learn that it’s the process of purchasing items from a thrift store, fixing them up and then reselling them for a profit. Now that you know, you can easily close out and resume watching the video.\nCircle to Search is launching January 31 on select premium Android smartphones — the Pixel 8, Pixel 8 Pro and the new Samsung Galaxy S24 series — in all languages and locations where they’re available.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Our 2024 Environmental Report",
    "link": "https://blog.google/outreach-initiatives/sustainability/2024-environmental-report/R",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVZZWFpYUWxKbFpHSkJjamxQVFJDM0FSaVRBaWdCTWdZQkFJYkVGQVU=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-07-02T07:00:00.000Z",
    "time": "Jul 2",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google’s AI Overviews misunderstand why people use Google",
    "link": "https://arstechnica.com/ai/2024/06/googles-ai-overviews-misunderstand-why-people-use-google/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNVRUWEF4YjJJMFdFcEhRMjFJVFJDb0FSaXNBaWdCTWdtTlpJck1LYVdoTWdF=-w400-h224-p-df-rw",
    "source": "Ars Technica",
    "datetime": "2024-06-04T07:00:00.000Z",
    "time": "Jun 4",
    "articleType": "regular",
    "content": "Aurich Lawson | Getty Images\nLast month, we looked into some of the most incorrect, dangerous, and downright weird answers generated by Google's new AI Overviews feature. Since then, Google has offered a partial apology/explanation for generating those kinds of results and has reportedly rolled back the feature's rollout for at least some types of queries.\nBut the more I've thought about that rollout, the more I've begun to question the wisdom of Google's AI-powered search results in the first place. Even when the system doesn't give obviously wrong results, condensing search results into a neat, compact, AI-generated summary seems like a fundamental misunderstanding of how people use Google in the first place.\nWhen people type a question into the Google search bar, they only sometimes want the kind of basic reference information that can be found on a Wikipedia page or corporate website (or even a Google information snippet). Often, they're looking for subjective information where there is no one \"right\" answer: \"What are the best Mexican restaurants in Santa Fe?\" or \"What should I do with my kids on a rainy day?\" or \"How can I prevent cheese from sliding off my pizza?\"\nThe value of Google has always been in pointing you to the places it thinks are likely to have good answers to those questions. But it's still up to you, as a user, to figure out which of those sources is the most reliable and relevant to what you need at that moment.\nThis wasn't funny when the guys at Pep Boys said it, either. (via)\nWeird Al recommends \"running with scissors\" as well! (via)\nThis list of steps actually comes from a forum thread response about doing something completely different. (via)\nAn island that's part of the mainland? (via)\nIf everything's cheaper now, why does everything seem so expensive?\nPretty sure this Truman was never president... (via)\nFor reliability, any savvy Internet user makes use of countless context clues when judging a random Internet search result. Do you recognize the outlet or the author? Is the information from someone with seeming expertise/professional experience or a random forum poster? Is the site well-designed? Has it been around for a while? Does it cite other sources that you trust, etc.?\nBut Google also doesn't know ahead of time which specific result will fit the kind of information you're looking for. When it comes to restaurants in Santa Fe, for instance, are you in the mood for an authoritative list from a respected newspaper critic or for more off-the-wall suggestions from random locals? Or maybe you scroll down a bit and stumble on a loosely related story about the history of Mexican culinary influences in the city.\nOne of the unseen strengths of Google's search algorithm is that the user gets to decide which results are the best for them. As long as there's something reliable and relevant in those first few pages of results, it doesn't matter if the other links are \"wrong\" for that particular search or user.",
    "favicon": "https://cdn.arstechnica.net/favicon.ico"
  },
  {
    "title": "Our next-generation model: Gemini 1.5",
    "link": "https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVlTVzlIUkUxZlZtRnRhaTFQVFJDb0FSaXNBaWdCTWdZRk5aSWtOZ1k=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-15T08:00:00.000Z",
    "time": "Feb 15",
    "articleType": "regular",
    "content": "The model delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.\nA note from Google and Alphabet CEO Sundar Pichai:Last week, we rolled out our most capable model, Gemini 1.0 Ultra, and took a significant step forward in making Google products more helpful, starting with Gemini Advanced. Today, developers and Cloud customers can begin building with 1.0 Ultra too — with our Gemini API in AI Studio and in Vertex AI.Our teams continue pushing the frontiers of our latest models with safety at the core. They are making rapid progress. In fact, we’re ready to introduce the next generation: Gemini 1.5. It shows dramatic improvements across a number of dimensions and 1.5 Pro achieves comparable quality to 1.0 Ultra, while using less compute.This new generation also delivers a breakthrough in long-context understanding. We’ve been able to significantly increase the amount of information our models can process — running up to 1 million tokens consistently, achieving the longest context window of any large-scale foundation model yet.Longer context windows show us the promise of what is possible. They will enable entirely new capabilities and help developers build much more useful models and applications. We’re excited to offer a limited preview of this experimental feature to developers and enterprise customers. Demis shares more on capabilities, safety and availability below.— Sundar\nIntroducing Gemini 1.5By Demis Hassabis, CEO of Google DeepMind, on behalf of the Gemini teamThis is an exciting time for AI. New advances in the field have the potential to make AI more helpful for billions of people over the coming years. Since introducing Gemini 1.0, we’ve been testing, refining and enhancing its capabilities.Today, we’re announcing our next-generation model: Gemini 1.5.Gemini 1.5 delivers dramatically enhanced performance. It represents a step change in our approach, building upon research and engineering innovations across nearly every part of our foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture.The first Gemini 1.5 model we’re releasing for early testing is Gemini 1.5 Pro. It’s a mid-size multimodal model, optimized for scaling across a wide-range of tasks, and performs at a similar level to 1.0 Ultra, our largest model to date. It also introduces a breakthrough experimental feature in long-context understanding.Gemini 1.5 Pro comes with a standard 128,000 token context window. But starting today, a limited group of developers and enterprise customers can try it with a context window of up to 1 million tokens via AI Studio and Vertex AI in private preview.As we roll out the full 1 million token context window, we’re actively working on optimizations to improve latency, reduce computational requirements and enhance the user experience. We’re excited for people to try this breakthrough capability, and we share more details on future availability below.These continued advances in our next-generation models will open up new possibilities for people, developers and enterprises to create, discover and build using AI.\nContext lengths of leading foundation models\nHighly efficient architectureGemini 1.5 is built upon our leading research on Transformer and MoE architecture. While a traditional Transformer functions as one large neural network, MoE models are divided into smaller \"expert” neural networks.Depending on the type of input given, MoE models learn to selectively activate only the most relevant expert pathways in its neural network. This specialization massively enhances the model’s efficiency. Google has been an early adopter and pioneer of the MoE technique for deep learning through research such as Sparsely-Gated MoE, GShard-Transformer, Switch-Transformer, M4 and more.Our latest innovations in model architecture allow Gemini 1.5 to learn complex tasks more quickly and maintain quality, while being more efficient to train and serve. These efficiencies are helping our teams iterate, train and deliver more advanced versions of Gemini faster than ever before, and we’re working on further optimizations.\nGreater context, more helpful capabilitiesAn AI model’s “context window” is made up of tokens, which are the building blocks used for processing information. Tokens can be entire parts or subsections of words, images, videos, audio or code. The bigger a model’s context window, the more information it can take in and process in a given prompt — making its output more consistent, relevant and useful.Through a series of machine learning innovations, we’ve increased 1.5 Pro’s context window capacity far beyond the original 32,000 tokens for Gemini 1.0. We can now run up to 1 million tokens in production.This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we’ve also successfully tested up to 10 million tokens.Complex reasoning about vast amounts of information1.5 Pro can seamlessly analyze, classify and summarize large amounts of content within a given prompt. For example, when given the 402-page transcripts from Apollo 11’s mission to the moon, it can reason about conversations, events and details found across the document.\nGemini 1.5 Pro can understand, reason about and identify curious details in the 402-page transcripts from Apollo 11’s mission to the moon.\nBetter understanding and reasoning across modalities1.5 Pro can perform highly-sophisticated understanding and reasoning tasks for different modalities, including video. For instance, when given a 44-minute silent Buster Keaton movie, the model can accurately analyze various plot points and events, and even reason about small details in the movie that could easily be missed.\nGemini 1.5 Pro can identify a scene in a 44-minute silent Buster Keaton movie when given a simple line drawing as reference material for a real-life object.\nRelevant problem-solving with longer blocks of code1.5 Pro can perform more relevant problem-solving tasks across longer blocks of code. When given a prompt with more than 100,000 lines of code, it can better reason across examples, suggest helpful modifications and give explanations about how different parts of the code works.\nGemini 1.5 Pro can reason across 100,000 lines of code giving helpful solutions, modifications and explanations.\nEnhanced performanceWhen tested on a comprehensive panel of text, code, image, audio and video evaluations, 1.5 Pro outperforms 1.0 Pro on 87% of the benchmarks used for developing our large language models (LLMs). And when compared to 1.0 Ultra on the same benchmarks, it performs at a broadly similar level.Gemini 1.5 Pro maintains high levels of performance even as its context window increases. In the Needle In A Haystack (NIAH) evaluation, where a small piece of text containing a particular fact or statement is purposely placed within a long block of text, 1.5 Pro found the embedded text 99% of the time, in blocks of data as long as 1 million tokens.Gemini 1.5 Pro also shows impressive “in-context learning” skills, meaning that it can learn a new skill from information given in a long prompt, without needing additional fine-tuning. We tested this skill on the Machine Translation from One Book (MTOB) benchmark, which shows how well the model learns from information it’s never seen before. When given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person learning from the same content.As 1.5 Pro’s long context window is the first of its kind among large-scale models, we’re continuously developing new evaluations and benchmarks for testing its novel capabilities.For more details, see our Gemini 1.5 Pro technical report.\nExtensive ethics and safety testingIn line with our AI Principles and robust safety policies, we’re ensuring our models undergo extensive ethics and safety tests. We then integrate these research learnings into our governance processes and model development and evaluations to continuously improve our AI systems.Since introducing 1.0 Ultra in December, our teams have continued refining the model, making it safer for a wider release. We’ve also conducted novel research on safety risks and developed red-teaming techniques to test for a range of potential harms.In advance of releasing 1.5 Pro, we've taken the same approach to responsible deployment as we did for our Gemini 1.0 models, conducting extensive evaluations across areas including content safety and representational harms, and will continue to expand this testing. Beyond this, we’re developing further tests that account for the novel long-context capabilities of 1.5 Pro.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Attorney General Bonta Announces $93 Million Settlement Regarding Google’s Location-Privacy Practices",
    "link": "https://oag.ca.gov/news/press-releases/attorney-general-bonta-announces-93-million-settlement-regarding-google%E2%80%99s",
    "source": "California Department of Justice",
    "datetime": "2023-09-14T07:00:00.000Z",
    "time": "Sep 14, 2023",
    "articleType": "regular",
    "content": "OAKLAND — California Attorney General Rob Bonta today announced a $93 million settlement with Google resolving allegations that its location-privacy practices violated California consumer protection laws. The settlement follows a multi-year investigation by the California Department of Justice that determined Google was deceiving users by collecting, storing, and using their location data for consumer profiling and advertising purposes without informed consent. In addition to paying $93 million, Google has agreed to accept strong injunctive terms to deter future misconduct.\n“Our investigation revealed that Google was telling its users one thing – that it would no longer track their location once they opted out – but doing the opposite and continuing to track its users’ movements for its own commercial gain. That’s unacceptable, and we’re holding Google accountable with today’s settlement,” said Attorney General Bonta. “I want to thank my Consumer Protection Section for their work on this matter and for securing important privacy safeguards on behalf of all Californians.”\nBased in Mountain View, California, Google generates the majority of its revenue from advertising, and location-based advertising (or geotargeted advertising) is a critical feature of Google’s advertising platform because advertisers want the ability to market to users based on their geographical locations. Google also uses their location data to build behavioral profiles of users to help determine which ads to serve users.\nUnder the settlement, Google must pay the state $93 million and be subject to a number of injunctive terms that will protect the privacy interests of California users, including requirements that Google:\nShow additional information to users when enabling location-related account settings.\nProvide more transparency about location tracking.\nProvide users with detailed information about the location data that Google collects and how it is used through a “Location Technologies” webpage.\nDisclose to users that their location information may be used for ads personalization.\nDisclose to users before using Location History data to build ad targeting profiles for users.\nObtain review by Google’s internal Privacy Working Group and document approval for all material changes to location-setting and ads personalization disclosures that will have a material impact on privacy.\nA copy of the complaint and proposed stipulated judgment, which details the aforementioned settlement terms and remains subject to court approval, can be found here and here.",
    "favicon": "https://oag.ca.gov/sites/default/files/favicon_0.ico"
  },
  {
    "title": "Google lays off hundreds in Assistant, hardware, engineering teams",
    "link": "https://www.reuters.com/technology/google-lays-off-hundreds-working-assistant-software-other-parts-company-2024-01-11/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW9PVUprV0VoRVMwZFlNbmxQVFJDM0FSaVRBaWdCTWdZTjhZcnBUQVE=-w400-h224-p-df-rw",
    "source": "Reuters",
    "datetime": "2024-01-10T08:00:00.000Z",
    "time": "Jan 10",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google lays off hundreds in hardware, voice assistant teams amid cost-cutting drive",
    "link": "https://apnews.com/article/google-layoffs-alphabet-amazon-tech-1de7e1a6b806a3252cfbb25fc4c5b2e3",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNDJXbk4wT0RaTk5XaDBlbWRxVFJDM0FSaVRBaWdCTWdrQmNJcktQS1d4N1FF=-w400-h224-p-df-rw",
    "source": "The Associated Press",
    "datetime": "2024-01-11T08:00:00.000Z",
    "time": "Jan 11",
    "articleType": "regular",
    "content": "Updated 11:20 AM GMT+2, January 11, 2024\nGoogle has laid off hundreds of employees working on its hardware, voice assistance and engineering teams as part of cost-cutting measures.The cuts come as Google looks towards “responsibly investing in our company’s biggest priorities and the significant opportunities ahead,” the company said in a statement.“Some teams are continuing to make these kinds of organizational changes, which include some role eliminations globally,” it said.Google earlier said it was eliminating a few hundred roles, with most of the impact on its augmented reality hardware team.\nThe cuts follow pledges by executives of Google and its parent company Alphabet to reduce costs. A year ago, Google said it would lay off 12,000 employees or around 6% of its workforce.In a post on X — previously known as Twitter — the Alphabet Workers Union described the job cuts as “another round of needless layoffs.”\n“Our members and teammates work hard every day to build great products for our users, and the company cannot continue to fire our coworkers while making billions every quarter,” the union wrote. “We won’t stop fighting until our jobs are safe!”Google is not the only technology company cutting back. In the past year, Meta -- the parent company of Facebook -- has slashed more than 20,000 jobs to reassure investors. Meta’s stock price gained about 178% in 2023.Spotify said in December that it was axing 17% of its global workforce, the music streaming service’s third round of layoffs in 2023 as it moved to slash costs and improve its profitability.\nEarlier this week, Amazon laid off hundreds of employees in its Prime Video and studios units. It also will lay off about 500 employees who work on its livestreaming platform Twitch.Amazon has cut thousands of jobs after a hiring surge during the pandemic. In March, Amazon announced that it planned to lay off 9,000 employees, on top of 18,000 employees it said that it would lay off in January 2023.\nGoogle is currently locked in a fierce rivalry with Microsoft as both firms strive to lead in the artificial intelligence domain.Microsoft has stepped up its artificial intelligence offerings to rival Google’s. In September, Microsoft introduced a Copilot feature that incorporates artificial intelligence into products like search engine Bing, browser Edge as well as Windows for its corporate customers.",
    "favicon": "/favicon-32x32.png"
  },
  {
    "title": "What Google’s trial means for the company — and your web browsing",
    "link": "https://www.vox.com/technology/2023/9/11/23864514/google-search-antitrust-trial",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXRUbUZyVEdkd1dtTkNiRWgxVFJDNEFSaVNBaWdCTWdZTllwYVBJUWs=-w400-h224-p-df-rw",
    "source": "Vox.com",
    "datetime": "2023-09-11T07:00:00.000Z",
    "time": "Sep 11, 2023",
    "articleType": "regular",
    "content": "The first big trial of the modern Big Tech antitrust movement is here: On September 12, the Justice Department’s lawsuit against Google’s search engine monopoly began. What’s at stake? Oh, nothing much — just the future of the internet. Or maybe the future of antitrust law in the US. Maybe both.This is the first antitrust trial that goes after a Big Tech company’s business practices since the DOJ took on Microsoft in the late ’90s, and it’s the first in a set of antitrust lawsuits against dominant tech platforms from federal and state antitrust enforcers that will play out in the next few months. Those include the DOJ and state attorneys general’s lawsuits against Google over its ad tech business, the FTC’s case against Meta over its acquisitions of Instagram and WhatsApp, and the FTC’s lawsuit against Amazon over its marketplace platform. Apple might even catch a lawsuit, too. The outcomes of these cases, starting with this one, will tell us if our antitrust laws, written decades before the internet existed and tried before an increasingly business-friendly justice system, can be applied to dominant digital platforms’ business practices now. “If the DOJ loses, it becomes a very serious question of what’s it going to take,” Harold Feld, senior vice president at Public Knowledge, an open internet advocacy group, said. “Other than an act of Congress, is there any way that a court is going to apply the antitrust laws to these new business models and new technologies?”That is to say, this case may change how much power those platforms have over us and how they’re allowed to wield it. And it all boils down to a simple question: Which search engine do you use, and why?The first part of this isn’t in dispute. If you’re like 90 percent of Americans, it’s Google, which has been synonymous with internet search for decades. The “why” is where the fight is. Google says it’s because it’s the best search engine out there. The DOJ and attorneys general from almost every state and territory in the country say it’s because Google pays a host of companies — everyone from Apple to Verizon — billions of dollars a year to make its search the default on the vast majority of devices and browsers. While Google has refused to give the exact amount, it was revealed during the trial that it paid $26.3 billion in 2021 alone, and made $146.4 billion in revenue for search ads in that period. The majority of that money is believed to go to Apple.Most of us probably take search engines for granted at this point, but they’re still a hugely important part of how the internet works. The proof is Google, which in just 25 years has grown into a $1.7 trillion company that owns major swaths of what we do online. It was all built on that search engine, which remains Google’s biggest revenue generator even now. Search ads were nearly 60 percent of the company’s revenue in 2022, to the tune of $162.45 billion. And that doesn’t count all the other ways Google can and does monetize its exclusive knowledge of what most of the world wants to know all the time.Ironically enough, it was another tech company’s antitrust woes that helped Google emerge in the first place: Microsoft.Remember Internet Explorer? The DOJ sure does.A few decades ago, your internet experience almost certainly began with Microsoft’s Internet Explorer, as was the case for up to 95 percent of internet users when the browser was at its early 2000s peak. But that market share didn’t happen because Internet Explorer was better, the DOJ contended in its 1998 antitrust lawsuit against the company. It was because Microsoft leveraged its dominance over computer operating systems to force its browser onto users. Internet Explorer was bundled with Microsoft’s Windows operating system, and Microsoft ensured it was just about impossible to remove. Installing an alternate browser was technically possible but difficult, so most people didn’t bother. This killed off most of Internet Explorer’s competitors and gave Microsoft a monopoly over internet browsers that was similar to the one it enjoyed over computer operating systems. And that, the DOJ said, was an abuse of Microsoft’s monopoly power.The US District Court for the District of Columbia agreed and ordered Microsoft to be broken up into two companies. But a higher court overturned part of that ruling, and the DOJ subsequently settled with Microsoft. The company got to stay in one piece, but it paid a price. While Microsoft was tied up in court, paying billions in fines, afraid to make any major moves that could incur more government wrath and no longer allowed to gatekeep the internet through its browser, new companies like Google emerged.Now, the DOJ says, the cycle is repeating. But Google is the one that is using its dominance to freeze out competitors, and consumers are being denied the kind of innovation that put Google on the map in the first place. “If the government’s allegations are to be believed, Google is doing exactly what Microsoft did in many respects,” said Gary Reback, an antitrust lawyer who was instrumental in convincing the DOJ to bring the case against Microsoft back then and tried to get the FTC to take on Google 10 years ago. “The major arguments — I’ve seen them all before — they were made by Microsoft, and they failed.”The DOJ’s lawsuit was filed in October 2020, at the very end of Trump’s presidency and when anti-Big Tech sentiment was high and bipartisan. It came just a few weeks after the House’s long investigation into Amazon, Apple, Google, and Meta’s business practices, which led to a set of bipartisan, bicameral antitrust bills meant to address the unique ways digital platforms operate and maintain their dominance. Eleven states joined that suit; three more signed on a few months later. In December 2020, 35 states, the territories of Puerto Rico and Guam, and Washington, DC, filed their own lawsuit against Google over its search practices. Those two cases have been combined for this trial.Microsoft has a place in this lawsuit, too, by the way: This time, it’s as a witness for the government. CEO Satya Nadella testified on October 2 that Google’s dominance has made it impossible for his company’s search engine, Bing, to truly compete — even as Microsoft has invested about $100 billion into its search engine to try. He said his company has tried to negotiate with Apple for years to break up its “oligopolistic” relationship with Google, offering the iPhone maker tens of billions of dollars to switch the search default from Google to Bing.“Defaults are the only thing that matter,” Nadella said.Apple, obviously, didn’t bite. Google’s argument is that Bing just isn’t as good as Google is. Even Windows users who have Microsoft’s Edge browser with its Bing default pre-installed prefer Google to Bing (though Bing’s market share is bigger on Windows PCs than it is elsewhere), and, as Nadella admitted, the most queried word on Bing is “Google.” Apple, Google says, is choosing the search engine it thinks is best for its customers — not the one that happens to pay it the most.This isn’t to be confused with all the other antitrust lawsuits the government has filed against Google that address other parts of its business. One of those, about Google’s app store, was recently settled. Two others about Google’s ad tech business are winding their way through the courts. Here, we’re just looking at Google’s search arm, which is the foundation of the company but far from the only thing it does.There are also a few things you won’t see in this case that used to be there. A few weeks ago, Judge Mehta threw out several of the plaintiffs’ claims. The states’ argument that Google harmed competitors like Yelp and Expedia by designing its search results to prominently feature its own services over theirs was tossed. The DOJ’s claims that Google’s agreements with manufacturers to give its services default placement on Androids and Internet of Things devices were exclusionary were also dismissed.So we’re left with two claims. One is from the states’ case about Google’s search engine marketing tool, and it accuses the company of making certain features available to its search engine and not Microsoft’s Bing in order to give it an unfair advantage. But the core of this case is the second claim about Google’s default search agreements. How Google’s default search agreements hurt you — or helpWith so much of its revenue riding on the popularity and scale of its search product, Google is willing to spend a lot of money to ensure that it’s the default search in as many places as possible. The company shells out billions of dollars every year to browser developers, device manufacturers, and phone carriers for Google to be the default search engine almost everywhere. The exact amounts of those default search agreements have been redacted for this trial, but estimates put it at as much as $20 billion a year to Apple alone. This paid placement, the DOJ says, has helped Google maintain its dominance and made it impossible for just about anyone else to compete. Very few companies have billions of dollars to throw around. Or, as the DOJ said, it’s “creating a continuous and self-reinforcing cycle of monopolization.”And while it’s possible for users to switch to a different search engine, very few of them actually do. The DOJ is expected to say that’s because Google has locked up the best distribution channels. Using a competitor requires knowing that it’s even possible to do it in the first place as well as how to make the switch. There are also countless studies that will tell you how difficult it is to overcome consumer inertia. The vast majority of people just go with whatever’s there, which is why Google is paying to be there. Microsoft’s defense that people could install alternate browsers if they so chose didn’t work 25 years ago. The DOJ doesn’t think it should work now.All this has hurt competitors, who can’t get a foothold in the market, according to the DOJ. It has impacted advertisers, who have to pay what Google is charging for those search ads because there’s no other game in town, and consumers, who don’t have much choice in search engines.The lack of choice is also, the suit says, stifling innovation. There’s no pressure on Google to improve its product because there aren’t any companies trying to develop their own, possibly better, ones. The DOJ will likely argue that the quality of Google’s product has gone down as its dominance became more entrenched. One example could be all of those knowledge panels Google sticks on top of search results that direct users to other Google products, not to mention the presence of more and more search ads. The states’ case that this harmed third parties like Yelp was thrown out, but the DOJ could still say that it harms consumers who have to do more work to get to the search results they came to Google for in the first place.There are other search engines, but they’ve struggled to gain market share. The aforementioned Bing currently has just 6.4 percent of the US market (Yahoo!, which uses Bing, is another 2.4 percent). There’s also DuckDuckGo, which has been trying to compete with Google as a privacy-preserving alternative. But it only has a fraction of the market, and it blames Google’s default search agreements for that.“Even though DuckDuckGo provides something extremely valuable that people want and Google won’t provide — real privacy — Google makes it unduly difficult to use DuckDuckGo by default. We’re glad this issue is finally going to have its day in court,” Kamyl Bazbaz, spokesperson for DuckDuckGo, said in a statement. DuckDuckGo, obviously, is an existing product. This case is also very much about the search engines that don’t exist and never will, the ones that you, the consumer, will never get to use. The DOJ will likely argue that’s because Google intentionally made the search engine barrier to entry too high. The co-founder of now-defunct search engine Neeva recently testified that his company, which had a subscription model rather than ad-based, couldn’t get the traction it needed in the face of Google’s monopoly.For its part, Google maintains that it’s the most popular search engine because it’s the best one out there, giving its users the most meaningful and relevant results. The company says that the DOJ’s case is aimed at helping competitors — not consumers.Google says the companies that choose its search to be the default on their products do so because it’s better, not because Google is paying them. And consumers use Google because it’s better, not because it happens to be there when they turn their new phones on or fire up their new computer’s browser for the first time.“People don’t use Google because they have to — they use it because they want to,” Kent Walker, Google’s president of global affairs, said in a blog post. “Making it easier for people to get the products they want benefits consumers and is supported by American antitrust law.”But why, you might ask, is Google paying anyone at all if it’s so great? Well, the company has long maintained that this is equivalent to a brand paying a grocery store for prime shelf space, something that is perfectly legal and happens all the time. (People who disagree with this will point out that occupying the only search engine slot on the vast majority of web browsers and devices is not quite the same thing as sitting on a shelf in a grocery store.) Google thinks it’s improving customer access to what it believes is the best product. And that, Google says, is good for consumers.Google CEO Sundar Pichai took the stand on October 30 to say as much. He acknowledged that the default agreements are valuable to Google, but framed them as a promotional tool for the company.But the DOJ referenced a Google executive’s notes from a 2018 meeting between Pichai and Apple CEO Tim Cook, which described them as wanting to “work as if we are one company.” Pichai said he didn’t remember saying that and doesn’t agree with it either, stressing that Apple is a competitor, not a partner. The government has also maintained that part of the reason why Google paid off Apple was to prevent the company from developing its own search engine. Pichai admitted that Google has, at times, had concerns that Apple could become a search competitor, but maintained that wasn’t the reason why it made those deals with the company.Google also says it’s easy to switch to a different search engine — much easier, in fact, than it was to install a new browser back in the Microsoft lawsuit days. Apps can be downloaded in seconds, and it takes just a few clicks to change your search engine settings, as long as you know it’s possible and how to do it.“While default settings matter (that’s why we bid for them), they’re easy to change. People can and do switch,” Walker said.Google also says it’s continuously improving and innovating. Any perceived lack of competition (and the company says it has plenty of competition) hasn’t caused it to rest on its laurels.“We invest billions of dollars in R&D and make thousands of quality improvements to Search every year to ensure we’re delivering the most helpful results,” Walker said.Finally, Google has maintained that the market is more than just general search engines like Bing or DuckDuckGo, because general search engines aren’t the only way people look for things on the internet. They may also go directly to Reddit or Amazon, for example. So it has more competitors than the DOJ claims as well as a smaller market share. That’s probably not going to fly with the judge, but Google will give it a try anyway.The future of the internet, as determined by a business-friendly justice systemAs Reback says, we saw many of these concepts litigated with the Microsoft case nearly three decades ago. So we should have case law that says some of the same or very similar practices Google is engaged in are illegal, right? Not necessarily. Google has a few things going for it here. For one, it’s been more careful about how it phrases and frames things in internal documents than Microsoft was (assuming those internal documents exist — the DOJ has accused Google of withholding or destroying some of them). For another, the courts that will ultimately decide how to apply the law are different, too. “Since Microsoft, there’s been a couple of Supreme Court decisions that are, by their attitude and their approach, tolerant of dominant firm behavior,” William Kovacic, who served as the chair of the FTC under George W. Bush, said. “Their attitude toward plaintiffs is not nearly so generous as the Court of Appeals was in the Microsoft case.”No matter what the judge decides, it will be a while before we know the final outcome. The trial is expected to last about nine weeks, and Judge Mehta’s ruling won’t come out until next year. We’re sure to have a long appeals process after that. But whatever the outcome is, it may be hugely consequential, especially when viewed in combination with the other digital platform antitrust cases we have now (or likely will have soon) and the larger antitrust reform movement.If Google loses, it faces the possibility of being broken up into smaller companies (an extreme, but not unheard of, measure that the DOJ is asking for) or forbidden from offering those search agreements. We could be looking at a much different Google, or we’ll get to see which search engine users pick when Google is not the default.If the DOJ loses, there are a few ways to look at it. One is that this is proof that Google isn’t doing anything wrong and should be allowed to continue to operate as it always has, without being unfairly targeted by the government with its anti-Big Tech agenda.But if you believe that Google and its Big Tech brethren’s dominance and power is a problem that needs to be solved, a DOJ loss would show that our antitrust laws and the courts that are charged with interpreting them aren’t equipped to deal with the realities of this digital economy and how its major players operate within it.“If the government gets the door slammed on its face ... if they try and they lose, then they can turn to Congress and say, ‘Well, our antitrust system is so cramped and limited that we can’t do the job. You’ve got to fix it,’” Kovacic said.That could be what motivates Congress to pass antitrust laws that do account for dominant digital platforms. An internet that’s essentially controlled by a handful of companies may well open back up again — assuming it isn’t already too late.Update, October 30, 5 pm ET: This story was originally published on September 9 and has been updated to include testimony from Microsoft CEO Satya Nadella, Neeva’s co-founder, and Google CEO Sundar Pichai. Google’s default payments in 2021 have also been added.",
    "favicon": "/static-assets/icons/favicon.ico"
  },
  {
    "title": "Explore new augmented reality features in Google Maps",
    "link": "https://blog.google/products/maps/immersive-ar-content-google-maps-paris/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNHRkRWhsTUZaWFNsVjNXV3BxVFJDb0FSaXNBaWdCTWdrTlZJeW1JdWhqYndF=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-07-30T16:33:14.000Z",
    "time": "2 days ago",
    "articleType": "regular",
    "content": "These immersive experiences — available right from Google Maps — let you experience Parisian landmarks in new ways.\nLens in Maps AR experience of Eiffel Tower\nAugmented reality (AR) in Google Maps offers new ways to explore places and find things to do. Last week, we shared how this technology is helping visitors and locals navigate Paris this summer. Here's a closer look at the new immersive experiences that let you not only explore the city’s iconic landmarks, but also interact and engage with them.\nNew ways to explore with AR in MapsWhen you search for a location in Google Maps that offers AR content, you can explore it right from the app. Simply touch the \"AR Experience\" icon and raise your device to view it from Lens in Maps. If you’re not at the landmark, you can still view the experience from anywhere in the world using Street View.\nAR experience of Eiffel Tower via Street View\nSimulation of AR experience in Lens in Maps of Exposition Universelle country pavilions\nLater this summer, you’ll also be able to go back even further in time to experience historically accurate AR reimaginings of 18th-century Paris. Visit Notre-Dame Cathedral and its surroundings as it existed in 1789, explore a 3D recreation of the Bastille Saint-Antoine prior to the French Revolution, or tour the storied Palais des Tuileries before its demolition hundreds of years ago.\nLeft Image: Lens in Maps AR experience of La Bastille. Right Image: Lens in Maps AR experience of Notre Dame\nThese experiences come to life thanks to our work with Google Arts & Culture and Ubisoft. Check it out on Google Maps and be on the lookout for more content in the future.\nMapsStay informed on the go with new updates from Maps and Waze\nSearch8 ways to keep up with the Olympic Games Paris 2024 on Google\nCompany announcements4 ways Google will show up in NBCUniversal’s Olympic Games Paris 2024 coverage\nMapsUse these 5 AI-powered tools to plan your summer travel\nArts & CultureArtist Felipe Pantone turns the world’s streets into his AR canvas\nSustainabilityMapping human activity at sea with AI",
    "favicon": "/favicon.ico"
  },
  {
    "title": "A first-of-its-kind geothermal project is now operational",
    "link": "https://blog.google/outreach-initiatives/sustainability/google-fervo-geothermal-energy-partnership/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTJORnBRUmtwcGMySktXVVp4VFJEQ0FSaURBaWdCTWdZaEpJV3RWUU0=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-28T08:00:00.000Z",
    "time": "Nov 28, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Gemini image generation got it wrong. We'll do better.",
    "link": "https://blog.google/products/gemini/gemini-image-generation-issue/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDNMV3BRVFhneFZXTmZlRkJqVFJDb0FSaXNBaWdCTWdZcGhaQ3RNUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-23T08:00:00.000Z",
    "time": "Feb 23",
    "articleType": "regular",
    "content": "We recently made the decision to pause Gemini’s image generation of people while we work on improving the accuracy of its responses. Here is more about how this happened and what we’re doing to fix it.\nThree weeks ago, we launched a new image generation feature for the Gemini conversational app (formerly known as Bard), which included the ability to create images of people.It’s clear that this feature missed the mark. Some of the images generated are inaccurate or even offensive. We’re grateful for users’ feedback and are sorry the feature didn't work well.We’ve acknowledged the mistake and temporarily paused image generation of people in Gemini while we work on an improved version.What happenedThe Gemini conversational app is a specific product that is separate from Search, our underlying AI models, and our other products. Its image generation feature was built on top of an AI model called Imagen 2.When we built this feature in Gemini, we tuned it to ensure it doesn’t fall into some of the traps we’ve seen in the past with image generation technology — such as creating violent or sexually explicit images, or depictions of real people. And because our users come from all over the world, we want it to work well for everyone. If you ask for a picture of football players, or someone walking a dog, you may want to receive a range of people. You probably don’t just want to only receive images of people of just one type of ethnicity (or any other characteristic).However, if you prompt Gemini for images of a specific type of person — such as “a Black teacher in a classroom,” or “a white veterinarian with a dog” — or people in particular cultural or historical contexts, you should absolutely get a response that accurately reflects what you ask for.So what went wrong? In short, two things. First, our tuning to ensure that Gemini showed a range of people failed to account for cases that should clearly not show a range. And second, over time, the model became way more cautious than we intended and refused to answer certain prompts entirely — wrongly interpreting some very anodyne prompts as sensitive.These two things led the model to overcompensate in some cases, and be over-conservative in others, leading to images that were embarrassing and wrong.Next steps and lessons learnedThis wasn’t what we intended. We did not want Gemini to refuse to create images of any particular group. And we did not want it to create inaccurate historical — or any other — images. So we turned the image generation of people off and will work to improve it significantly before turning it back on. This process will include extensive testing.One thing to bear in mind: Gemini is built as a creativity and productivity tool, and it may not always be reliable, especially when it comes to generating images or text about current events, evolving news or hot-button topics. It will make mistakes. As we’ve said from the beginning, hallucinations are a known challenge with all LLMs — there are instances where the AI just gets things wrong. This is something that we’re constantly working on improving.Gemini tries to give factual responses to prompts — and our double-check feature helps evaluate whether there’s content across the web to substantiate Gemini’s responses — but we recommend relying on Google Search, where separate systems surface fresh, high-quality information on these kinds of topics from sources across the web.I can’t promise that Gemini won’t occasionally generate embarrassing, inaccurate or offensive results — but I can promise that we will continue to take action whenever we identify an issue. AI is an emerging technology which is helpful in so many ways, with huge potential, and we’re doing our best to roll it out safely and responsibly.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google agrees to invest up to $2 billion in OpenAI rival Anthropic",
    "link": "https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNUdXSGMxWDBOME9VcEJhMms1VFJDcEFSaXFBaWdCTWdtQmNJNkZ0Q2FaYkFJ=-w400-h224-p-df-rw",
    "source": "Reuters",
    "datetime": "2023-10-27T07:00:00.000Z",
    "time": "Oct 27, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "4 Lessons Businesses Can Learn From Google's Evolution",
    "link": "https://www.forbes.com/sites/johnhall/2024/02/18/4-lessons-businesses-can-learn-from-googles-evolution/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNTBjbkpxVmxJemVEZDFMVzQ0VFJEQ0FSaURBaWdCTWdrQk1JTFVsS2xTMVFB=-w400-h224-p-df-rw",
    "source": "Forbes",
    "datetime": "2024-02-18T08:00:00.000Z",
    "time": "Feb 18",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "“Google Vids” is Google’s fourth big productivity app for Workspace",
    "link": "https://arstechnica.com/gadgets/2024/04/google-vids-is-googles-fourth-big-productivity-app-for-workspace/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXNXbGhvTVRKZlJtUnVjSE5mVFJDb0FSaXNBaWdCTWdhaFk0UnZyUWM=-w400-h224-p-df-rw",
    "source": "Ars Technica",
    "datetime": "2024-04-09T07:00:00.000Z",
    "time": "Apr 9",
    "articleType": "regular",
    "content": "Please don't bore your co-workers    —\nGoogle's \"video editor\" feels more like a souped-up version of Google Slides.\nIs that Google Slides? Nope it's Google Vids, the new video editor that seems to just make souped-up slideshows.\nGoogle's demo starts with an existing slideshow and then generates an outline.\nChoose a theme, which all look like PowerPoints.\nWrite a script, preferably with the help of Google Gemini.\nYou can record a voiceover, or pick from Google's robot voices.\nThis is a Google Workspace app, so there's lots of realtime collaboration features, like these live mouse cursors that were brought over from Slides.\nIt's interesting you get a \"stock media\" library while apps like Slides would use generative AI images here.\nRecord a talk from your webcam.\nEmbed your video in the slideshow.\nIf you had asked me before what Google's video editor app was, I would say \"YouTube Studio,\" but now Google Workspace has a new productivity app called \"Google Vids.\" Normally a video editor is considered a secondary application in many productivity suites, but Google apparently imagines Vids as a major pillar of Workspace, saying Vids is an \"all-in-one video creation app for work that will sit alongside Docs, Sheets and Slides.\" So, that is an editor for documents, spreadsheets, presentations, and videos?\nGoogle's demo of the new video editor pitches the product not for YouTube videos or films but more as a corporate super slideshow for things like training materials or product demos. Really, this \"video editor\" almost looks like it could completely replace Google Slides since the interface is just Slides but with a video timeline instead of a slideshow timeline.\nGoogle's example video creates a \"sales training video\" that starts with a Slides presentation as the basic outline. You start with an outline editor, where each slideshow page gets its own major section. Google then has video \"styles\" you can pick from, which all seem very Powerpoint-y with a big title, subheading, and a slot for some kind of video. Google then wants you to write a script and either read it yourself or have a text-to-speech voice read the script. A \"stock media\" library lets you fill in some of those video slots with generic corporate imagery like a video of a sunset, choose background music, and use a few pictures. You can also fire up your webcam and record something, sort of like a pre-canned Zoom meeting. After that it's a lot of the usual Google productivity app features: real-time editing collaboration with visible mouse cursors from each participant and a stream of comments.\nLike all Google products after the rise of OpenAI, Google pitches Vids as an \"AI-powered\" video editor, even though there didn't seem to be many generative AI features in the presentation. The videos, images, and music were \"stock\" media, not AI-generated inventions (Slides can generate images, but that wasn't in this demo). There's nothing in here like OpenAI's \"Sora,\" which generates new videos out of its training data. There's probably a Gemini-powered \"help me write\" feature for the script, and Google describes the initial outline as \"generated\" from your starting Slides presentation, but that seemed to be it.\nGoogle says Vids is being released to \"Workspace Labs\" in June, so you'll be able to opt in to testing it.",
    "favicon": "https://cdn.arstechnica.net/favicon.ico"
  },
  {
    "title": "5 ways to use the new Find My Device on Android",
    "link": "https://blog.google/products/android/android-find-my-device/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNDBVRTF2TFZwa00wNU9SbkZ5VFJDUkFSamNBaWdCTWdZUkVJcW9GQXM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-04-08T07:00:00.000Z",
    "time": "Apr 8",
    "articleType": "regular",
    "content": "The new Find My Device experience helps you easily find your misplaced Android devices and other belongings.\nToday, the all-new Find My Device is rolling out to Android devices around the world, starting in the U.S. and Canada. With a new, crowdsourced network of over a billion Android devices, Find My Device can help you find your misplaced Android devices and everyday items quickly and securely. Here are five ways you can try it out.\n1. Locate offline devicesLocate your compatible Android phone and tablet by ringing them or viewing their location on a map in the app — even when they’re offline. And thanks to specialized Pixel hardware, Pixel 8 and 8 Pro owners will also be able to find their devices if they’re powered off or the battery is dead.\n2. Keep track of everyday items with compatible Bluetooth tagsStarting in May, you’ll be able to locate everyday items like your keys, wallet or luggage with Bluetooth tracker tags from Chipolo and Pebblebee in the Find My Device app. These tags, built specifically for the Find My Device network, will be compatible with unknown tracker alerts across Android and iOS to help protect you from unwanted tracking. Keep an eye out later this year for additional Bluetooth tags from eufy, Jio, Motorola and more.\n3. Find nearby itemsSometimes what we’re looking for is right under our noses. If you're close to your lost device but need a little extra help tracking it down, a “Find nearby” button will appear to help you figure out exactly where it’s hiding. You’ll also be able to use this to find everyday items, like your wallet or keys, when Bluetooth tags launch in May.\n4. Pinpoint devices at home with NestMore often than not, we lose everyday items like our keys or phone right at home. So the Find My Device app now shows a lost device’s proximity to your home Nest devices, giving you an easy reference point.\n5. Share accessories with friends and familyShare an accessory so everyone can keep an eye on it in the app. For instance, share your house key with your roommate, the TV remote with your friend or luggage with a travel buddy so you can easily divide and conquer if something goes missing.\nAndroid10 fun facts about emoji for World Emoji Day\nAndroid10 years ago, Android expanded to 3 new platforms\nAndroid4 Google updates coming to Samsung devices\nAndroid EnterpriseHow we’re making Android Enterprise signup and access to Google services better\nGoogle AdsMeet the Google TV network\nMessagesTexting 911 becomes even more useful with RCS",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google unleashes AI in search, raising hopes for better results and fears about less web traffic",
    "link": "https://apnews.com/article/google-search-ai-overviews-internet-traffic-ebb6bbbde17ed29a5f7b630d9e5e285b",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVZZV2x5U25GeWVVYzNPVXQzVFJDM0FSaVRBaWdCTWdZZGRZYXVMUWM=-w400-h224-p-df-rw",
    "source": "The Associated Press",
    "datetime": "2024-05-15T07:00:00.000Z",
    "time": "May 15",
    "articleType": "regular",
    "content": "MOUNTAIN VIEW, Calif. (AP) — Google on Tuesday rolled out a retooled search engine that will frequently favor responses crafted by artificial intelligence over website links, a shift promising to quicken the quest for information while also potentially disrupting the flow of money-making internet traffic.The makeover announced at Google’s annual developers conference will begin this week in the U.S. when hundreds of millions of people will start to periodically see conversational summaries generated by the company’s AI technology at the top of the search engine’s results page.The AI overviews are supposed to only crop up when Google’s technology determines they will be the quickest and most effective way to satisfy a user’s curiosity — a solution mostly likely to happen with complex subjects or when people are brainstorming, or planning. People will likely still see Google’s traditional website links and ads for simple searches for things like a store recommendation or weather forecasts.\nGoogle began testing AI overviews with a small subset of selected users a year ago, but the company is now making it one of the staples in its search results in the U.S. before introducing the feature in other parts of the world. By the end of the year, Google expects the recurring AI overviews to be part of its search results for about 1 billion people.\nBesides infusing more AI into its dominant search engine, Google also used the packed conference held at a Mountain View, California, amphitheater near its headquarters to showcase advances in a technology that is reshaping business and society.\nThe next AI steps included more sophisticated analysis powered by Gemini — a technology unveiled five months ago — and smarter assistants, or “agents,” including a still-nascent version dubbed “Astra” that will be able to understand, explain and remember things it is shown through a smartphone’s camera lens. Google underscored its commitment to AI by bringing in Demis Hassabis, the executive who oversees the technology, to appear on stage at its marquee conference for the first time.\nThe injection of more AI into Google’s search engine marks one of the most dramatic changes that the company has made in its foundation since its inception in the late 1990s. It’s a move that opens the door for more growth and innovation but also threatens to trigger a sea change in web surfing habits.“This bold and responsible approach is fundamental to delivering on our mission and making AI more helpful for everyone,” Google CEO Sundar Pichai told a group of reporters. Well aware of how much attention is centered on the technology, Pichai ended a nearly two-hour succession of presentations by asking Google’s Gemini model how many times AI had been mentioned. The count: 120, and then the tally edged up by one more when Pichai said, “AI,” yet again. The increased AI emphasis will bring new risks to an internet ecosystem that depends heavily on digital advertising as its financial lifeblood.Google stands to suffer if the AI overviews undercuts ads tied to its search engine — a business that reeled in $175 billion in revenue last year alone. And website publishers — ranging from major media outlets to entrepreneurs and startups that focus on more narrow subjects — will be hurt if the AI overviews are so informative that they result in fewer clicks on the website links that will still appear lower on the results page.\nBased on habits that emerged during the past year’s testing phase of Google’s AI overviews, about 25% of the traffic could be negatively affected by the de-emphasis on website links, said Marc McCollum, chief innovation officer at Raptive, which helps about 5,000 website publishers make money from their content. A decline in traffic of that magnitude could translate into billions of dollars in lost ad revenue, a devastating blow that would be delivered by a form of AI technology that culls information plucked from many of the websites that stand to lose revenue.“The relationship between Google and publishers has been pretty symbiotic, but enter AI, and what has essentially happened is the Big Tech companies have taken this creative content and used it to train their AI models,” McCollum said. “We are now seeing that being used for their own commercial purposes in what is effectively a transfer of wealth from small, independent businesses to Big Tech.”\nBut Google found the AI overviews resulted in people in conducting even more searches during the technology’s testing “because they suddenly can ask questions that were too hard before,” said Liz Reid, who oversees the company’s search operations, told The Associated Press during an interview. She declined to provide any specific numbers about link-clicking volume during the tests of AI overviews. “In reality, people do want to click to the web, even when they have an AI overview,” Reid said. “They start with the AI overview and then they want to dig in deeper. We will continue to innovate on the AI overview and also on how do we send the most useful traffic to the web.”\nThe increasing use of AI technology to summarize information in chatbots such as Google’s Gemini and OpenAI’s ChatGPT during the past 18 months already has been raising legal questions about whether the companies behind the services are illegally pulling from copyrighted material to advance their services. It’s an allegation at the heart of a high-profile lawsuit that The New York Times filed late last year against OpenAI and its biggest backer, Microsoft.Google’s AI overviews could provoke lawsuits too, especially if they siphon away traffic and ad sales from websites that believe the company is unfairly profiting from their content. But it’s a risk that the company had to take as the technology advances and is used in rival services such as ChatGPT and upstart search engines such as Perplexity, said Jim Yu, executive chairman of BrightEdge, which helps websites rank higher in Google’s search results.“This is definitely the next chapter in search,” Yu said. “It’s almost like they are tuning three major variables at once: the search quality, the flow of traffic in the ecosystem and then the monetization of that traffic. There hasn’t been a moment in search that is bigger than this for a long time.”Outside of the amphitheater, several dozen protesters chained themselves to each other and blocked one of the entrances to the conference. Demonstrators targeted a $1.2 billion deal known as Project Nimbus that provides artificial intelligence technology to the Israeli government. They contend the system is being lethally deployed in the Gaza war — an allegation Google refutes. The demonstration didn’t seem to affect the conferences attendance or the enthusiasm of the crowd inside the venue.",
    "favicon": "/favicon-32x32.png"
  },
  {
    "title": "Google I/O Highlights: Unpacking Gemini Updates and AI Overviews",
    "link": "https://www.cnet.com/tech/services-and-software/google-io-highlights-unpacking-gemini-updates-and-ai-overviews/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWtka0ptZG5obGQwZFFPRzVCVFJDb0FSaXNBaWdCTWdZVkE0U01HUVE=-w400-h224-p-df-rw",
    "source": "CNET",
    "datetime": "2024-05-15T07:00:00.000Z",
    "time": "May 15",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Opinion | Female popes? Google’s amusing AI bias underscores a serious problem.",
    "link": "https://www.washingtonpost.com/opinions/2024/02/27/google-gemini-bias-race-politics/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUpXRU16UkVkdlMzUlhlRWxIVFJDM0FSaVRBaWdCTWdhZE1Jak51UVE=-w400-h224-p-df-rw",
    "source": "The Washington Post",
    "datetime": "2024-02-27T08:00:00.000Z",
    "time": "Feb 27",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Gemma 2 is now available to researchers and developers",
    "link": "https://blog.google/technology/developers/google-gemma-2/",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-06-27T07:00:00.000Z",
    "time": "Jun 27",
    "articleType": "regular",
    "content": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.\nSorry, your browser doesn't support embedded videos, but don't worry, you can\nand watch it with your favorite video player!\nAI has the potential to address some of humanity's most pressing problems — but only if everyone has the tools to build with it. That's why earlier this year we introduced Gemma, a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. We’ve continued to grow the Gemma family with CodeGemma, RecurrentGemma and PaliGemma — each offering unique capabilities for different AI tasks and easily accessible through integrations with partners like Hugging Face, NVIDIA and Ollama.Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.A new open model standard for efficiency and performanceWe built Gemma 2 on a redesigned architecture, engineered for both exceptional performance and inference efficiency. Here’s what makes it stand out:Outsized performance: At 27B, Gemma 2 delivers the best performance for its size class, and even offers competitive alternatives to models more than twice its size. The 9B Gemma 2 model also delivers class-leading performance, outperforming Llama 3 8B and other open models in its size category. For detailed performance breakdowns, check out the technical report.Unmatched efficiency and cost savings: The 27B Gemma 2 model is designed to run inference efficiently at full precision on a single Google Cloud TPU host, NVIDIA A100 80GB Tensor Core GPU, or NVIDIA H100 Tensor Core GPU, significantly reducing costs while maintaining high performance. This allows for more accessible and budget-friendly AI deployments.Blazing fast inference across hardware: Gemma 2 is optimized to run at incredible speed across a range of hardware, from powerful gaming laptops and high-end desktops, to cloud-based setups. Try Gemma 2 at full precision in Google AI Studio, unlock local performance with the quantized version with Gemma.cpp on your CPU, or try it on your home computer with an NVIDIA RTX or GeForce RTX via Hugging Face Transformers.\nBuilt for developers and researchersGemma 2 is not only more powerful, it's designed to more easily integrate into your workflows:Open and accessible: Just like the original Gemma models, Gemma 2 is available under our commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.Broad framework compatibility: Easily use Gemma 2 with your preferred tools and workflows thanks to its compatibility with major AI frameworks like Hugging Face Transformers, and JAX, PyTorch and TensorFlow via native Keras 3.0, vLLM, Gemma.cpp, Llama.cpp and Ollama. In addition, Gemma is optimized with NVIDIA TensorRT-LLM to run on NVIDIA-accelerated infrastructure or as an NVIDIA NIM inference microservice, with optimization for NVIDIA’s NeMo to come. You can fine-tune today with Keras and Hugging Face. We are actively working to enable additional parameter-efficient fine-tuning options.Effortless deployment: Starting next month, Google Cloud customers will be able to easily deploy and manage Gemma 2 on Vertex AI.Explore the new Gemma Cookbook, a collection of practical examples and recipes to guide you through building your own applications and fine-tuning Gemma 2 models for specific tasks. Discover how to easily use Gemma with your tooling of choice, including for common tasks like retrieval-augmented generation.Responsible AI developmentWe're committed to providing developers and researchers with the resources they need to build and deploy AI responsibly, including through our Responsible Generative AI Toolkit. The recently open-sourced LLM Comparator helps developers and researchers with in-depth evaluation of language models. Starting today, you can use the companion Python library to run comparative evaluations with your model and data, and visualize the results in the app. Additionally, we’re actively working on open sourcing our text watermarking technology, SynthID, for Gemma models.When training Gemma 2, we followed our robust internal safety processes, filtering pre-training data and performing rigorous testing and evaluation against a comprehensive set of metrics to identify and mitigate potential biases and risks. We publish our results on a large set of public benchmarks related to safety and representational harms.\nProjects built with GemmaOur first Gemma launch led to more than 10 million downloads and countless inspiring projects. Navarasa, for instance, used Gemma to create a model rooted in India’s linguistic diversity.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Gemini and Google’s Culture",
    "link": "https://stratechery.com/2024/gemini-and-googles-culture/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNU5UVVkyYWtKclUyODJTSGhmVFJDbUFSaXdBaWdCTWdrQkVaZ3JuYW5RNWdJ=-w400-h224-p-df-rw",
    "source": "Stratechery by Ben Thompson",
    "datetime": "2024-02-26T08:00:00.000Z",
    "time": "Feb 26",
    "articleType": "regular",
    "content": "This Article is available as a video essay on YouTube\nLast Wednesday, when the questions about Gemini’s political viewpoint were still limited to its image creation capabilities, I accused the company of being timid:\nStepping back, I don’t, as a rule, want to wade into politics, and definitely not into culture war issues. At some point, though, you just have to state plainly that this is ridiculous. Google specifically, and tech companies broadly, have long been sensitive to accusations of bias; that has extended to image generation, and I can understand the sentiment in terms of depicting theoretical scenarios. At the same time, many of these images are about actual history; I’m reminded of George Orwell in 1984:\nEvery record has been destroyed or falsified, every book has been rewritten, every picture has been repainted, every statue and street and building has been renamed, every date has been altered. And that process is continuing day by day and minute by minute. History has stopped. Nothing exists except an endless present in which the Party is always right. I know, of course, that the past is falsified, but it would never be possible for me to prove it, even when I did the falsification myself. After the thing is done, no evidence ever remains. The only evidence is inside my own mind, and I don’t know with any certainty that any other human being shares my memories.\nEven if you don’t want to go so far as to invoke the political implications of Orwell’s book, the most generous interpretation of Google’s over-aggressive RLHF of their models is that they are scared of being criticized. That, though, is just as bad: Google is blatantly sacrificing its mission to “organize the world’s information and make it universally accessible and useful” by creating entirely new realities because it’s scared of some bad press. Moreover, there are implications for business: Google has the models and the infrastructure, but winning in AI given their business model challenges will require boldness; this shameful willingness to change the world’s information in an attempt to avoid criticism reeks — in the best case scenario! — of abject timidity.\nIf timidity were the motivation, then it’s safe to say that the company’s approach with Gemini has completely backfired; while Google turned off Gemini’s image generation capabilities, it’s text generation is just as absurd:\nThat is just one examples of many: Gemini won’t help promote meat, write a brief about fossil fuels, or even help sell a goldfish. It says that effective accelerationism is a violent ideology, that libertarians are morally equivalent to Stalin, and insists that it’s hard to say what caused more harm: repealing net neutrality or Hitler.\nSome of these examples, particularly the Hitler comparisons (or Mao vs George Washington), are obviously absurd and downright offensive; others are merely controversial. They do, though, all seem to have a consistent viewpoint: Nate Silver, in another tweet, labeled it “the politics of the median member of the San Francisco Board of Supervisors.”\nNeedless to say, overtly expressing those opinions is not timid, which raises another question from Silver:\nGemini is behaving exactly as instructed. Asking it to draw different groups of people (e.g. \"Vikings\" or \"NHL players\") is the base case, not an edge case. The questions are all about how it got greenlit by a $1.8T market cap company despite this incredibly predictable behavior.\n— Nate Silver (@NateSilver538) February 23, 2024\nIn fact, I think there is a precedent for Gemini; like many comparison points for modern-day Google, it comes from Microsoft.\nMicrosoft and The Curse of Culture\nMicrosoft workers celebrated the release to manufacturing of Windows Phone 7 by parading through their Redmond campus on Friday with iPhone and BlackBerry hearses. Employees dressed up in fancy dress and also modified cars to include Windows Phone branding. Aside from the crazy outfits the workers made fake hearses for giant BlackBerry and iPhone devices. Employees cheekily claimed they had buried the competition with Windows Phone 7.\nThis was, to be clear, insane. I wrote about the episode in 2013’s The Curse of Culture; it’s been eight years, so I hope you’ll allow me a particularly long excerpt:\nAs with most such things, culture is one of a company’s most powerful assets right until it isn’t: the same underlying assumptions that permit an organization to scale massively constrain the ability of that same organization to change direction. More distressingly, culture prevents organizations from even knowing they need to do so. From Edgar Schein’s Organizational Culture and Leadership:\nBasic assumptions, like theories-in-use, tend to be nonconfrontable and nondebatable, and hence are extremely difficult to change. To learn something new in this realm requires us to resurrect, reexamine, and possibly change some of the more stable portions of our cognitive structure…Such learning is intrinsically difficult because the reexamination of basic assumptions temporarily destabilizes our cognitive and interpersonal world, releasing large quantities of basic anxiety. Rather than tolerating such anxiety levels, we tend to want to perceive the events around us as congruent with our assumptions, even if that means distorting, denying, projecting, or in other ways falsifying to ourselves what may be going on around us. It is in this psychological process that culture has its ultimate power.\nProbably the canonical example of this mindset was Microsoft after the launch of the iPhone. It’s hard to remember now, but no company today comes close to matching the stranglehold Microsoft had on the computing industry from 1985 to 2005 or so. The company had audacious goals — “A computer on every desk and in every home, running Microsoft software” — which it accomplished and then surpassed: the company owned enterprise back offices as well. This unprecedented success changed that goal — originally an espoused belief — into an unquestioned assumption that of course all computers should be Microsoft-powered. Given this, the real shock would have been then-CEO Steve Ballmer not laughing at the iPhone.\nA year-and-a-half later, Microsoft realized that Windows Mobile, their current phone OS, was not competitive with the iPhone and work began on what became Windows Phone. Still, unacknowledged cultural assumptions remained: one, that Microsoft had the time to bring to bear its unmatched resources to make something that might be worse at the beginning but inevitably superior over time, and two, that the company could leverage Windows’ dominance and their Office business. Both assumptions had become cemented in Microsoft’s victory in the browser wars and their slow-motion takeover of corporate data centers; in truth, though, Microsofts’ mobile efforts were already doomed, and nearly everyone realized it before Windows Phone even launched with a funeral for the iPhone.\nSteve Ballmer never figured it out; his last acts were to reorganize the company around a “One Microsoft” strategy centered on Windows, and to buy Nokia to prop up Windows Phone. It fell to Satya Nadella, his successor, to change the culture, and it’s why the fact his first public event was to announce Office for iPad was so critical. I wrote at the time:\nThis is the power CEOs have. They cannot do all the work, and they cannot impact industry trends beyond their control. But they can choose whether or not to accept reality, and in so doing, impact the worldview of all those they lead.\nMicrosoft under Nadella’s leadership has, over the last three years, undergone a tremendous transformation, embracing its destiny as a device-agnostic service provider; still, it is fighting the headwinds of Amazon’s cloud, open source tooling, and the fact that mobile users had six years to get used to a world without Microsoft software. How much stronger might the company have been had it faced reality in 2007, but the culture made that impossible.\nGoogle is not in nearly as bad of shape as Microsoft was when it held that funeral. The company’s revenue and profits are as high as ever, and the release of Gemini 1.5 in particular demonstrated how well-placed the company is for the AI era: the company not only has leading research, it also has unmatched infrastructure that enables entirely new and valuable use cases. That, though, makes the Gemini fiasco all the more notable.\nThe questions around Google and AI have, to date, been mostly about business model. In last year’s AI and the Big Five I talked about how Kodak invented the digital camera, but didn’t pursue it because of business model reasons, and made the obvious analogy to Google’s seeming inability to ship:\nGoogle has long been a leader in using machine learning to make its search and other consumer-facing products better (and has offered that technology as a service through Google Cloud). Search, though, has always depended on humans as the ultimate arbiter: Google will provide links, but it is the user that decides which one is the correct one by clicking on it. This extended to ads: Google’s offering was revolutionary because instead of charging advertisers for impressions — the value of which was very difficult to ascertain, particularly 20 years ago — it charged for clicks; the very people the advertisers were trying to reach would decide whether their ads were good enough…\nThat, though, ought only increase the concern for Google’s management that generative AI may, in the specific context of search, represent a disruptive innovation instead of a sustaining one. Disruptive innovation is, at least in the beginning, not as good as what already exists; that’s why it is easily dismissed by managers who can avoid thinking about the business model challenges by (correctly!) telling themselves that their current product is better. The problem, of course, is that the disruptive product gets better, even as the incumbent’s product becomes ever more bloated and hard to use — and that certainly sounds a lot like Google Search’s current trajectory.\nGoogle has started shipping, and again, Gemini 1.5 is an incredible breakthrough; the controversy over Gemini, though, is a reminder that culture can restrict success as well. Google has its own unofficial motto — “Don’t Be Evil” — that founder Larry Page explained in the company’s S-1:\nDon’t be evil. We believe strongly that in the long term, we will be better served — as shareholders and in all other ways — by a company that does good things for the world even if we forgo some short term gains. This is an important aspect of our culture and is broadly shared within the company.\nGoogle has by-and-large held to that promise, at least as defined by Page: the company does not sell search result placement. Of course the company has made ads look more and more like organic results, and crammed ever more into the search results page, and squeezed more and more verticals, but while there are always whispers about what is or isn’t included in search, or the decisions made by the algorithm, most people still trust the product, and use it countless times every day.\nOne does wonder, though, if the sanctity of search felt limiting to some inside of Google. In 2018 a video leaked of an all-hands meeting after the 2016 election where Google executives expressed dismay over the results; the footage was damaging enough that Google felt compelled to issue a statement:\nAt a regularly scheduled all hands meeting, some Google employees and executives expressed their own personal views in the aftermath of a long and divisive election season. For over 20 years, everyone at Google has been able to freely express their opinions at these meetings. Nothing was said at that meeting, or any other meeting, to suggest that any political bias ever influences the way we build or operate our products. To the contrary, our products are built for everyone, and we design them with extraordinary care to be a trustworthy source of information for everyone, without regard to political viewpoint.\nPerhaps this seemed to some employees to be an outdated view of the world; I’m reminded of that quote from Angela Y Davis: “In a racist society it is not enough to be non-racist, we must be anti-racist.” In this view calls for color-blindness in terms of opportunity are insufficient; the only acceptable outcome is one in which outcomes are equal as well. The equivalent in the case of Google would be that it is not enough to not be evil; one must be “anti-evil” as well.\nThe end result is that just as Microsoft could, shielded by years of a Windows monopoly, delude themselves into thinking they had an iPhone killer, Google could, shielded by years of a search monopoly, delude themselves into thinking they had not just the right but the obligation to tell users what they ought to believe.\nAs I noted in the excerpt, I very much try to avoid politics on Stratechery; I want to talk about business models and societal impact, and while that has political implications, it doesn’t need to be partisan (for example, I think this piece about the 2016 election holds up very well, and isn’t partisan in the slightest). AI, though, is increasingly giving all of us no choice in the matter.\nTo that end, my Article last fall about the Biden executive order, Attenuating Innovation, was clearly incomplete: not only must we keep in mind the potential benefits of AI — which are massive — but it is clearly essential that we allow open source models to flourish as well. It is Google or OpenAI’s prerogative to train their models to have whatever viewpoint they want; any meaningful conception of freedom should make space for an open market of alternatives, and that means open source.\nSecondly, it behooves me, and everyone else in tech, to write Articles like the one you are reading; “the politics of the median member of the San Francisco Board of Supervisors” has had by far the loudest voice in tech because most people just want to build cool new things, or write about them, without being fired or yelled at on social media. This does, though, give the perception that tech is out of touch, or actively authoritarian; I don’t think that’s true, but those of us who don’t want to tell everyone else what to think, do, paradoxically, need to say so.\nThe biggest question of all, though, is Google. Again, this is a company that should dominate AI, thanks to their research and their infrastructure. The biggest obstacle, though, above and beyond business model, is clearly culture. To that end, the nicest thing you can say about Google’s management is to assume that they, like me and everyone else, just want to build products and not be yelled at; that, though, is not leadership. Schein writes:\nWhen we examine culture and leadership closely, we see that they are two sides of the same coin; neither can really be understood by itself. On the one hand, cultural norms define how a given nation or organizations will define leadership — who will get promoted, who will get the attention of followers. On the other hand, it can be argued that the only thing of real importance that leaders do is to create and manage culture; that the unique talent of leaders is their ability to understand and work with culture; and that it is an ultimate act of leadership to destroy culture when it is viewed as dysfunctional.\nThat is exactly what Nadella did at Microsoft. I recounted in The End of Windows how Nadella changed the company’s relationship to Windows, unlocking the astronomical growth that has happened under his watch, including the company’s position in AI.\nGoogle, quite clearly, needs a similar transformation: the point of the company ought not be to tell users what to think, but to help them make important decisions, as Page once promised. That means, first and foremost, excising the company of employees attracted to Google’s power and its potential to help them execute their political program, and return decision-making to those who actually want to make a good product. That, by extension, must mean removing those who let the former run amok, up to and including CEO Sundar Pichai. The stakes, for Google specifically and society broadly, are too high to simply keep one’s head down and hope that the San Francisco Board of Supervisors magically comes to its senses.\nImage credit Carl J on Flickr",
    "favicon": "/favicon/favicon-32x32.png"
  },
  {
    "title": "The latest Titan Security Key is in the Google Store",
    "link": "https://blog.google/technology/safety-security/titan-security-key-google-store/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNVBPR0ZFVFZVMWRuWXhhRWwzVFJEQ0FSaURBaWdCTWdhbGxKQ01OUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-15T08:00:00.000Z",
    "time": "Nov 15, 2023",
    "articleType": "regular",
    "content": "During today’s Aspen Cyber Summit in New York City, we rolled out the latest version of our Titan Security Key — now available on the Google Store. Throughout 2024, we’ll also begin distributing 100,000 of these new security keys at no cost to global high risk users alongside our industry partners.Why we started with security keysStolen passwords are one of the biggest threats to online security, and we first introduced the Titan Security Key in 2018 to defend against phishing attacks, stopping bad actors from accessing your Google Account. However, we think the best way to eliminate the risks of passwords is to get rid of them altogether. That’s why we helped develop passkeys and announced support for passkeys across all Google Accounts earlier this year. Passkeys are a simpler, safer way to sign into your accounts — without the need for a password. They use FIDO2 credentials and cryptography, so you can use your existing devices to securely confirm who you are.The passkey friendly Titan Security KeyWe’re excited about the potential of passkeys, but know there’s no security silver bullet for everyone. Some people require a solution not dependent on smartphones or use devices that don’t support passkeys — everyone has different approaches to security, but we all share one goal: stop attacks. That’s why we intentionally designed the latest Titan Security Keys to encompass the secure cryptography of passkeys on a portable piece of hardware.The two newly introduced models will replace our current USB-A and USB-C devices — and both will provide NFC capabilities for easy and fast connections with mobile devices. The new keys are also able to store more than 250 unique passkeys — enough storage for users to register them with all their favorite services supporting passkeys — which makes them best-in-class among FIDO2 security keys.\nWith phishing attacks on the rise and busy elections on the horizon, we’re looking forward to providing a tool that not only eliminates the pain that comes with passwords, but that keeps you safer along the way.Get your new Titan Security Key on the Google Store today.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "New investments to help build the U.S. cybersecurity workforce",
    "link": "https://blog.google/outreach-initiatives/google-org/google-cybersecurity-investments-june-2024/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW5UbWhPV1dzek1VMW5TM3BVVFJDb0FSaXJBaWdCTWdhVlVveUlzUVU=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-06-04T07:00:00.000Z",
    "time": "Jun 4",
    "articleType": "regular",
    "content": "Google is investing $5 million more in cybersecurity education to help address the shortage of cybersecurity professionals in the U.S. This brings the total investment to $25 million, supporting 25 cybersecurity clinics nationwide by 2025. The clinics provide students with hands-on experience and help local organizations protect themselves from cyber threats. Fifteen new clinics will receive $1 million in funding, mentorship from Google volunteers, Titan Security Keys, and scholarships for the Google Career Certificate in Cybersecurity.\nSummaries were generated by Google AI. Generative AI is experimental.\nGoogle announces new investments to help build the U.S. cybersecurity workforce.\nAiming to address the shortage of cybersecurity professionals and the growing demand for skilled workers in the field.\nProviding $1 million in Google.org funding, mentorship, and scholarships to 15 new cybersecurity clinics across the U.S.\nThese clinics offer students hands-on experience and help local organizations protect themselves from cyber threats.\nGoogle is committed to making cybersecurity education accessible and supporting diversity within the industry.\nSummaries were generated by Google AI. Generative AI is experimental.\nToday we’re announcing new recipients of the Google Cybersecurity Clinics Fund to help establish 15 cybersecurity clinics across the U.S.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "How we built Immersive View for routes on Maps",
    "link": "https://blog.google/products/maps/google-maps-immersive-view-routes/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW9YMFpMZWt0bFNUSlRWM2xyVFJDb0FSaXJBaWdCTWdhVk1vcXRIUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-02T07:00:00.000Z",
    "time": "Nov 2, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Access Locked Folder in Google Photos across your devices with cloud backup",
    "link": "https://blog.google/products/photos/locked-folder-cloud-backup/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTJaMnBCTUMxYWNYZFdaRmsxVFJDb0FSaXNBaWdCTWdZSm9aUUZPd2M=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-08-29T07:00:00.000Z",
    "time": "Aug 29, 2023",
    "articleType": "regular",
    "content": "Plus, Locked Folder is coming to iOS devices and the web\nWhether you’re taking pictures of an engagement ring you plan to buy or your driver's license to refer back to, you might want to add an extra layer of privacy to some of the content you capture. With Google Photos you’re always in control of the photos you share, save or even view in your photo grid.A few years ago, we launched Locked Folder for Android devices — a passcode-protected space to save sensitive photos and videos on-device, so they won’t show up in your photo grid or other apps. Today, we’re starting to roll out the option to back up your Locked Folder so you can access those photos and videos with your passcode on any of your devices. Plus, you’ll now be able to set up and access Locked Folder on iOS devices and the web.\nPhotos and videos you save and back up in your Locked Folder still won’t appear anywhere else in the app — you won’t see them in your photos grid, Memories, albums or when you search photos in Google Photos. When you turn on backup for Locked Folder, you’ll be able to access that content across your devices when you sign in to Google Photos and open Locked Folder with your device’s passcode. And, as always, all the content you backup in Google Photos is protected by one of the world’s most advanced security infrastructures. If you prefer to not use Locked Folder’s backup capabilities, you still have the option to store content in Locked Folder on your device.Another way we’re helping you take control of your photos and privacy is through a simplified and improved settings page in Google Photos that makes it easier to find and adjust your privacy controls and other settings. The new layout replaces the single settings page with easy to navigate sections for privacy, backup, sharing, notifications and more.\nThe new settings page is available now in Google Photos on Android and iOS, and backup and iOS support for Locked Folder starts rolling out today.\nPhotosHow to use Google Photos AI editing tools, now available to everyone\nPhotosAsk Photos: A new way to search your photos with Gemini\nPixel8 things I loved in my first week with the Pixel 8a\nPixel4 tips on getting the most out of Pixel 8 Pro’s Video Boost\nPhotosAI editing tools are coming to all Google Photos users\nGoogle Workspace5 AI tools to help organize your digital life",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Bard gets its biggest upgrade yet with Gemini",
    "link": "https://blog.google/products/gemini/google-bard-try-gemini-ai/",
    "image": "https://news.google.com/api/attachments/CC8iMkNnNXJZMEoyV1ZveGJVMXFkRkZmVFJDb0FSaXJBaWdCTWdzQkFJSzFVR081MW9oTHlB=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-12-06T08:00:00.000Z",
    "time": "Dec 6, 2023",
    "articleType": "regular",
    "content": "Today we announced Gemini, our most capable model with sophisticated multimodal reasoning capabilities. Designed for flexibility, Gemini is optimized for three different sizes — Ultra, Pro and Nano — so it can run on everything from data centers to mobile devices.Now, Gemini is coming to Bard in Bard’s biggest upgrade yet. Gemini is rolling out to Bard in two phases: Starting today, Bard will use a specifically tuned version of Gemini Pro in English for more advanced reasoning, planning, understanding and more. And early next year, we’ll introduce Bard Advanced, which gives you first access to our most advanced models and capabilities — starting with Gemini Ultra.Try Gemini Pro in BardBefore bringing it to the public, we ran Gemini Pro through a number of industry-standard benchmarks. In six out of eight benchmarks, Gemini Pro outperformed GPT-3.5, including in MMLU (Massive Multitask Language Understanding), one of the key leading standards for measuring large AI models, and GSM8K, which measures grade school math reasoning.On top of that, we’ve specifically tuned Gemini Pro in Bard to be far more capable at things like understanding, summarizing, reasoning, coding and planning. And we’re seeing great results: In blind evaluations with our third-party raters, Bard is now the most preferred free chatbot compared to leading alternatives.We also teamed up with YouTuber and educator Mark Rober to put Bard with Gemini Pro to the ultimate test: crafting the most accurate paper airplane. Watch how Bard helped take the creative process to new heights.\nYou can try out Bard with Gemini Pro today for text-based prompts, with support for other modalities coming soon. It will be available in English in more than 170 countries and territories to start, and come to more languages and places, like Europe, in the near future.\nLook out for Gemini Ultra in an advanced version of Bard early next yearGemini Ultra is our largest and most capable model, designed for highly complex tasks and built to quickly understand and act on different types of information — including text, images, audio, video and code.One of the first ways you’ll be able to try Gemini Ultra is through Bard Advanced, a new, cutting-edge AI experience in Bard that gives you access to our best models and capabilities. We’re currently completing extensive safety checks and will launch a trusted tester program soon before opening Bard Advanced up to more people early next year.This aligns with the bold and responsible approach we’ve taken since Bard launched. We’ve built safety into Bard based on our AI Principles, including adding contextual help, like Bard’s “Google it” button to more easily double-check its answers. And as we continue to fine-tune Bard, your feedback will help us improve.With Gemini, we’re one step closer to our vision of making Bard the best AI collaborator in the world. We’re excited to keep bringing the latest advancements into Bard, and to see how you use it to create, learn and explore. Try Bard with Gemini Pro today.\nExplore our collection to find out more about Gemini, the most capable and general model we’ve ever built.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "4 Google updates coming to Samsung devices",
    "link": "https://blog.google/products/android/google-updates-samsung-galaxy-unpacked-2024/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNHlaWFIyVVdwVmJHcFFUVTltVFJDb0FSaXJBaWdCTWdrQllJeUdydWRNREFJ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-07-10T07:00:00.000Z",
    "time": "Jul 10",
    "articleType": "regular",
    "content": "Today at Galaxy Unpacked, we announced that Circle to Search's latest capabilities, Wear OS 5 and more are coming to Samsung’s new devices.\nToday at Galaxy Unpacked, we announced four Google updates coming to Samsung’s latest devices, including the Galaxy Z Flip6, Z Fold6 and the newest Galaxy Watches. Here’s what to expect:1. Get suggestions from Gemini based on what’s on your screen\nWith the Gemini app on Android, you have a helpful, personal AI assistant right on your phone. You can easily learn about the world around you, find the right words and generate images on the fly. And because Gemini is deeply integrated with Google apps, it can help you take action across Gmail, Docs, Maps, Drive and more.With the new Galaxy Z series, Gemini will soon give relevant suggestions based on what's on your screen. Just swipe the corner of the screen or say “Hey Google” to bring up Gemini. And if you’re, say, watching something on YouTube, the suggestion “Ask about this video” will appear. The Galaxy Z Fold6’s large screen is particularly helpful for multitasking. Soon, when you watch a video, you’ll be able to bring up the Gemini overlay and move it for a split screen experience.The Gemini app on Android is currently available in 29 languages in over 200 countries. These updates on the Galaxy series will be available in the coming months.2. Circle to Search on the new Galaxy Z Series\nAt I/O, we shared how Wear OS 5 brings improved performance and battery life. Samsung’s new Galaxy Watch lineup, including the Watch Ultra and Watch7, will be the first smartwatches powered by Wear OS 5. And they’re the perfect companion for when you’re on the go: These smartwatches offer advanced health monitoring capabilities, including heart rate tracking and sleep monitoring, and a personalized health experience, as well as access to a wide range of apps in Google Play.4. Watch YouTube TV in multiviewOn the GalaxyZ Fold6, YouTube TV subscribers will be able to watch in multiview, enjoying up to four different streams at the same time. You can choose from pre-selected combinations of football, news, weather and simultaneous sporting events.We’re constantly working with Samsung to bring the latest Google updates to Galaxy products, from smartphones and wearables to even future technologies, like the upcoming XR platform. Check out everything that was announced at Galaxy Unpacked today.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Enhance visual storytelling in Demand Gen with generative AI",
    "link": "https://blog.google/products/ads-commerce/enhance-visual-storytelling-in-demand-gen-with-generative-ai/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXFXbU5zZFhKQ2FIWkhkV3hLVFJDb0FSaXJBaWdCTWdZTmNwTEZKZ2c=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-04-16T07:00:00.000Z",
    "time": "Apr 16",
    "articleType": "regular",
    "content": "New AI-powered creative features are coming to Demand Gen campaigns to help take your visual storytelling to the next level. With generative image tools, you can now create a variety of compelling image assets in just a few steps.\nDemand Gen campaigns were introduced last year to help advertisers create and convert new demand with visual storytelling on Google’s most immersive and entertainment-focused touchpoints — YouTube, YouTube Shorts, Discover, and Gmail. And now, Google is empowering brands and agencies with more tools to tell a compelling and engaging story in a fast-paced, non-linear digital landscape.Starting today, generative image tools in Demand Gen will be rolling out to advertisers around the world in English with more languages to come later this year. Powered by Google AI, these tools create stunning, high-quality image assets in just a few steps using prompts provided by you. And, if you have existing images that perform well, you can generate similar options with the new “Generate more like this” feature.\nMultiply your creative impact in secondsWith generative image tools, you can now test new creative concepts more efficiently — whether it’s experimenting with new types of images or simply building your creatives from scratch. Your knowledge and expertise are crucial to help Google AI generate images tailored to your business or client's needs.For example, if you have an outdoor lifestyle brand that sells camping gear, use prompts like “vibrantly colored tents illuminated under the Aurora Borealis” to create images that will appeal to those who are shopping for camping trips to Iceland.\nDemand Gen will now allow you to provide text prompts to generate more image assets based on your preferences.\nOf course, you’ll remain in full control to decide which suggested images will be added to your campaigns. Google is committed to upholding our principles for developing generative AI technology responsibly for fairness, privacy and security. On top of making sure advertising content adheres to our long-standing Google Ads policies, we also employ additional technical measures to ensure generative image tools in Google Ads produce novel and unique content. Google AI will never create two identical images. And all images generated by Google Ads include mechanisms that allow them to be identified as generated, such as an open-standard markup that will surface on tools like Google Image Search, as well as a SynthID, which is an imperceptible, digital watermark that is resistant to manipulations of the image, such as screenshots, filters, and compression. Read more about eligibility and safety principles for generated images in Google Ads on our support page.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google’s AI Overview Search Results Copied My Original Work",
    "link": "https://www.wired.com/story/google-ai-overview-search-results-copied-my-original-work/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNXBSbEZEZEhaSlZ5MVdNa0Z4VFJEaEFSamhBU2dCTWdtUlVwYUlzdWRVaUFJ=-w400-h224-p-df-rw",
    "source": "WIRED",
    "datetime": "2024-06-05T07:00:00.000Z",
    "time": "Jun 5",
    "articleType": "regular",
    "content": "Reece Rogers via GoogleWhile many AI lawsuits remain unresolved, one legal expert I spoke with who specializes in copyright law was skeptical whether I could win any hypothetical litigation. “I think you would not have a strong case for copyright infringement,” says Janet Fries, an attorney at Faegre Drinker Biddle & Reath. “Copyright law, generally, is careful not to get in the way of useful things and helpful things.” Her perspective focused on the type of content in this specific example of original work, explaining that it is quite difficult to make a claim about instructional or fact-based writing, like my advice column, versus more creative work, like poetry.I’m definitely not the first person to suggest focusing on your intended audience when writing chatbot prompts, so I agree that the fact-based aspect of my writing does complicate the overall situation. It’s hard for me, though, to imagine a world where Google arrives at that exact paragraph about Claude’s chatbot in its AI Overview results without referencing my work first.",
    "favicon": "https://www.wired.com/verso/static/wired/assets/favicon.ico"
  },
  {
    "title": "Put Google AI to work with Search ads",
    "link": "https://blog.google/products/ads-commerce/put-google-ai-to-work-with-search-ads/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNUtjVVZ1ZEZseGJURm5iR04xVFJDb0FSaXJBaWdCTWdNRmdSSQ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-01-23T08:00:00.000Z",
    "time": "Jan 23",
    "articleType": "regular",
    "content": "Powered by the latest breakthroughs in large language models, the conversational experience in Google Ads helps you build better Search campaigns that drive more performance with less effort.\nGenerative AI can empower advertisers from streamlining campaign creation to increasing the effectiveness of ads as the consumer Search experience evolves. Last year, we introduced a new era of AI-powered ads along with a commitment to ensuring advertisers have the opportunity to reach potential customers along their search journeys. Today, we’re sharing an update on our progress.\nBuilding better Search campaigns through Gemini-powered chatAs we announced last month, Gemini, our largest and most capable AI model, will expand to more of our core products in the coming months, including Google Ads. We’ve been actively testing Gemini to further enhance our ads solutions. And, we’re pleased to share that Gemini is now powering the conversational experience. It’s the first of many Gemini integrations to come.Beta access to the conversational experience in Google Ads is now fully available to English language advertisers in the U.S. and U.K. It will begin rolling out globally to all English language advertisers over the next few weeks. We’re excited to open up access in additional languages in the months ahead and look forward to hearing your feedback.The conversational experience workflow is designed to help you build better Search campaigns through a chat-based experience. It combines your expertise with Google AI. All you need to start is your website URL and Google AI will help you create optimized Search campaigns by generating relevant ad content, including creatives and keywords.Over the last few months, we’ve been testing the conversational experience with a small group of advertisers. We observed that it helps them build higher quality Search campaigns with less effort. One of the ways we measure this is with a metric called Ad Strength that looks at the relevance, quality and diversity of your ad copy and gives you a score from “Poor” all the way up to “Excellent.”\nI found the conversational experience very easy to use. It helped me create even more high-quality ads with ‘Good’ or ‘Excellent’ Ad Strength, which has further improved the performance of my campaigns.\nPaid Search Manager at Page1\nAs Search becomes more visual, we’ve heard advertisers tell us that it can be challenging to create compelling images that drive performance. That’s why we designed the conversational experience to suggest images tailored to your campaign using generative AI and images from your landing page. This capability will be added over the coming months. Advertisers approve assets — including images — before the campaign goes live.All images created with generative AI in Google Ads, including the conversational experience, will be identified as such. We’re using SynthID to invisibly watermark these images and they will include open standard metadata to indicate the image was generated by AI.\nChat your way into better performance with the conversational experience in Google Ads\nOur data shows that small business advertisers that use the conversational experience in Google Ads are 42% more likely to publish Search campaigns with “Good” or “Excellent” Ad Strength.1 This is significant because we’ve found a strong correlation between Ad Strength and conversions. For example, advertisers who improve Ad Strength for their responsive search ads from “Poor” to “Excellent” see 12% more conversions on average.2\nTaking a bold and responsible AI approachWe share the industry’s enthusiasm for AI’s potential to unlock value for consumers and advertisers alike. New use cases — and opportunities — are emerging on a regular basis as AI continues to evolve quickly. And that’s why we think AI will continue to make our products even more useful.As we continue to explore what’s possible with AI, we remain committed to developing and applying this new technology responsibly. We believe the best way to do this is by upholding our AI principles that we established in 2018.Stay tuned for more AI-powered updates to Google Ads in the year ahead. And if you’re interested in getting started with AI today, check out our AI Essentials.\nGoogle AdsNew reporting and genAI tools to boost creative results\nAnalyticsFour ways Google Analytics delivers actionable insights for your business\nGoogle AdsThe AI Era: Expanding marketing and creative potential\nGoogle AdsMeet the Google TV network\nGoogle AdsAds creativity and performance at scale with Google AI\nShoppingNew AI tools to help merchants market brands and products",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Epic Games wins antitrust lawsuit against Google over barriers to its Android app store",
    "link": "https://apnews.com/article/google-epic-games-antitrust-trial-android-app-store-dd6b26be7447b5ff8cc0d20a4d01b6b4",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUhVak55VkVwR1N5MTZTMXBMVFJDM0FSaVRBaWdCTWdhcFpZVE5xUWM=-w400-h224-p-df-rw",
    "source": "The Associated Press",
    "datetime": "2023-12-11T08:00:00.000Z",
    "time": "Dec 11, 2023",
    "articleType": "regular",
    "content": "Updated 3:01 AM GMT+2, December 12, 2023\nSAN FRANCISCO (AP) — A federal court jury has decided that Google’s Android app store has been protected by anticompetitive barriers that have damaged smartphone consumers and software developers, dealing a blow to a major pillar of a technology empire.The unanimous verdict reached Monday came after just three hours of deliberation following a four-week trial revolving around a lucrative payment system within Google’s Play Store. The store is the main place where hundreds of millions of people around the world download and install apps that work on smartphones powered by Google’s Android software.Epic Games, the maker of the popular Fortnite video game, filed a lawsuit against Google three years ago, alleging that the internet search giant has been abusing its power to shield its Play Store from competition in order to protect a gold mine that makes billions of dollars annually. Just as Apple does for its iPhone app store, Google collects a commission ranging from 15% to 30% on digital transactions completed within apps.\nApple prevailed in a similar case that Epic brought against the iPhone app store. But that 2021 trial was decided by a federal judge in a ruling that is under appeal at the U.S. Supreme Court.\nThe nine-person jury in the Play Store case apparently saw things through a different lens, even though Google technically allows Android apps to be downloaded from different stores — an option that Apple prohibits on the iPhone.\nJust before the Play Store trial started, Google sought to avoid having a jury determine the outcome, only to have its request rejected by U.S. District Judge James Donato. Now it will be up to Donato to determine what steps Google will have to take to unwind its illegal behavior in the Play Store. The judge indicated he will hold hearings on the issue during the second week of January.Epic CEO Tim Sweeney broke into a wide grin after the verdict was read and slapped his lawyers on the back and also shook the hand of a Google attorney, whom he thanked for his professional attitude during the proceedings.\n“Victory over Google!” Sweeney wrote in a post on X, the platform formerly known as Twitter. In a company post, Epic hailed the verdict as “a win for all app developers and consumers around the world.”Google plans to appeal the verdict, according to a statement from Wilson White, the company’s vice president of government affairs and public policy. “Android and Google Play provide more choice and openness than any other major mobile platform,” White said.Depending on how the judge enforces the jury’s verdict, Google could lose billions of dollars in annual profit generated from its Play Store commissions. The company’s main source of revenue — digital advertising tied mostly to its search engine, Gmail and other services — won’t be directly affected by the trial’s outcome.The jury reached its decision after listening to two hours of closing arguments from the lawyers on the opposing sides of the case.Epic lawyer Gary Bornstein depicted Google as a ruthless bully that deploys a “bribe and block” strategy to discourage competition against its Play Store for Android apps. Google lawyer Jonathan Kravis attacked Epic as a self-interested game maker trying to use the courts to save itself money while undermining an ecosystem that has spawned billions of Android smartphones to compete against Apple and its iPhone.\nMuch of the lawyers’ dueling arguments touched upon the testimony from a litany of witnesses who came to court during the trial.The key witnesses included Google CEO Sundar Pichai, who sometimes seemed like a professor explaining complex topics while standing behind a lectern because of a health issue, and Sweeney, who painted himself as a video game lover on a mission to take down a greedy tech titan.In his closing argument for Epic, Bornstein railed against Google for exploiting its power over the Android software in a way that “has led to higher prices for developers and consumers, as well as less innovation and quality.”\nGoogle has staunchly defended the commissions as a way to help recoup the more than $40 billion that it has poured into building into the Android software that it has been giving away since 2007 to manufacturers to compete against the iPhone.“Android phones cannot compete against the iPhone without a great app store on them,” Kravis asserted in his closing argument. “The competition between the app stores is tied to the competition between the phones.”But Bornstein ridiculed the notion of Google and Android competing against Apple and its incompatible iPhone software system. “Apple is not the ‘get out of jail for free’ card that Google wants it to be,” Bornstein told the jury.Google also pointed to rival Android app stores such as the one that Samsung installs on its popular smartphones as evidence of a free market. Combined with the rival app stores pre-installed on devices made by other companies, more than 60% of Android phones offer alternative outlets for Android apps.\nEpic, though, presented evidence asserting the notion that Google welcomes competition as a pretense, citing the hundreds of billions of dollars it has doled out to companies, such as game maker Activision Blizzard, to discourage them from opening rival app stores. Besides making these payments, Bornstein also urged the jury to consider the Google “scare screens” that pop up, warning consumers of potential security threats when they try to download Android apps from some of the alternatives to the Play Store.“These are classic anticompetitive strategies used by dominant firms to protect their monopolies,” Bornstein said. Google’s empire could be further undermined by another major antitrust trial in Washington that will be decided by a federal judge after hearing final arguments in May. That trial has cast a spotlight on Google’s cozy relationship with Apple in online search, the technology that turned Google into a household word a few years after two former Stanford University graduate students started the company in a Silicon Valley garage in 1998.",
    "favicon": "/favicon-32x32.png"
  },
  {
    "title": "Boost your productivity: Use Gemini in Gmail, Docs and more with the new Google One plan",
    "link": "https://blog.google/products/google-one/google-one-gemini-ai-gmail-docs-sheets/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNHRabUZOUW1kNGVXSnFRVmR6VFJDb0FSaXJBaWdCTWdPTnNRUQ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-02-21T08:00:00.000Z",
    "time": "Feb 21",
    "articleType": "regular",
    "content": "With the Google One AI Premium plan, you can get more creative and productive using Gemini in Gmail, Docs, Slides, Sheets and Meet.\nSince Google One first launched in 2018, more than 100 million people have subscribed to get more out of Google, including extra storage and advanced features in Google Photos, Meet and Calendar. And just a few weeks ago we announced the Google One AI Premium plan, which gives you access to Gemini Advanced — a new experience that uses 1.0 Ultra, our largest and most capable AI model widely available today.Now we’re bringing even more value to the Google One AI Premium plan with Gemini in Gmail, Docs, Slides, Sheets and Meet (formerly Duet AI). Starting today, you’ll be able to access Gemini capabilities directly within the Google products you’re already using, and get more done without jumping between tabs or apps. For instance, you can get help writing a potluck invitation in Gmail, drafting a trip itinerary in Docs, building a family budget in Sheets or creating a vision board for a kitchen remodel in Slides.\nIn Google Workspace, our long-standing commitments to protect user data and prioritize privacy continue to guide everything we do. We don’t use your personal or business Workspace data to train or improve the underlying generative AI and large language models that power other systems outside of Workspace without permission. You can read more about our core privacy principles in the generative AI era.Gemini in Gmail, Docs, Slides, Sheets and Meet is starting to roll out today for AI Premium Plan members in more than 150 countries in English. AI Premium members also have access to Gemini Advanced, 2TB of storage and other Google One benefits for $19.99/month.\nIf you’re not already a Google One AI Premium member, you can start a two-month trial at no cost today.\nGoogle Workspace5 tips for writing great prompts for Gemini in the Workspace side panel\nGeminiGemini’s big upgrade: Faster responses with 1.5 Flash, expanded access and more\nSearch8 ways to keep up with the Olympic Games Paris 2024 on Google\nCompany announcements4 ways Google will show up in NBCUniversal’s Olympic Games Paris 2024 coverage\nAndroid4 Google updates coming to Samsung devices\nLearning & EducationNew AI tools for Google Workspace for Education",
    "favicon": "/favicon.ico"
  },
  {
    "title": "AlphaFold 3 predicts the structure and interactions of all of life’s molecules",
    "link": "https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/R",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUtWamxtYW5ONE1WTlpWQzFKVFJDb0FSaXJBaWdCTWdZcFpaRE5LUWM=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-05-08T07:00:00.000Z",
    "time": "May 8",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "New features to celebrate Messages' 1 billion RCS users",
    "link": "https://blog.google/products/android/7-new-messages-features/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNWFhekJwWlVsS1ZHVkxaVjkzVFJDb0FSaXJBaWdCTWdrQlVJcm1PU1VtWndJ=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-11-30T08:00:00.000Z",
    "time": "Nov 30, 2023",
    "articleType": "regular",
    "content": "Google Messages welcomes a new era of personalized messaging with 7 new features to help your personality shine through.\nFor years, we’ve been working across the mobile industry, including carriers and smartphone manufacturers around the world, to evolve and adopt RCS, the modern texting standard.By replacing the outdated SMS and MMS protocols, users benefit from a more modern and secure messaging experience with typing indicators, read receipts, threaded replies, high-quality media sharing, improved group chats, better privacy like end-to-end encryption and more with RCS. RCS is a much better messaging protocol, and we know that people love using it.Today marks a new milestone that we are incredibly proud of: There are now more than one billion monthly active users with RCS enabled in Google Messages. We are grateful to our partners and our users that have advocated for RCS over the years — it’s been a lot of work to get here, and we want to thank you. Beyond Google Messages, there are other messaging clients that use RCS and we are pleased that Apple also took their first step two weeks ago in announcing that they’re embracing RCS.\nTo celebrate our one billion milestone, Google Messages is introducing seven fun new ways to express yourself when communicating and connecting with other Android users – from shared themes and screen effects to AI-powered reactions. Here they are!1. Photomoji\nPhotomoji allows you to transform your favorite photos into reactions with the help of on-device Google AI. Want to react to a text with a snap of you or your four-legged best friend? Simply select the photo, choose the object you’d like to turn into a Photomoji and hit send. Your creations will be saved in a special tab for reuse and, as a bonus, your friends in group chats can use your sent Photomoji as well.2. Voice Moods, plus improved audio quality\nVoice Moods bring life into voice messaging. With nine different emotions to choose from, your voice can sprinkle heart-eye emoji, fume with fireballs or break out the party popper so the recipient can hear your words along with a visual effect that expresses how you're feeling at that moment. Thanks to user feedback, we’ve also improved the overall audio quality of voice messages by increasing the bitrate and sampling rate.3. Screen Effects\nWith Screen Effects, your messages come alive with vibrant animations that will transform your words into dazzling visual displays. Simply type specific messages like \"it's snowing\" or \"I love you\" and watch your screen erupt in a symphony of colors and motion. And together with your friends, try to uncover all 15+ hidden Screen Effects prompt words.4. Custom Bubbles\nWith new Custom Bubbles, you can customize the bubble color and backgrounds of your conversation and the person you’re communicating with. Pick a different color for every chat if you like — it’s no longer about blue vs. green. Now you can differentiate your conversations and avoid accidentally texting your family group chat a weekend update meant for your friend.5. Reaction Effects\nReaction Effects is an addition to your messaging experience that injects more life into your conversations. Picture this: Your friend texts you to let you know the two of you have reservations to that trendy new restaurant that everyone has been buzzing about. If you react to the message with a simple 👍emoji, magic instantly unfolds as an animated trio of hands dances around the message bubble. This feature, designed to add a dynamic twist to your chats, embraces the power of expression with all ten of these popular emoji: 👍❤️ 😂😮😡👎💩🎉😠😢.6. Animated Emoji\nWhile emoji are the foundation of our expressions when we text, Animated Emoji take it a step further by infusing each message with a captivating burst of visual effects. For example, if your friend sends you a message revealing the breathtaking vacation spot they just booked for your trip and in response you tap ❤️, you’ll unleash an Animated Emoji that sparkles with a dazzling pink heart to properly express your excitement.7. Profile\nEasily change your profile name and picture that accompanies your phone number. We know self-expression is important, so you can now change how you appear across Google services. This helps people recognize you, particularly in group chats.Try the new Google Messages todayWhether you’re using Reaction effects to stay connected in the group chat over the holidays or sending a heartfelt message with Voice Moods to a friend on New Year’s Eve, we want your Google Messages with RCS experience to feel authentic–to all one billion of you!Download Google Messages from the Google Play Store and start enjoying many of these new features rolling out to beta starting today1.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Confused by All of Google's AI Tools? We Break Down 13 of Them",
    "link": "https://www.cnet.com/tech/services-and-software/google-io-ai-tools-breakdown/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNXVaV2xFU1cxUWJVbFJUbVJNVFJDb0FSaXNBaWdCTWdhUlU1aG9KUWc=-w400-h224-p-df-rw",
    "source": "CNET",
    "datetime": "2024-05-14T07:00:00.000Z",
    "time": "May 14",
    "articleType": "regular",
    "content": "",
    "favicon": "/favicon-96.png"
  },
  {
    "title": "7 years of software updates for the Pixel 8 series",
    "link": "https://blog.google/products/pixel/software-support-pixel-8-pixel-8-pro/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWZiRWh2T0RkTWJpMXhSbVkxVFJDM0FSaVRBaWdCTWdZZFFvWUxzZ1k=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-10-04T07:00:00.000Z",
    "time": "Oct 4, 2023",
    "articleType": "regular",
    "content": "Today we announced our commitment to providing seven years of software support for Pixel 8 and Pixel 8 Pro, including Android OS upgrades, security updates and regular Feature Drops.1 That means your Pixel 8 and Pixel 8 Pro will be supported all the way into 2030. No major smartphone brand offers this committed level of support and longevity. This makes these phones a more sustainable smartphone choice, because they’ll be secure and perform well for seven years.\nMeet Pixel 8 and Pixel 8 Pro, our newes…\nTake a closer look at the new phones — everything from the beautiful design and new sensors to updated cameras.\nThe latest Android OS updates and Feature DropsWith Pixel, you’re treated to exclusive features, updates and fixes that keep your phone working harder for you. Think Pixel Feature Drops, security upgrades, Android software releases and AI technology from Google. We know your whole life is on your phone — from photos to passwords — so we’re providing consistent security fixes and improvements that help keep you and your data secure.Extending our commitment with Pixel 8 and Pixel 8 Pro was a natural progression for us, especially as we’re seeing people use their Pixel phones longer and longer. To make this possible, we’ve been working to secure long-term commitments from partner teams, and put the necessary testing infrastructure in place.We also dug into how we can deliver the highest quality, best tested updates to Pixel users on a consistent basis. As part of this effort, our security updates, bug fixes and feature updates won’t roll out on a specific day each month. Instead, we’ll deploy updates as soon as they’ve completed the necessary tests to ensure they improve the experience for all Pixel customers.All-new features, all the timeA hallmark of Pixel devices, Feature Drops roll out multiple times a year — to all supported Pixel devices — and include feature enhancements and even new features across areas like safety, security, camera and more. For example, you might receive updates to all-time favorite features — like Night Sight, Call Assist, Magic Eraser and Recorder — automatically and without needing to get a new phone.Pixel 8 and Pixel 8 Pro are the first phones to launch with the new Android 14. Google will also extend hardware support and make parts available for seven years to match the software commitment. The result? You can be confident that your Pixel gets even more helpful over time.\nMade By Google: Helpful, simple and per…\nTake a look at our growing family of Pixel devices packed with premium hardware, helpful software and state-of-the-art AI research.",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Passkeys are now available for high risk users to enroll in the Advanced Protection Program",
    "link": "https://blog.google/technology/safety-security/google-passkeys-advanced-protection-program/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNUZPWFZ5YW1wMWRXVjJVRFJEVFJDb0FSaXJBaWdCTWdhaFU0eXRKUVk=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-07-10T07:00:00.000Z",
    "time": "Jul 10",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google Cuts Hundreds of Jobs in Engineering and Other Divisions",
    "link": "https://www.nytimes.com/2024/01/11/technology/google-layoffs.html",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNWtjMnRsTmxObWRHbFRURWczVFJDM0FSaVRBaWdCTWdZWlpZU3JMUVk=-w400-h224-p-df-rw",
    "source": "The New York Times",
    "datetime": "2024-01-11T08:00:00.000Z",
    "time": "Jan 11",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Meet Pixel 8 and Pixel 8 Pro, our newest phones",
    "link": "https://blog.google/products/pixel/google-pixel-8-pro/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHpZa3BOYlhSV09YSTVOM0ZwVFJEQkFSaUZBaWdCTWdhZGc0eU5zUVU=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2023-10-04T07:00:00.000Z",
    "time": "Oct 4, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google Zero is here — now what?",
    "link": "https://www.theverge.com/24167865/google-zero-search-crash-housefresh-ai-overviews-traffic-data-audience",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNHdNM1J0V2tKTVVUZzRhRlpDVFJDM0FSaVRBaWdCTWdZbE1aQ3VMUWM=-w400-h224-p-df-rw",
    "source": "The Verge",
    "datetime": "2024-05-30T07:00:00.000Z",
    "time": "May 30",
    "articleType": "regular",
    "content": "We’ve been covering big changes to Google and Google Search very closely here on Decoder and The Verge. There’s a good reason for that: the entire business of the modern web is built around Google.It’s a whole ecosystem. Websites get traffic from Google Search, they all get built to work in Google Chrome, and Google dominates the stack of advertising technologies that turn all of it into money. It’s honestly been challenging to explain just how Google operates as a platform, because it’s so large, pervasive, and dominant that it’s almost invisible.But if you think about it another way — considering the relationship YouTubers have to YouTube or TikTokers have to the TikTok algorithm — it starts to become clear. The entire web is Google’s platform, and creators on the web are often building their entire businesses on that platform, just like any other.I think about Decoder as a show for people who are trying to build things, and the number one question I have for people who build things on any platform is: what are you going to do when that platform changes the rules?There’s a theory I’ve had for a long time that I’ve been calling “Google Zero” — my name for that moment when Google Search simply stops sending traffic outside of its search engine to third-party websites.Regular Decoder listeners have heard me talk a lot about Google Zero in the last year or two. I asked Google CEO Sundar Pichai about it directly earlier this month. I’ve also asked big media executives, like The New York Times’ Meredith Kopit Levien and Fandom’s Perkins Miller, how it would affect them. Nobody has given me a good answer — and it seems like the media industry still thinks it can deal with it when the time comes. But for a lot of small businesses. Google Zero is now. It’s here, it’s happening, and it can feel insurmountable.Earlier this year, a small site called HouseFresh, which is dedicated to reviewing air purifiers, published a blog post that really crystallized what was happening with Google and these smaller sites. HouseFresh managing editor Gisele Navarro titled the post “How Google is killing independent sites like ours,” and she had receipts. The post shared a whole lot of clear data showing what specifically had happened to HouseFresh’s search traffic — and how big players ruthlessly gaming SEO were benefiting at their expense.I wanted to talk to Gisele about all of this, especially after she published an early May follow-up post with even more details about the shady world of SEO spam and how Google’s attempts to fight it have crushed her business.I often joke that The Verge is the last website on Earth, but there’s a kernel of truth to it. Building an audience on the web is harder than ever, and that leaves us with one really big question: what’s next? Folks like Gisele, who make all the content Google’s still hoovering up but not really serving to users anymore, have a plan.",
    "favicon": "/icons/favicon.ico"
  },
  {
    "title": "How Google uses AI to reduce stop-and-go traffic on your route — and fight fuel emissions",
    "link": "https://blog.google/outreach-initiatives/sustainability/google-ai-project-greenlight/",
    "image": "https://news.google.com/api/attachments/CC8iJ0NnNU1XV2xzUm1obVJXSjVjbmhqVFJDUkFSamNBaWdCTWdNUllnZw=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-07-29T14:18:49.000Z",
    "time": "3 days ago",
    "articleType": "regular",
    "content": "At the start of 2020, a team within Google Research was asked to explore new ideas for research projects that focused on accelerating climate mitigation. “We were looking into all kinds of big ideas, from cultivated meat to energy to air pollution,” Dotan Emanuel, a software engineer on the team, says.At the dinner table one evening, Dotan shared some of those big ideas with his family — and the conversation soon pivoted to a frustration familiar to many of us: “My wife Osnat said, ‘Why don’t you do something about traffic lights? We stand at them for no good reason,’” he recalls.Road transportation is responsible for significant global and urban greenhouse gas emissions. It’s especially problematic at city intersections where pollution can be 29 times higher than on open roads, and about half of these emissions come from traffic accelerating after stopping. With millions of traffic lights across the world, the scale of the problem was huge — and if Google could do something to address it, so was the opportunity.“My initial thought was that we can’t do anything about traffic lights,” Dotan says. “But when it comes to research, the most fascinating challenges lie in the unknown.”With their curiosity sufficiently piqued, Dotan and his team dug into the mechanics of traffic engineering. They found that while some amount of stop-and-go traffic is unavoidable, a portion can be prevented by optimizing traffic light timing. To do that, cities traditionally needed to either install expensive hardware or run time-consuming manual vehicle counts, neither of which provide complete information on key parameters they need.“We quickly understood we have a strong advantage that cities could benefit from — over a decade of Google Maps driving trends from across the globe,” Dotan says. “And a few weeks later, we had a project proposal ready.”That proposal was for Project Green Light, an initiative that uses AI to make recommendations for city engineers to optimize existing traffic lights and reduce stop-and-go emissions. After evaluating dozens of other great ideas, Green Light was chosen for its simplicity, scalability and potential for impact.\nThe Green Light team used Google Maps’ driving trends to create an AI model that measures how traffic flows through an intersection, including patterns of starting and stopping, average wait times at a traffic light, and coordination between adjacent intersections. The model identifies possible improvements, like shaving off several seconds from a red traffic light during off-peak hours or an opportunity to coordinate between intersections that aren’t yet synced. The city’s engineers then review those recommendations and can implement them in as little as five minutes, using their city's existing infrastructure.“In order to achieve a positive climate impact, we want to be able to deploy high-quality Green Light recommendations to many cities globally and scale fast. So we purposely set up everything to be simple and lightweight — cities don’t need to invest in any dedicated software or hardware integrations,” says Green Light Program Manager Alon Harris. “We just share our recommendations with the city, and then they evaluate them and take action.”Since their first pilot in 2021, the team has tested more and more intersections, developed more accurate predictions and took Green Light on the road to more than a dozen cities across the world, including Rio de Janeiro, Seattle, Bengaluru, and most recently, Boston. The team also developed a comprehensive dashboard to easily share recommendations and analytics with partner cities, while continuing to monitor for any new needed changes.\nThe Green Light dashboard provides city-specific actionable recommendations and supporting trends. After a recommendation has been implemented, the dashboard shows an impact analysis report.\n“We offer each city dedicated reports with tangible impact metrics, such as how many stops drivers saved at an intersection over time. We think that’s going to be a real incentive to not just implement the first recommendations, but also bring Green Light to more intersections,” Alon says.Today, Green Light is live in over 70 intersections, helping to save fuel and lower emissions for up to 30 million car rides monthly. Early numbers indicate the potential to reduce stops by up to 30% and reduce emissions at intersections by up to 10%.The team is working to scale Green Light to hundreds of cities and tens of thousands of intersections in the next few years. And on that road to making the world a little better, they hope helping people experience less stop-and-go traffic will spread some joy along the way.“You know when you drive through a sequence of five green lights, and it feels like your lucky day?” Dotan says. “We want to make that feeling more common to more people around the world.”",
    "favicon": "/favicon.ico"
  },
  {
    "title": "Google to defend generative AI users from copyright claims",
    "link": "https://www.reuters.com/technology/google-defend-generative-ai-users-copyright-claims-2023-10-12/",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNTVhR1ZsYUdOb2JqVmxUalJHVFJDM0FSaVRBaWdCTWdhRk00U0dHZ28=-w400-h224-p-df-rw",
    "source": "Reuters",
    "datetime": "2023-10-12T07:00:00.000Z",
    "time": "Oct 12, 2023",
    "articleType": "regular",
    "content": "",
    "favicon": ""
  },
  {
    "title": "Google cuts hundreds of jobs across engineering, hardware teams",
    "link": "https://www.cnbc.com/2024/01/11/google-layoffs-hundreds-of-jobs-cut-across-engineering-and-hardware.html",
    "image": "https://news.google.com/api/attachments/CC8iK0NnNW5aWEp0U1VvMVFrOTFaa05pVFJDb0FSaXNBaWdCTWdZQk1KYTBtQWs=-w400-h224-p-df-rw",
    "source": "CNBC",
    "datetime": "2024-01-11T08:00:00.000Z",
    "time": "Jan 11",
    "articleType": "regular",
    "content": "This Cookie Notice (“Notice”) explains how NBCUniversal and its affiliates (“NBCUniversal” or “we”), along with our partners, including advertisers and vendors, use cookies and similar tracking technologies when you use our websites, applications, such as games, interactive TV, voice-activated assistants, and other services that link to this policy, as well as connected devices, including those used in our theme parks (“Services”). This Notice provides more information about these technologies, your choices, and is part of the NBCUniversal Privacy Policy available here. You should read the Privacy Policy and this Notice for a full picture of NBCUniversal’s use of your information.\nWHAT ARE COOKIES AND HOW ARE THEY USED?\nLike many companies, we use cookies (small text files placed on your computer or device) and other tracking technologies on the Services (referred to together from this point forward as “Cookies”, unless otherwise stated), including HTTP cookies, HTML5 and Flash local storage/flash cookies, web beacons/GIFs, embedded scripts, ETags/cache browsers, and software development kits.\nFirst-party Cookies are placed by us (including through the use of third-party service providers) and are used to allow you to use the Services and their features and to assist in analytics activities.\nCertain third parties may place their Cookies on your device and use them to recognize your device when you visit the Services and when you visit other websites or online services. These third parties collect and use this information pursuant to their own privacy policies. Third-party Cookies enable certain features or functionalities, and advertising, to be provided on the Services.\nThe Services use the following types of first and third-party Cookies for these purposes:\nStrictly Necessary Cookies: These Cookies are required for Service functionality, including for system administration, security and fraud prevention, and to enable any purchasing capabilities. You can set your browser to block these Cookies, but some parts of the site may not function properly.\nInformation Storage and Access: These Cookies allow us and our partners to store and access information on the device, such as device identifiers.\nMeasurement and Analytics: These Cookies collect data regarding your usage of and performance of the Services, apply market research to generate audiences, and measure the delivery and effectiveness of content and advertising. We and our third-party vendors use these Cookies to perform analytics, so we can improve the content and user experience, develop new products and services, and for statistical purposes. They are also used to recognize you and provide further insights across platforms and devices for the above purposes.\nPersonalization Cookies: These Cookies enable us to provide certain features, such as determining if you are a first-time visitor, capping message frequency, remembering choices you have made (e.g., your language preferences, time zone), and assist you with logging in after registration (including across platforms and devices).  These Cookies also allow your device to receive and send information, so you can see and interact with ads and content.\nContent Selection and Delivery Cookies: Data collected under this category can also be used to select and deliver personalized content, such as news articles and videos.\nSocial Media Cookies: These Cookies are set by social media platforms on the Services to enable you to share content with your friends and networks. Social media platforms have the ability to track your online activity outside of the Services. This may impact the content and messages you see on other services you visit.\nWe and third parties may associate Measurement And Analytics Cookies, Personalization Cookies, Content Selection, Delivery Cookies, and Reporting, Ad Selection, Delivery and Reporting Cookies, and Social Media Cookies with other information we have about you.\nDepending on where you live, you may be able to adjust your Cookie preferences at any time via the “Cookie Settings” link in the footer of relevant websites. You can also use the methods described below to manage Cookies. You must take such steps on each browser or device that you use. If you replace, change or upgrade your browser or device, or delete your cookies, you may need to use these opt-out tools again. As some Cookie-management solutions also rely on Cookies, please adjust your browser Cookie settings carefully, following the relevant instructions below.\nBrowser Controls: You may be able to disable and manage some Cookies through your browser settings. If you use multiple browsers on the same device, you will need to manage your settings for each browser. Please click on any of the below browser links for instructions:\nIf the browser you use is not listed above, please refer to your browser’s help menu for information on how to manage Cookies. Please be aware that disabling cookies will not disable other analytics tools we may use to collect information about you or your use of our Services.\nAnalytics Provider Opt-Outs: To disable analytics Cookies you can use the browser controls discussed above or, for some of our providers, you can use their individual opt-out mechanisms:\nGoogle’s Privacy Policy  and  Google Analytics Opt-Out\nOmniture’s Privacy Policy  and  Omniture’s Opt-Out\nMixpanel’s Privacy Policy  and  Mixpanel’s Opt-Out\nThe above are examples of our analytics providers and this is not an exhaustive list. We are not responsible for the effectiveness of any other providers’ opt-out mechanisms.\nFlash Local Storage: These cookies are also known as local shared objects and may be used to store your preferences or display content by us, advertisers and other third-parties. Flash cookies need to be deleted in the storage section of your Flash Player Settings Manager.\nDigital Advertising Alliance in the US\nDigital Advertising Alliance of Canada\nEuropean Interactive Digital Advertising Alliance\nYou can also opt out of some of the advertising providers we use by visiting their opt-out pages:\nGoogle’s Privacy Policy and  Google Analytics Opt-Out Page\nFacebook Privacy Policy and  Facebook’s Opt-Out Page\nTwitter Privacy Policy and  Twitter’s Opt-Out Page\nLiveramp’s Privacy Policy and  Liveramp Opt-Out Page\nThese are examples of our advertising providers and this is not an exhaustive list. In addition, we are not responsible for the effectiveness of any of these providers’ opt-out mechanisms.\nMobile Settings: You may manage the collection of information for interest-based advertising purposes in mobile apps via the device’s settings, including managing the collection of location data. To opt out of mobile ad tracking from Nielsen or other third parties, you can do so by selecting the “Limit Ad Tracking” (for iOS devices) or “Opt out of Ads Personalization” (for Android devices) options in your device settings.\nConnected Devices: For connected devices, such as smart TVs or streaming devices, you should review the device’s settings and select the option that allows you to disable automatic content recognition or ad tracking.  Typically, to opt out, such devices require you to select options like “limit ad tracking” or to disable options such as “interest-based advertising,” “interactive TV,” or “smart interactivity”.  These settings vary by device type.\nConsequences of Deactivation of Cookies: If you disable or remove Cookies, some parts of the Services may not function properly. Information may still be collected and used for other purposes, such as research, online services analytics or internal operations, and to remember your opt-out preferences.\nFor inquiries about this Cookies Notice, please contact us at Privacy@nbcuni.com or Chief Privacy Officer, NBCUniversal Legal Department, 30 Rockefeller Plaza, New York, NY 10112, US.\nFor inquiries from users who reside in the European Economic Area, the United Kingdom or Switzerland, please contact us at Privacy@nbcuni.com  or Privacy, Legal Department, Central Saint Giles, St Giles High Street, London, WC2H 8NU, UK\nThis Notice may be revised occasionally and in accordance with legal requirements. Please revisit this Cookie Notice regularly to stay informed about our and our analytic and advertising partners’ use of Cookies.",
    "favicon": "https://sc.cnbcfm.com/applications/cnbc.com/staticcontent/img/favicon.ico"
  },
  {
    "title": "June Feature Drop: New features and upgrades for the Pixel portfolio",
    "link": "https://blog.google/products/pixel/pixel-feature-drop-june-2024/",
    "image": "https://news.google.com/api/attachments/CC8iL0NnNVdTbVZtVld4TlpETm9Wa2g2VFJDb0FSaXNBaWdCTWdrQlJZYWpSZU80N3dF=-w400-h224-p-df-rw",
    "source": "The Keyword | Google Product and Technology News",
    "datetime": "2024-06-11T07:00:00.000Z",
    "time": "Jun 11",
    "articleType": "regular",
    "content": "June Feature Drop for Pixel Devices\nNew features and upgrades for Pixel phones, watches, and tablets.\nPixel Phones: Gemini Nano, improved transcripts, big screen display, Find My Device upgrade, better camera features, and quick phone number lookup.\nPixel Watches: Car Crash Detection, improved Fall Detection, easier payments with PayPal, and enhanced Google Home app.\nPixel Tablet: Richer doorbell notifications, new Google Home Favorites widget, and quick access to smart home devices.\nSummaries were generated by Google AI. Generative AI is experimental.\nGemini comes to more Pixel phones, Pixel Watches get Car Crash Detection and upgrades to Wear OS.\nOur latest Feature Drop is here, and it’s packed with incredible new capabilities for your device, including Gemini Nano coming to Pixel 8 and Pixel 8a as a developer option, and Car Crash Detection to Pixel Watch 2, plus so much more. These features begin rolling out to devices today and will continue over the next few weeks.\nFor Pixel PhonesGemini Nano expands to Pixel 8 and Pixel 8aGemini Nano is Google’s most efficient AI model built for on-device tasks,1 and starting today, you can use Pixel 8 and Pixel 8a to access Gemini Nano as a developer option. You can enable developer options in your Pixel’s settings.Take your transcripts to the next levelGemini powers Summarize in Recorder to run on-device on Pixel 8 and 8a – and the Recorder app2now delivers more detailed, downloadable summaries3to Pixel 8, Pixel 8 Pro and Pixel 8a. It’s able to detect and include the names of speakers so conversational transcripts are improved. And with the ability to export transcripts into text files or Google Docs,4keeping track of things like interviews and class lectures is made easy.\nTake your entertainment to the big screenPixel 8a, Pixel 8 and Pixel 8 Pro can now display content on a larger screen,5like your computer monitor, when you plug it in via USB-C, giving you more space to enjoy your favorite movies, shows and slides.6\nFind your phone, even when the battery diesFind My Device is getting a helpful upgrade that enables you to locate your phone even when it’s off or the battery is dead,7all while keeping your location data encrypted and private from Google.Put your best faces forwardThe Pixel camera can now automatically identify the best moment from your photo in HDR+ with just a single shutter press, so it’s easier to snap a photo where your face is in focus and smiling.8\nQuickly look up a mystery phone numberDon’t recognize a phone number? With just a few taps, you can now do a reverse phone number search directly from your call log.9\nSave and access your everyday essentials from Google WalletGoogle Wallet recently expanded to India, so even more people can securely store and access everyday items, like boarding passes, loyalty cards, event tickets, public transport tickets, gift cards, and more.Additional phone improvementsPixel’s camera now lets you manually pick which camera lens you want to use while taking photos, giving you more control over capturing the perfect shot.10\nFor Pixel WatchesExtra peace of mind when you’re on the goIn time for National Safety Month, Car Crash Detection11is launching as a brand new safety feature on Pixel Watch 2 that gives you more ways to stay safe and get help when you might need it most, even when you don’t have your Pixel phone. If you’re in a severe car accident, your watch can now check in to make sure you’re okay. If you need help or don’t respond, it will automatically dial emergency services. Car Crash Detection integrates with Emergency Sharing on your watch,12 so your emergency contacts will be notified and get your real-time location if you’ve been in an accident. We’ve also improved Fall Detection to better detect falls from bicycles.13\nMake life easier with improved app experiencesNow you have more ways to pay from your Pixel Watch when you’re out and about without taking your physical wallet out. You can now link your PayPal account to Google Wallet14on your watch for ease the next time you leave your wallet at home or your hands are full.\nYour Google Home app is getting a revamp and you can now access your smart home devices faster and easier directly from your watch:1516Access your Google Home Favorites with just a swipe on your watch. Tap the device icon to open the Home app and adjust the temperature, dim the lights, and more.You can now access a smart home device from your watch face on Wear OS. Tap the device shortcut and make adjustments, right on your wrist.The Google Home app on Wear OS gives you more ways to control your smart home devices. Adjust the rotation of your blinds or fan speed, all from your watch.\nFor Pixel TabletSee who’s thereIs someone ringing your doorbell? Your Pixel Tablet will now have richer doorbell notifications17when docked in hub mode, giving you a snapshot of who is at your front door. You can talk to them with two-way talk, or send a Quick Response.\nSmart home favoritesThe new Google Home Favorites widget18gives you even quicker access to your compatible smart home devices. You’ll get quick, customizable smart home controls on your phone and tablet home screen, so you can access your thermostat or turn on your lights with just a quick tap.\nAll of these features team up to make your Pixel devices even better. Whether you’re at home or traveling, your Pixel devices are must-haves for this summer.",
    "favicon": "/favicon.ico"
  }
]